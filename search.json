[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Subsetting vectors in R\n\n\n\n\n\n\nR\n\n\nBasics\n\n\n\nExtracting one or more elements from a vector is a fundamental and important operation in any data analysis pipeline. In this post, I showcase various options for creating subsets of vectors.\n\n\n\n\n\nNov 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSubsetting data frames in R\n\n\n\n\n\n\nR\n\n\nBasics\n\n\n\nSelecting rows and columns from a data frame are basic data manipulation operations. In this post, I show several options for creating subsets of data frames in R, and I also point out important differences between classic data.frame and Tidyverse tibble objects.\n\n\n\n\n\nNov 29, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWarming stripes with R\n\n\n\n\n\n\nR\n\n\nVisualization\n\n\n\nWarming stripes are a popular way to represent average temperature changes in a particular location over time. In this post, I show how to generate these visualizations using R.\n\n\n\n\n\nFeb 9, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving Google Fonts from a Quarto website\n\n\n\n\n\n\nQuarto\n\n\nWeb\n\n\n\nMany website frameworks and themes embed Google Fonts, which is very convenient. However, contacting third-party servers in the background without acquiring explicit consent might violate data privacy laws. In addition, using local fonts (or fonts available directly on a website) is often faster than retrieving from an external server. In this post, I show how to disable Google Fonts and use fonts stored on your own server for websites generated with Quarto and its built-in themes.\n\n\n\n\n\nSep 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nWhitening with PCA and ZCA\n\n\n\n\n\n\nPython\n\n\nEEG\n\n\nICA\n\n\n\nWhitening (or sphering) is an important preprocessing step prior to performing independent component analysis (ICA) on EEG/MEG data. In this post, I explain the intuition behind whitening and illustrate the difference between two popular whitening methods – PCA (principal component analysis) and ZCA (zero-phase component analysis).\n\n\n\n\n\nDec 17, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving eye activity from EEG via ICA\n\n\n\n\n\n\nPython\n\n\nMNE\n\n\nEEG\n\n\nEOG\n\n\nArtifacts\n\n\nICA\n\n\n\nEEG signals often contain eye activity (movement and/or blinks), which usually needs to be removed before performing EEG analysis. In this post, I show how to remove such ocular artifacts using independent component analysis (ICA).\n\n\n\n\n\nJan 29, 2018\n\n\n\n\n\n\n\n\n\n\n\n\nVisualizing EEG data\n\n\n\n\n\n\nPython\n\n\nMNE\n\n\nEEG\n\n\nArtifacts\n\n\n\nAfter importing EEG data, it is usually helpful to visualize the raw EEG traces. Despite the availability of numerous automated artifact removal or reduction techniques, manual inspection remains important (often in combination with automated methods) to obtain clean data. In this post, I show how to visualize an EEG data set and how to interactively mark segments containing artifacts.\n\n\n\n\n\nNov 28, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nImporting EEG data\n\n\n\n\n\n\nPython\n\n\nMNE\n\n\nEEG\n\n\n\nPython is an extremely popular programming language, and the scientific Python community has created a striving ecosystem of neuroscience tools. Among these, MNE is the most popular EEG/MEG package, which offers almost anything required in an EEG processing pipeline. In this post, I show how to import EEG data sets and how to view and edit associated meta data.\n\n\n\n\n\nOct 23, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nRemoving eye activity from EEG via regression\n\n\n\n\n\n\nPython\n\n\nMNE\n\n\nEEG\n\n\nEOG\n\n\nArtifacts\n\n\n\nEEG signals often contain eye activity (movement and/or blinks), which usually needs to be removed before performing EEG analysis. In this post, I show how to get rid of ocular artifacts using a regression-based approach.\n\n\n\n\n\nOct 20, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSetting up Python for EEG analysis\n\n\n\n\n\n\nPython\n\n\nEEG\n\n\nBasics\n\n\n\nInstalling Python is pretty straightforward. In this post, I describe how to set up Python for EEG analysis on Windows, macOS, and Linux.\n\n\n\n\n\nOct 9, 2017\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html",
    "href": "blog/whitening-pca-zca/index.html",
    "title": "Whitening with PCA and ZCA",
    "section": "",
    "text": "Whitening (also known as sphering) is a linear transformation used for decorrelating signals. Applied to EEG, this means that the original channel time series (which tend to be highly correlated) are transformed into uncorrelated signals with identical variances. The term whitening is derived from white noise (which in turn draws its name from white light), which consists of serially uncorrelated samples. Whitening thus transforms a random vector into a white noise vector with uncorrelated components.\nTheoretically, there are infinitely many possibilities to perform a whitening transformation. We will explore two popular whitening methods in more detail, namely principal component analysis (PCA) and zero-phase component analysis (ZCA), which was introduced by Bell and Sejnowski (1997). These methods are commonly used in EEG/MEG analysis as a preprocessing step prior to independent component analysis (ICA) (see this post on how to remove ocular artifacts with ICA). If you want to delve into the matter more deeply, Kessy et al. (2018) discuss even more possible whitening methods.\nMathematically, whitening takes a random vector \\(\\mathbf{x}\\) (the original EEG channel time series) and transforms it into a random vector \\(\\mathbf{z}\\) using a whitening matrix \\(\\mathbf{W}\\):\n\\[\\mathbf{z} = \\mathbf{W} \\mathbf{x}\\]\nImportantly, the original covariance matrix \\(\\text{cov}(\\mathbf{x}) = \\mathbf{\\Sigma}\\) becomes \\(\\text{cov}(\\mathbf{z}) = \\mathbf{I}\\) after the transformation – the identity matrix. This means that all components of \\(\\mathbf{z}\\) have unit variance and all correlations have been removed."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#introduction",
    "href": "blog/whitening-pca-zca/index.html#introduction",
    "title": "Whitening with PCA and ZCA",
    "section": "",
    "text": "Whitening (also known as sphering) is a linear transformation used for decorrelating signals. Applied to EEG, this means that the original channel time series (which tend to be highly correlated) are transformed into uncorrelated signals with identical variances. The term whitening is derived from white noise (which in turn draws its name from white light), which consists of serially uncorrelated samples. Whitening thus transforms a random vector into a white noise vector with uncorrelated components.\nTheoretically, there are infinitely many possibilities to perform a whitening transformation. We will explore two popular whitening methods in more detail, namely principal component analysis (PCA) and zero-phase component analysis (ZCA), which was introduced by Bell and Sejnowski (1997). These methods are commonly used in EEG/MEG analysis as a preprocessing step prior to independent component analysis (ICA) (see this post on how to remove ocular artifacts with ICA). If you want to delve into the matter more deeply, Kessy et al. (2018) discuss even more possible whitening methods.\nMathematically, whitening takes a random vector \\(\\mathbf{x}\\) (the original EEG channel time series) and transforms it into a random vector \\(\\mathbf{z}\\) using a whitening matrix \\(\\mathbf{W}\\):\n\\[\\mathbf{z} = \\mathbf{W} \\mathbf{x}\\]\nImportantly, the original covariance matrix \\(\\text{cov}(\\mathbf{x}) = \\mathbf{\\Sigma}\\) becomes \\(\\text{cov}(\\mathbf{z}) = \\mathbf{I}\\) after the transformation – the identity matrix. This means that all components of \\(\\mathbf{z}\\) have unit variance and all correlations have been removed."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#toy-data",
    "href": "blog/whitening-pca-zca/index.html#toy-data",
    "title": "Whitening with PCA and ZCA",
    "section": "Toy data",
    "text": "Toy data\nTo illustrate the differences between PCA and ZCA whitening, let’s create some toy data. Specifically, we generate 1000 samples of two correlated time series \\(x_1\\) and \\(x_2\\).\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nrng = np.random.default_rng(seed=2)\nmu = [0, 0]\nsigma = [[5, 4], [4, 5]]  # must be positive semi-definite\nn = 1000\nx = rng.multivariate_normal(mu, sigma, size=n).T\n\nThe two time series are stored in the NumPy array x of shape (2, 1000).\nFor visualization purposes that will become clear in a moment, we determine the 20 most extreme values and denote their indices as set1 (the indices of the remaining data points are labeled set2):\n\nset1 = np.argsort(np.linalg.norm(x, axis=0))[-20:]\nset2 = list(set(range(n)) - set(set1))\n\nLet’s now plot all values of \\(x_1\\) versus their corresponding values of \\(x_2\\).\n\nfig, ax = plt.subplots()\nax.scatter(x[0, set1], x[1, set1], s=20, c=\"red\", alpha=0.2)\nax.scatter(x[0, set2], x[1, set2], s=20, alpha=0.2)\nax.set_aspect(\"equal\")\nax.set_xlim(-8, 8)\nax.set_ylim(-8, 8)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.set_title(\"Original\")\n\n\n\n\n\n\n\n\n\n\nClearly, these two time series appear to be highly correlated. The tilted ellipsoidal shape of the scatter plot indicates that as the values of \\(x_1\\) increase, the values of \\(x_2\\) also tend to increase. Indeed, the Pearson correlation between \\(x_1\\) and \\(x_2\\) is 0.81 and can be computed with:\n\nnp.corrcoef(x)[0, 1]\n\nnp.float64(0.8108740736106009)\n\n\nThe red dots indicate the most extreme values, and we will observe how these points are transformed by the subsequent whitening procedures."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#eigendecomposition",
    "href": "blog/whitening-pca-zca/index.html#eigendecomposition",
    "title": "Whitening with PCA and ZCA",
    "section": "Eigendecomposition",
    "text": "Eigendecomposition\nBoth PCA and ZCA are based on eigenvectors and eigenvalues of the (empirical) covariance matrix. In particular, the covariance matrix can be decomposed into its eigenvectors \\(\\mathbf{U}\\) and eigenvalues \\(\\mathbf{\\Lambda}\\) as:\n\\[\\mathbf{\\Sigma} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^T\\]\nLet’s compute these quantities for our toy data:\n\nsigma = np.cov(x)\nevals, evecs = np.linalg.eigh(sigma)\n\nNote that we use the empirical covariance matrix derived from the data instead of the true covariance matrix, which is generally unknown. Furthermore, because a covariance matrix is always symmetric, we can use the optimized np.linalg.eigh() function instead of the more generic np.linalg.eig() version (this also makes sure that we will always get real eigenvalues instead of complex ones). Alternatively, we could also use np.linalg.svd() directly on the data x (instead of the covariance matrix) to compute the eigenvectors and eigenvalues, which can be numerically more stable in some situations."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#whitening-with-pca",
    "href": "blog/whitening-pca-zca/index.html#whitening-with-pca",
    "title": "Whitening with PCA and ZCA",
    "section": "Whitening with PCA",
    "text": "Whitening with PCA\nThe whitening matrix \\(\\mathbf{W}^{\\mathrm{PCA}}\\) for PCA can be written as:\n\\[\\mathbf{W}^{\\mathrm{PCA}} = \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T\\]\nThis means that the data can be transformed as follows:\n\\[\\mathbf{z} = \\mathbf{W}^{\\mathrm{PCA}} \\mathbf{x} = \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T \\mathbf{x}\\]\nTherefore, we can whiten our toy data accordingly:\n\nz = np.diag(evals**(-1/2)) @ evecs.T @ x\n\nLet’s see how our transformed toy data looks in a scatter plot:\n\nfig, ax = plt.subplots()\nax.scatter(z[0, set1], z[1, set1], s=20, c=\"red\", alpha=0.2)\nax.scatter(z[0, set2], z[1, set2], s=20, alpha=0.2)\nax.set_aspect(\"equal\")\nax.set_xlim(-8, 8)\nax.set_ylim(-8, 8)\nax.set_xlabel(\"$z_1$\")\nax.set_ylabel(\"$z_2$\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.set_title(\"PCA\")\n\n\n\n\n\n\n\n\n\n\nClearly, the transformation removed the correlation between the two time series, because the scatter plot now looks like a sphere (a circle in two dimensions) – hence the name sphering. Indeed, the correlation coefficient yields a value practically equal to zero:\n\nnp.corrcoef(z)[0, 1]\n\nnp.float64(-4.997670612351674e-16)\n\n\nImportantly, PCA has rotated all data points as illustrated by the new positions of the red dots; these do not lie on a diagonal with roughly 45 degrees anymore, but are now aligned with the vertical axis."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#whitening-with-zca",
    "href": "blog/whitening-pca-zca/index.html#whitening-with-zca",
    "title": "Whitening with PCA and ZCA",
    "section": "Whitening with ZCA",
    "text": "Whitening with ZCA\nThe whitening matrix \\(\\mathbf{W}^{\\mathrm{ZCA}}\\) for ZCA can be written as:\n\\[\\mathbf{W}^{\\mathrm{ZCA}} = \\mathbf{U} \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T\\]\nIn fact, this transformation looks almost like PCA whitening, but with an additional rotation by \\(\\mathbf{U}\\). Again, the original data can be transformed as follows:\n\\[\\mathbf{z} = \\mathbf{W}^{\\mathrm{ZCA}} \\mathbf{x} = \\mathbf{U} \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T \\mathbf{x}\\]\nWe whiten our data accordingly and take a look at the resulting scatter plot:\n\nz = evecs @ np.diag(evals**(-1/2)) @ evecs.T @ x\n\n\nfig, ax = plt.subplots()\nax.scatter(z[0, set1], z[1, set1], s=20, c=\"red\", alpha=0.2)\nax.scatter(z[0, set2], z[1, set2], s=20, alpha=0.2)\nax.set_aspect(\"equal\")\nax.set_xlim(-8, 8)\nax.set_ylim(-8, 8)\nax.set_xlabel(\"$z_1$\")\nax.set_ylabel(\"$z_2$\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.set_title(\"ZCA\")\n\n\n\n\n\n\n\n\n\n\nZCA has also decorrelated the data because the scatter plot looks spherical; the correlation is again practically zero:\n\nnp.corrcoef(z)[0, 1]\n\nnp.float64(-1.5772057222702893e-15)\n\n\nHowever, and in contrast to PCA, ZCA has preserved the orientation of the original data points. This can be observed from the positions of the red dots, which are aligned along the same direction as the original data. This property has given this whitening transformation its name “zero-phase”, because it minimally distorts the original phase (i.e., orientation) of the data."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#conclusions",
    "href": "blog/whitening-pca-zca/index.html#conclusions",
    "title": "Whitening with PCA and ZCA",
    "section": "Conclusions",
    "text": "Conclusions\nBoth PCA and ZCA whiten the original data, but they perform different rotations. It can be shown that PCA is optimal if the goal is compression of the original data (because principal components are sorted according to their explained variance), whereas ZCA is optimal if the goal is to keep the transformed random vector as similar as possible to the original one (thus ZCA cannot be used to compress the data). Kessy et al. (2018) provide mathematical proofs of these propositions.\nIt is worth mentioning that standardizing the data prior to whitening might sometimes be useful, especially if the individual signals are on different scales. Usually, standardization is not necessary if all signals are EEG channels, but if a combination of EEG and MEG signals simultaneously enter the analysis, all data should be rescaled to avoid biasing the whitening transformation to signals with higher variance.\nICA algorithms are typically kick-started from whitened data. An article by Montoya-Martínez et al. (2017) suggests that some ICA variants can be sensitive to the choice of the initial whitening procedure. Specifically, it can make a difference whether PCA or ZCA is used prior to performing Extended Infomax as implemented in MNE or EEGLAB. The reason seems to be the slow convergence of this particular ICA algorithm. Picard (Ablin et al., 2018) improves upon this implementation and provides much faster convergence for both Extended Infomax and FastICA variants. Therefore, it should be rather insensitive to the choice of the particular whitening procedure.\nFinally, regarding the common practice of reducing dimensionality with PCA prior to ICA, an article by Artoni et al. (2018) argues that pruning principal components might adversely affect the quality of the resulting independent components. This means that if PCA is used to whiten the data, all components should be retained (i.e. the data should not be compressed)."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#acknowledgments",
    "href": "blog/whitening-pca-zca/index.html#acknowledgments",
    "title": "Whitening with PCA and ZCA",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI’d like to thank Pierre Ablin for his very helpful comments on an earlier version of this post."
  },
  {
    "objectID": "blog/whitening-pca-zca/index.html#code",
    "href": "blog/whitening-pca-zca/index.html#code",
    "title": "Whitening with PCA and ZCA",
    "section": "Code",
    "text": "Code\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# generate toy data\nrng = np.random.default_rng(seed=2)\nmu = [0, 0]\nsigma = [[5, 4], [4, 5]]  # must be positive semi-definite\nn = 1000\nx = rng.multivariate_normal(mu, sigma, size=n).T\n\n# store 20 most extreme values for visualization\nset1 = np.argsort(np.linalg.norm(x, axis=0))[-20:]\nset2 = list(set(range(n)) - set(set1))\n\n# plot original data\nfig, ax = plt.subplots()\nax.scatter(x[0, set1], x[1, set1], s=20, c=\"red\", alpha=0.2)\nax.scatter(x[0, set2], x[1, set2], s=20, alpha=0.2)\nax.set_aspect(\"equal\")\nax.set_xlim(-8, 8)\nax.set_ylim(-8, 8)\nax.set_xlabel(\"$x_1$\")\nax.set_ylabel(\"$x_2$\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.set_title(\"Original\")\n\nnp.corrcoef(x)[0, 1]\n\nsigma = np.cov(x)\nevals, evecs = np.linalg.eigh(sigma)\n\n# PCA\nz = np.diag(evals**(-1/2)) @ evecs.T @ x\n\nfig, ax = plt.subplots()\nax.scatter(z[0, set1], z[1, set1], s=20, c=\"red\", alpha=0.2)\nax.scatter(z[0, set2], z[1, set2], s=20, alpha=0.2)\nax.set_aspect(\"equal\")\nax.set_xlim(-8, 8)\nax.set_ylim(-8, 8)\nax.set_xlabel(\"$z_1$\")\nax.set_ylabel(\"$z_2$\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.set_title(\"PCA\")\n\nnp.corrcoef(z)[0, 1]\n\n# ZCA\nz = evecs @ np.diag(evals**(-1/2)) @ evecs.T @ x\n\nfig, ax = plt.subplots()\nax.scatter(z[0, set1], z[1, set1], s=20, c=\"red\", alpha=0.2)\nax.scatter(z[0, set2], z[1, set2], s=20, alpha=0.2)\nax.set_aspect(\"equal\")\nax.set_xlim(-8, 8)\nax.set_ylim(-8, 8)\nax.set_xlabel(\"$z_1$\")\nax.set_ylabel(\"$z_2$\")\nax.spines[\"top\"].set_visible(False)\nax.spines[\"right\"].set_visible(False)\nax.set_title(\"ZCA\")\n\nnp.corrcoef(z)[0, 1]"
  },
  {
    "objectID": "blog/importing-eeg-data/index.html",
    "href": "blog/importing-eeg-data/index.html",
    "title": "Importing EEG data",
    "section": "",
    "text": "MNE is currently the most popular Python package for EEG/MEG processing. It offers a wide variety of useful tools and methods such as reading and writing various file formats, temporal and spatial filtering, independent component analysis (ICA), forward modeling, inverse solutions, time-frequency decompositions, visualization, and more. In fact, if you are familiar with EEGLAB, you might find that you can perform many similar analyses with Python and MNE. In this post, I will show how to import EEG data sets as well as view and edit associated meta data."
  },
  {
    "objectID": "blog/importing-eeg-data/index.html#introduction",
    "href": "blog/importing-eeg-data/index.html#introduction",
    "title": "Importing EEG data",
    "section": "",
    "text": "MNE is currently the most popular Python package for EEG/MEG processing. It offers a wide variety of useful tools and methods such as reading and writing various file formats, temporal and spatial filtering, independent component analysis (ICA), forward modeling, inverse solutions, time-frequency decompositions, visualization, and more. In fact, if you are familiar with EEGLAB, you might find that you can perform many similar analyses with Python and MNE. In this post, I will show how to import EEG data sets as well as view and edit associated meta data."
  },
  {
    "objectID": "blog/importing-eeg-data/index.html#prerequisites",
    "href": "blog/importing-eeg-data/index.html#prerequisites",
    "title": "Importing EEG data",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe first step in almost any EEG processing pipeline is importing the raw data files. There are many different file formats for storing EEG data, mostly because every amplifier manufacturer uses its own data format. For example, BrainProducts amplifiers store data as an EEG/VHDR/VMRK file triplet, BioSemi amplifiers create BDF files (an extension of the European Data Format), Neuroscan amplifiers use proprietary CNT files, and so on. Luckily, MNE comes with support for many popular EEG file formats (see here for a selection of supported formats).\nThe EEG motor movement/imagery data set we will use in this tutorial was contributed to the public domain by the developers of the BCI2000 system. In brief, they recorded 64-channel EEG from over 100 participants during motor execution and motor imagery tasks. In this tutorial, we will analyze only one participant and only one motor imagery run. If you want to follow along, go ahead and download run 4 of subject 1. All subsequent commands assume that this file is located in the current working directory.\n\n\n\n\n\n\nNote\n\n\n\nMNE has a dedicated loader mne.datasets.eegbci.load_data() for this data set, which makes it even easier to work with particular subjects and runs.\n\n\nNow that we have selected our data, it is time to fire up Python. Since we want to use the MNE package, we have to import it:\n\nimport mne\n\nYou can check the version of MNE as follows (in general, make sure to always use the latest version):\n\nmne.__version__\n\n'1.8.0'"
  },
  {
    "objectID": "blog/importing-eeg-data/index.html#importing-eeg-data",
    "href": "blog/importing-eeg-data/index.html#importing-eeg-data",
    "title": "Importing EEG data",
    "section": "Importing EEG data",
    "text": "Importing EEG data\nImporting EEG data is pretty straightforward. We can either specify the full path to the file, or we can make sure that the file is in the current working directory, in which case the file name alone is sufficient. In this case, we can use the following command to import the raw data:\n\nraw = mne.io.read_raw(\"S001R04.edf\", preload=True)\n\nThe argument preload=True means that MNE performs the actual loading process immediately instead of the default lazy behavior.\n\n\n\n\n\n\nTip\n\n\n\nI am not showing the output that is generated during the loading process here. Usually, it contains some more or less useful logging messages, so if anything goes wrong make sure to carefully study these messages. If you don’t want to see informative messages, you can suppress them as follows:\n\nmne.set_log_level(\"WARNING\")\n\nPutting this line right after import mne will instruct MNE to only print warnings and errors. Throughout my posts, this is the setting that I will use."
  },
  {
    "objectID": "blog/importing-eeg-data/index.html#meta-data",
    "href": "blog/importing-eeg-data/index.html#meta-data",
    "title": "Importing EEG data",
    "section": "Meta data",
    "text": "Meta data\nThe previous assignment generated an mne.io.Raw object called raw in our workspace. This object holds both the EEG and associated meta data. We can get some basic information by inspecting this object in the interactive Python session:\n\nraw\n\n\n\n&lt;RawEDF | S001R04.edf, 64 x 20000 (125.0 s), ~9.8 MB, data loaded&gt;\n\n\nWe can see the file name, the number of channels and sample points, the length in seconds, and the approximate size of the data in memory.\n\nThe info attribute\nWe can dig deeper into the meta data by inspecting the info attribute associated with raw:\n\nraw.info\n\n\n\n&lt;Info | 8 non-empty values\n bads: []\n ch_names: Fc5., Fc3., Fc1., Fcz., Fc2., Fc4., Fc6., C5.., C3.., C1.., ...\n chs: 64 EEG\n custom_ref_applied: False\n highpass: 0.0 Hz\n lowpass: 80.0 Hz\n meas_date: 2009-08-12 16:15:00 UTC\n nchan: 64\n projs: []\n sfreq: 160.0 Hz\n subject_info: 3 items (dict)\n&gt;\n\n\nThere are several non-empty values in this attribute. For example, the line chs: 64 EEG near the top of the output tells us that there are 64 EEG channels (the first few channel names are listed in the line starting with ch_names). Individual elements of raw.info can be accessed with dictionary-like indexing. For example, the sampling frequency is available as:\n\nraw.info[\"sfreq\"]\n\n160.0\n\n\nOther useful info keys are:\n\n\"bads\": A list of bad (noisy) channels which should be ignored in further analyses. Initially, this list is empty (as in our example), but we will manually populate it in another post.\n\"ch_names\": A list of channel names.\n\"chs\": A detailed list of channel properties, including their types (for example, \"EEG\", \"EOG\" or \"MISC\").\n\"highpass\" and \"lowpass\": Highpass and lowpass edge frequencies that were used during recording.\n\"meas_date\": The recording date (a datetime.datetime object).\n\n\n\nRenaming channels\nThe output of raw.info revealed that some channel names are suffixed with one or more dots. Since these are non-standard names, let’s rename the channels by removing all trailing dots:\n\nraw.rename_channels(lambda s: s.strip(\".\"))\n\n\n\n\n\n\n\nNote\n\n\n\nIn general, methods in MNE modify objects in place. In addition, most methods also return the (modified) object, which allows for chaining multiple methods.\n\n\n\n\nAssigning a montage\nFor good measure (and for later use), we can assign a montage to the data (a montage relates channels names to standardized or actual locations on the scalp). First, let’s list all montages that ship with MNE:\n\nmne.channels.get_builtin_montages()\n\n['standard_1005',\n 'standard_1020',\n 'standard_alphabetic',\n 'standard_postfixed',\n 'standard_prefixed',\n 'standard_primed',\n 'biosemi16',\n 'biosemi32',\n 'biosemi64',\n 'biosemi128',\n 'biosemi160',\n 'biosemi256',\n 'easycap-M1',\n 'easycap-M10',\n 'easycap-M43',\n 'EGI_256',\n 'GSN-HydroCel-32',\n 'GSN-HydroCel-64_1.0',\n 'GSN-HydroCel-65_1.0',\n 'GSN-HydroCel-128',\n 'GSN-HydroCel-129',\n 'GSN-HydroCel-256',\n 'GSN-HydroCel-257',\n 'mgh60',\n 'mgh70',\n 'artinis-octamon',\n 'artinis-brite23',\n 'brainproducts-RNP-BA-128']\n\n\nAccording to the data set documentation, the channel locations conform to the international 10–10 system. MNE does not seem to ship a 10–10 montage, but easycap-M1 contains template locations from the extended 10–20 system:\n\nmontage = mne.channels.make_standard_montage(\"easycap-M1\")\nmontage.plot(sphere=\"eeglab\")\n\n\n\n\n\n\n\n\n\n\nIt looks like this montage contains all channels except for T9 and T10, so let’s remove these two channels from our data:\n\nraw.drop_channels([\"T9\", \"T10\"])\n\nNow we can assign the montage to our raw object:\n\nraw.set_montage(montage, match_case=False)\n\nIf we wanted to keep channels T9 and T10, we would have to use a montage which includes these channels. This is not possible out of the box and requires some manual steps, which we will not cover in this post.\n\n\nRe-referencing\nThe data documentation does not mention any reference electrode, but it is safe to assume that all channels are referenced to some standard location such as a mastoid or the nose. Often, we want to re-reference EEG data to the so-called average reference (the average over all recording channels). In MNE, we can compute average referenced signals as follows:\n\nraw.set_eeg_reference(\"average\")\n\n\n\nAnnotations\nFinally, many EEG data sets include discrete events, either in the form of an analog stimulation channel or discrete annotations. Our example data set contains annotations that can be accessed with the annotations attribute:\n\nraw.annotations\n\n&lt;Annotations | 30 segments: T0 (15), T1 (8), T2 (7)&gt;\n\n\nWe see that there are 30 annotations in total. There are three types of annotations named T0, T1, and T2. According to the data set description, T0 corresponds to rest, T1 corresponds to motion onset of the left fist, and T2 corresponds to motion onset of the right fist.\nIf you encounter a file with an analog stimulation channel (this is typically the case for data recorded with BioSemi amplifiers), you need to extract discrete events from this channel as a first step. The mne.find_events() function converts information contained in an analog stimulation channel to a NumPy array of shape (N, 3), where N is the number of detected events. The first column contains event onsets (in samples), whereas the third column contains (integer) event codes. The second column contains the values of the stimulation channel one sample before the detected events (this column can usually be ignored).\nThis NumPy array can be converted to an Annotations object using mne.annotations_from_events(), which is often necessary for further analyses (note that you can associate an existing Annotations object with a Raw object by calling the raw.set_annotations() method).\nIn the next post, I will demonstrate how to visualize this data set and how to interactively mark bad channels and bad segments."
  },
  {
    "objectID": "blog/importing-eeg-data/index.html#code",
    "href": "blog/importing-eeg-data/index.html#code",
    "title": "Importing EEG data",
    "section": "Code",
    "text": "Code\n\n\nCode\nimport mne\n\nraw = mne.io.read_raw_edf(\"S001R04.edf\", preload=True)\nraw.rename_channels(lambda s: s.strip(\".\"))\n\nmontage = mne.channels.make_standard_montage(\"easycap-M1\")\n\nraw.drop_channels([\"T9\", \"T10\"])\nraw.set_montage(montage, match_case=False)\n\nraw.set_eeg_reference(\"average\")"
  },
  {
    "objectID": "blog/setting-up-python/index.html",
    "href": "blog/setting-up-python/index.html",
    "title": "Setting up Python for EEG analysis",
    "section": "",
    "text": "The official installer, which is available for Windows and macOS, is the easiest way for beginners to install Python. If you use Linux, chances are that Python is already installed (if not, use your package manager to install it).\nWhen running the installer on Windows, make sure to check the option “Add python.exe to PATH” (by default this setting is disabled). I strongly recommend to use the default values for all other settings (and yes, this includes installing Python in your user directory).\n\nOn macOS, run both “Install Certificates” and “Update Shell Profile” commands available in the application folder after the installation is complete:\n\n\n\n\n\n\n\nNote\n\n\n\nAnaconda, a distribution with tons of pre-installed packages specifically tailored towards scientific computing, provides yet another way to install Python. If you want to use it, consider installing Miniforge instead, which is more lightweight and uses conda-forge as its package repository.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are an experienced Pythonista, I recommend uv, an extremely fast and versatile Python package and project manager."
  },
  {
    "objectID": "blog/setting-up-python/index.html#installing-python",
    "href": "blog/setting-up-python/index.html#installing-python",
    "title": "Setting up Python for EEG analysis",
    "section": "",
    "text": "The official installer, which is available for Windows and macOS, is the easiest way for beginners to install Python. If you use Linux, chances are that Python is already installed (if not, use your package manager to install it).\nWhen running the installer on Windows, make sure to check the option “Add python.exe to PATH” (by default this setting is disabled). I strongly recommend to use the default values for all other settings (and yes, this includes installing Python in your user directory).\n\nOn macOS, run both “Install Certificates” and “Update Shell Profile” commands available in the application folder after the installation is complete:\n\n\n\n\n\n\n\nNote\n\n\n\nAnaconda, a distribution with tons of pre-installed packages specifically tailored towards scientific computing, provides yet another way to install Python. If you want to use it, consider installing Miniforge instead, which is more lightweight and uses conda-forge as its package repository.\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are an experienced Pythonista, I recommend uv, an extremely fast and versatile Python package and project manager."
  },
  {
    "objectID": "blog/setting-up-python/index.html#additional-packages",
    "href": "blog/setting-up-python/index.html#additional-packages",
    "title": "Setting up Python for EEG analysis",
    "section": "Additional packages",
    "text": "Additional packages\nAlthough Python ships with an extensive standard library, most scientific packages are not part of Python itself. However, installing third-party Python packages is not difficult with the package manager pip, which is bundled with every Python installation. Note that you need to open a terminal to use pip (for example, Windows Terminal on Windows or Terminal on macOS).\n\n\n\n\n\n\nImportant\n\n\n\nOn macOS, the tool is available as pip3, just like the Python interpreter is called python3. So whenever you see pip in this document, you should use pip3 instead.\n\n\nTo install a package, you need to know its name. Therefore, the first step is to determine if a particular package is available in the so-called Python Package Index (or short PyPI). Currently, the only way to find out is to search directly on the website.\nIf the package is available, you can install it with the following command in your terminal (replace &lt;package&gt; with the actual package name):\npip install &lt;package&gt;\nThe following packages are useful for scientific computing in general and EEG processing in particular:\n\nNumPy provides a multi-dimensional array data type, which is the basis for almost all scientific packages.\nPandas provides a flexible data frame type similar to the one available in R.\nSciPy contains numerous algorithms for scientific computing.\nMatplotlib is the most popular plotting package in Python.\nScikit-learn is a powerful machine learning package for Python.\nMNE is a package for EEG/MEG analysis.\nMNELAB is a graphical user interface for MNE.\nPython-Picard is an extremely fast and efficient ICA implementation.\n\nI recommend that you install all of these packages right now so you can use them later in your EEG analyses.\n\n\n\n\n\n\nNote\n\n\n\nPackage names are case-insensitive, so for example pip install NumPy and pip install numpy both work."
  },
  {
    "objectID": "blog/setting-up-python/index.html#housekeeping",
    "href": "blog/setting-up-python/index.html#housekeeping",
    "title": "Setting up Python for EEG analysis",
    "section": "Housekeeping",
    "text": "Housekeeping\nIt is generally a good idea to use the most recent Python version. If a new release becomes available (you might want to check the official Python website once in a while), install it as previously explained. You can even have multiple versions of Python installed on your system if you want (but usually you only need the latest version).\nIn addition to Python itself, you probably also want to keep all installed packages up to date. Package updates are independent of new Python releases, so you should check for new package versions more frequently. You can use pip to get a list of outdated packages:\npip list --outdated\nThe following command upgrades a package to its latest version (replace &lt;package&gt; with the actual package name):\npip install -U &lt;package&gt;\nUnfortunately, you need to upgrade each outdated package individually.\nYou can also completely remove a specific package (again, replace &lt;package&gt; with the actual package name):\npip uninstall &lt;package&gt;\n\n\n\n\n\n\nTip\n\n\n\nIf you are already familiar with Python, you should consider working with virtual environments. This will isolate packages for different projects, which is useful for reproducibility and avoids conflicts between different package versions. The official documentation for the venv module is a good starting point."
  },
  {
    "objectID": "blog/setting-up-python/index.html#code-editors",
    "href": "blog/setting-up-python/index.html#code-editors",
    "title": "Setting up Python for EEG analysis",
    "section": "Code editors",
    "text": "Code editors\nA code editor or integrated development environment is an essential tool for writing Python programs. Good editors include support for syntax highlighting, indentation, line numbers, linting, code inspection, and more.\nI recommend Visual Studio Code or PyCharm if you have never used Python before. Both tools provide a great Python development experience.\n\n\n\n\n\n\nNote\n\n\n\nIf you decide to install Visual Studio Code, there are two things you should do before you start writing Python code:\n\nClick on the Extensions section in the left sidebar, search for “Python”, and install the official Python extension.\nInstall the Ruff extension to automatically lint and format your code according to the PEP 8 style guide.\n\n\n\nThat’s it, you are now ready to start working with Python!"
  },
  {
    "objectID": "blog/r-vector-subsetting/index.html",
    "href": "blog/r-vector-subsetting/index.html",
    "title": "Subsetting vectors in R",
    "section": "",
    "text": "The most basic data type in R is the atomic vector, which is essentially a one-dimensional array comprised of elements of a single type. Therefore, even a scalar number (such as 4.17) is actually a vector under the hood. This might be surprising, especially if you have experience in other programming languages, but it implies that array operations (which are extremely useful for data analysis) are built right into R. In this post, I will discuss how to extract one or more elements from a vector, a process commonly referred to as subsetting."
  },
  {
    "objectID": "blog/r-vector-subsetting/index.html#introduction",
    "href": "blog/r-vector-subsetting/index.html#introduction",
    "title": "Subsetting vectors in R",
    "section": "",
    "text": "The most basic data type in R is the atomic vector, which is essentially a one-dimensional array comprised of elements of a single type. Therefore, even a scalar number (such as 4.17) is actually a vector under the hood. This might be surprising, especially if you have experience in other programming languages, but it implies that array operations (which are extremely useful for data analysis) are built right into R. In this post, I will discuss how to extract one or more elements from a vector, a process commonly referred to as subsetting."
  },
  {
    "objectID": "blog/r-vector-subsetting/index.html#subsetting-vectors",
    "href": "blog/r-vector-subsetting/index.html#subsetting-vectors",
    "title": "Subsetting vectors in R",
    "section": "Subsetting vectors",
    "text": "Subsetting vectors\nIn general, R uses a pair of square brackets [] for selecting specific elements in a given object. Let’s consider the following vector v:\n\n(v = c(\"A\", \"B\", \"C\", \"D\"))\n\n[1] \"A\" \"B\" \"C\" \"D\"\n\n\n\nSubsetting by position\nThis vector has four elements, and we can grab a specific element by specifying its position within the square brackets. For example, here’s how we can extract the second element of v:\n\nv[2]  # second element\n\n[1] \"B\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nR uses 1-based indexing, so the position of the first element is 1, followed by 2 for the second element, and so on.\n\n\nIf we want to select two or more elements, it is necessary to wrap the desired positions within c(), because R always expects a single vector within the square brackets:\n\nv[c(2, 4)]  # second and fourth element\n\n[1] \"B\" \"D\"\n\n\nNegative indices grab all elements except for those specified by the negative numbers. For example, to get all elements except the third one:\n\nv[-3]  # all elements except the third\n\n[1] \"A\" \"B\" \"D\"\n\n\nThere is no special syntax for selecting the last element of a vector, so we have to write:\n\nv[length(v)]  # last element\n\n[1] \"D\"\n\n\n\n\nLogical subsetting\nWe can also use logical vectors for subsetting. The outcome will comprise elements corresponding to positions where the logical vector evaluates to TRUE. The following example illustrates this idea:\n\nv[c(FALSE, TRUE, TRUE, FALSE)]\n\n[1] \"B\" \"C\"\n\n\nThe result contains the second and third elements of v, because only the second and third elements of the logical index vector are TRUE. Subsetting with logical vectors usually involve comparisons (which evalute to logical vectors), so we can filter elements based on the values of the original vector. The following example illustrates this idea with a numeric vector x and a subset containing only positive values of x:\n\n(x = c(54, -23.22, -13.04, 1.99, -1.31, 48.6))\n\n[1]  54.00 -23.22 -13.04   1.99  -1.31  48.60\n\nx[x &gt; 0]  # create subset with positive values of x\n\n[1] 54.00  1.99 48.60\n\n\n\n\nSubsetting by name\nFinally, we can select elements in a named vector not only by position but also by name:\n\nw = c(first=\"A\", two=\"B\", three=\"C\", last=\"D\")\nw[2]  # by position still works\n\ntwo \n\"B\" \n\nw[\"two\"]  # by name (must be in quotes)\n\ntwo \n\"B\""
  },
  {
    "objectID": "blog/removing-eog-regression/index.html",
    "href": "blog/removing-eog-regression/index.html",
    "title": "Removing eye activity from EEG via regression",
    "section": "",
    "text": "Eye movement and eye blinks are clearly visible in the ongoing EEG. These ocular artifacts are produced by changes in the electrical dipole between the front and back of the eyes. Usually, it is important to minimize the influence of eye activity on the recorded EEG.\nThere are two popular methods available to remove ocular artifacts, namely approaches based on (multivariate linear) regression and approaches based on independent component analysis (ICA). In this post, I will focus on the regression method and explain the ICA-based variant in another post.\nThe regression-based approach in its simplest form dates back to Hillyard and Galambos (1970), who used data from a pre-experimental calibration run containing voluntarily produced ocular artifacts to estimate regression coefficients on the EEG. More than a decade later, Gratton, Coles, and Donchin (1983) improved upon this procedure in two ways. First, they used EOG recordings from the experimental data (the same data that needs to be corrected) instead of a calibration run. Second, they estimated regression coefficients separately for eye movements and eye blinks.\nDespite its age, removing ocular artifacts via linear regression remains a popular approach because it works well in many situations. However, it is important to keep in mind that dedicated EOG electrodes placed close to the eyes must be available. Also, if an EOG electrode fails during the recording session, the whole method breaks down. Furthermore, it is reasonable to assume that EOG electrodes will also pick up some amount of brain activity (after all, the brain sits right behind these sensors). Therefore, this method will likely remove brain activity in addition to ocular artifacts. On the plus side, there is no restriction on the minimum number of EEG electrodes required – the regression-based approach can clean even single-channel EEG recordings.\n\n\n\n\n\n\nNote\n\n\n\nMNE now provides a built-in method for removing artifacts via regression (check out the tutorial for more information). However, since the method is rather simple, it is also instructive to implement it manually, which is what we will do in this post."
  },
  {
    "objectID": "blog/removing-eog-regression/index.html#background",
    "href": "blog/removing-eog-regression/index.html#background",
    "title": "Removing eye activity from EEG via regression",
    "section": "",
    "text": "Eye movement and eye blinks are clearly visible in the ongoing EEG. These ocular artifacts are produced by changes in the electrical dipole between the front and back of the eyes. Usually, it is important to minimize the influence of eye activity on the recorded EEG.\nThere are two popular methods available to remove ocular artifacts, namely approaches based on (multivariate linear) regression and approaches based on independent component analysis (ICA). In this post, I will focus on the regression method and explain the ICA-based variant in another post.\nThe regression-based approach in its simplest form dates back to Hillyard and Galambos (1970), who used data from a pre-experimental calibration run containing voluntarily produced ocular artifacts to estimate regression coefficients on the EEG. More than a decade later, Gratton, Coles, and Donchin (1983) improved upon this procedure in two ways. First, they used EOG recordings from the experimental data (the same data that needs to be corrected) instead of a calibration run. Second, they estimated regression coefficients separately for eye movements and eye blinks.\nDespite its age, removing ocular artifacts via linear regression remains a popular approach because it works well in many situations. However, it is important to keep in mind that dedicated EOG electrodes placed close to the eyes must be available. Also, if an EOG electrode fails during the recording session, the whole method breaks down. Furthermore, it is reasonable to assume that EOG electrodes will also pick up some amount of brain activity (after all, the brain sits right behind these sensors). Therefore, this method will likely remove brain activity in addition to ocular artifacts. On the plus side, there is no restriction on the minimum number of EEG electrodes required – the regression-based approach can clean even single-channel EEG recordings.\n\n\n\n\n\n\nNote\n\n\n\nMNE now provides a built-in method for removing artifacts via regression (check out the tutorial for more information). However, since the method is rather simple, it is also instructive to implement it manually, which is what we will do in this post."
  },
  {
    "objectID": "blog/removing-eog-regression/index.html#implementation",
    "href": "blog/removing-eog-regression/index.html#implementation",
    "title": "Removing eye activity from EEG via regression",
    "section": "Implementation",
    "text": "Implementation\n\nData preprocessing\nTo demonstrate this method, we will download a publicly available EEG data set from the BNCI Horizon 2020 website. Specifically, we’ll use participant A01T from data set 001-2014 (go ahead and download this file and put it in your working directory if you want to follow along). The description states that the third run contains calibration eye movements. We will use these to estimate regression coefficients to correct EEG data in the subsequent fourth run.\nAs always, we start with some setup boilerplate. Since the example data is stored in a MAT file, we need to import the scipy.io.loadmat() function. Besides mne, we will also use numpy in our analysis.\n\nimport mne\nimport numpy as np\nfrom scipy.io import loadmat\n\nLoading the example data is pretty straightforward:\n\nmat = loadmat(\"A01T.mat\", simplify_cells=True)\n\nThis gives us a dictionary containing the data in a nicely organized way. Let’s check the available dictionary keys:\n\nmat.keys()\n\ndict_keys(['__header__', '__version__', '__globals__', 'data'])\n\n\nThe EEG data is stored in mat[\"data\"] as a list, where each list item corresponds to a run. For example, mat[\"data\"][0] contains data from the first run, mat[\"data\"][1] corresponds to the second run, and so on. Each run is represented by a dictionary, and the EEG data is stored under the \"X\" key.\nWe are interested in EEG data from the calibration run (third run, list index 2) and the experimental run (fourth run, list index 3), which we name eeg1 and eeg2 (note that we also multiply by 10-6 to scale the signals from microvolts to volts). This results in two NumPy arrays:\n\neeg1 = mat[\"data\"][2][\"X\"] * 1e-6  # run 3 (calibration)\neeg2 = mat[\"data\"][3][\"X\"] * 1e-6  # run 4 (experiment)\n\nThe next step is not strictly necessary, but we’ll convert eeg1 and eeg2 into mne.io.Raw objects. This will give us the ability to quickly generate plots of raw EEG traces before and after ocular artifact removal.\n\ninfo = mne.create_info(25, 250, ch_types=[\"eeg\"] * 22 + [\"eog\"] * 3)\nraw1 = mne.io.RawArray(eeg1.T, info)\nraw2 = mne.io.RawArray(eeg2.T, info)\n\nThe arguments to mne.create_info() set the number of channels in the data (25), the sampling frequency (250 Hz), and the channel types (the first 22 channels are EEG whereas the last three channels are EOG). We have to transpose the arrays, because MNE expects channels in rows and samples in columns.\n\n\nEstimating regression coefficients\nWe are now ready to estimate regression coefficients to remove the influence of ocular artifacts on the recorded EEG. It turns out that converting the three monopolar EOG channels EOG1, EOG2, and EOG3 to two bipolar channels is a good idea. One way to go about this is to multiply the three monopolar EOG signals with a suitable matrix to generate two bipolar channels EOG1–EOG2 and EOG3–EOG2. If you happen to have four monopolar EOG channels (which is also a common EOG recording setup), you can convert them into a horizontal and a vertical bipolar channel, respectively.\n\nbip = np.array([[1, -1, 0], [0, -1, 1]])\nraw1_eog = bip @ raw1[22:, :][0]\nraw2_eog = bip @ raw2[22:, :][0]\n\nThe @ operator computes the matrix product. Note that we have performed this conversion for both runs raw1 and raw2 separately. Furthermore, indexing a raw object returns a tuple with the requested data and associated time values. We only need the data, and [0] selects just the first entry in the returned tuple.\nFor shorter notation, we also create separate names for the EEG signals (the first 22 channels) of both runs:\n\nraw1_eeg = raw1[:22, :][0]\nraw2_eeg = raw2[:22, :][0]\n\nNow comes the important part where we perform ordinary least squares to estimate the linear regression coefficients. Note that we actually solve for all EEG channels simultaneously (or in other words, there are several dependent or response variables). We also have several independent or predictor variables in the form of EOG channels. Such a model is referred to as a multivariate (more than one response variable) multiple (more than one predictor variable) regression model.\nIf we denote our response variables (the EEG signals) as \\(Y\\), our predictor variables (the EOG signals) as \\(X\\), and the regression coefficients as \\(B\\), we can write the linear model as follows:\n\\[Y = X B\\]\nWe can then compute the least squares solution for \\(B\\) by left-multiplying with the Moore-Penrose inverse \\(X^+ = (X^T X)^{-1} X^T\\):\n\\[B = X^+ Y = (X^T X)^{-1} X^T Y\\]\nIn Python code, this looks as follows (note that we need to transpose our signals, because we have our channels in rows, whereas the equation assumes that they are in columns):\n\nb = np.linalg.inv(raw1_eog @ raw1_eog.T) @ raw1_eog @ raw1_eeg.T\n\nAlternatively, we can use the numerically more stable np.linalg.solve() to compute the regression coefficients:\n\nb = np.linalg.solve(raw1_eog @ raw1_eog.T, raw1_eog @ raw1_eeg.T)\n\nAs a quick sanity check, let’s inspect the shape of our regression parameter matrix b:\n\nb.shape\n\n(2, 22)\n\n\nThis makes sense, because there are two EOG channels (predictors) and 22 EEG channels (responses).\nNow all we need to do to remove ocular artifacts from new data is to subtract the estimated EOG influence raw2_eog.T @ b from the measured EEG. We’ll create a new raw3 object to store this corrected data.\n\neeg_corrected = (raw2_eeg.T - raw2_eog.T @ b).T\nraw3 = raw2.copy()\nraw3._data[:22, :] = eeg_corrected\n\n\n\nVisualizing results\nLet’s see if the method worked. First, we plot a segment of the original raw2 data with some prominent eye activity. For this visualization, we set the number of simultaneously visible channels to 25, the start of the plot to second 53, and the duration to 5 seconds.\n\nraw2.plot(n_channels=25, start=53, duration=5)\n\n\n\n\n\n\n\n\n\n\nWe produce the same plot for the corrected raw3 data:\n\nraw3.plot(n_channels=25, start=53, duration=5)\n\n\n\n\n\n\n\n\n\n\nIf you look closely, the method successfully removed ocular artifacts clearly visible around second 55."
  },
  {
    "objectID": "blog/removing-eog-regression/index.html#code",
    "href": "blog/removing-eog-regression/index.html#code",
    "title": "Removing eye activity from EEG via regression",
    "section": "Code",
    "text": "Code\n\n\nCode\nimport mne\nimport numpy as np\nfrom scipy.io import loadmat\n\nmat = loadmat(\"A01T.mat\", simplify_cells=True)\n\neeg1 = mat[\"data\"][2][\"X\"] * 1e-6  # run 3 (calibration)\neeg2 = mat[\"data\"][3][\"X\"] * 1e-6  # run 4 (experiment)\n\ninfo = mne.create_info(25, 250, ch_types=[\"eeg\"] * 22 + [\"eog\"] * 3)\nraw1 = mne.io.RawArray(eeg1.T, info)\nraw2 = mne.io.RawArray(eeg2.T, info)\n\nbip = np.array([[1, -1, 0], [0, -1, 1]])\nraw1_eog = bip @ raw1[22:, :][0]\nraw2_eog = bip @ raw2[22:, :][0]\nraw1_eeg = raw1[:22, :][0]\nraw2_eeg = raw2[:22, :][0]\n\nb = np.linalg.solve(raw1_eog @ raw1_eog.T, raw1_eog @ raw1_eeg.T)\n\neeg_corrected = (raw2_eeg.T - raw2_eog.T @ b).T\nraw3 = raw2.copy()\nraw3._data[:22, :] = eeg_corrected\n\nraw2.plot(n_channels=25, start=53, duration=5, title=\"Before\")\nraw3.plot(n_channels=25, start=53, duration=5, title=\"After\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "I am a senior postdoc with a background in electrical/biomedical engineering and computer engineering. I work in the Talent and Learning Research group at the Department of Psychology, University of Graz, Austria.\nI am interested in neuroscientific substrates of number processing and arithmetic, EEG oscillations and connectivity analysis, biomedical signal processing, applied machine learning and statistics, brain-computer interfaces, and software development. I am a big fan of open source software and I believe that science should be open as well, including data and analysis scripts. Python is my favorite language, but I also enjoy performing data analysis and statistical tests with R (thanks in particular to its awesome Tidyverse). I have also started to use and like Julia.\nI maintain and develop the following projects:\n\nMNELAB, a graphical user interface for processing EEG/MEG data using MNE\nSleepECG, a Python package for sleep staging using ECG\nHeartBeats.jl, a Pan-Tompkins R-peak ECG detector in Julia\nXDF.jl, a Julia package for reading XDF files\nSigViewer, a Qt/C++ based biosignal visualization tool\nSCoT, a Python package for EEG-based source connectivity estimation\n\nIn addition, I am part of the MNE, pybv, and pyxdf development teams, and I have contributed to numerous open source projects such as scikit-learn, pandas, IPython, PsychoPy, SciPy, Matplotlib, BioSig, and several smaller contributions for other projects (check out my GitHub profile for a detailed activity summary).\nI mainly work on macOS, but I also like Arch Linux, for which I maintain several AUR packages.\nCheck out my activity on GitHub and Mastodon, or contact me via email if you want to get in touch."
  },
  {
    "objectID": "blog/removing-google-fonts/index.html",
    "href": "blog/removing-google-fonts/index.html",
    "title": "Removing Google Fonts from a Quarto website",
    "section": "",
    "text": "Quarto is, according to its own description, a tool for scientific and technical publishing. Personally, I think that Quarto revolutionizes the process of scientific writing, because it can generate multiple output formats (such as HTML, PDF, Word, PowerPoint, Reveal.js, entire websites, books, and more) from a single Markdown-like source. The source can also contain code, which can be executed during build time, and the corresponding results can be included in the rendered output. This is similar to Jupyter notebooks and R Markdown documents, but more powerful and versatile (in fact, Quarto uses Jupyter or Knitr under the hood).\nIn this post, I will focus on generating a website (a single or multiple HTML documents) with Quarto. There are many built-in Bootswatch-based themes to choose from, and almost all of them use beautiful typefaces provided by Google Fonts. However, using fonts from a third-party server like Google Fonts can be problematic if the website does not ask for explicit consent as mandated by the General Data Protection Regulation (GDPR). In fact, a Quarto website does not show a consent banner by default, and even if enabled, it does not include a mechanism to enable/disable Google Fonts (because they are baked into most themes).\nI am neither a lawyer nor particularly interested in the intricacies of data privacy regulations. However, an Austrian lawyer recently sent out a series of cease and desist letters to thousands of website owners (both companies and private individuals), demanding that they pay 190 € for using Google Fonts without asking for explicit consent (thereby allegedly violating GDPR). Even though it is far from clear that these claims are even justified, I thought that it might be a good idea to get rid of Google Fonts and use fonts located directly on the website server instead. Besides avoiding any legal GDPR-related issues, fetching fonts from the same server the website is located on should also be faster (which is always a good thing for any website)."
  },
  {
    "objectID": "blog/removing-google-fonts/index.html#background",
    "href": "blog/removing-google-fonts/index.html#background",
    "title": "Removing Google Fonts from a Quarto website",
    "section": "",
    "text": "Quarto is, according to its own description, a tool for scientific and technical publishing. Personally, I think that Quarto revolutionizes the process of scientific writing, because it can generate multiple output formats (such as HTML, PDF, Word, PowerPoint, Reveal.js, entire websites, books, and more) from a single Markdown-like source. The source can also contain code, which can be executed during build time, and the corresponding results can be included in the rendered output. This is similar to Jupyter notebooks and R Markdown documents, but more powerful and versatile (in fact, Quarto uses Jupyter or Knitr under the hood).\nIn this post, I will focus on generating a website (a single or multiple HTML documents) with Quarto. There are many built-in Bootswatch-based themes to choose from, and almost all of them use beautiful typefaces provided by Google Fonts. However, using fonts from a third-party server like Google Fonts can be problematic if the website does not ask for explicit consent as mandated by the General Data Protection Regulation (GDPR). In fact, a Quarto website does not show a consent banner by default, and even if enabled, it does not include a mechanism to enable/disable Google Fonts (because they are baked into most themes).\nI am neither a lawyer nor particularly interested in the intricacies of data privacy regulations. However, an Austrian lawyer recently sent out a series of cease and desist letters to thousands of website owners (both companies and private individuals), demanding that they pay 190 € for using Google Fonts without asking for explicit consent (thereby allegedly violating GDPR). Even though it is far from clear that these claims are even justified, I thought that it might be a good idea to get rid of Google Fonts and use fonts located directly on the website server instead. Besides avoiding any legal GDPR-related issues, fetching fonts from the same server the website is located on should also be faster (which is always a good thing for any website)."
  },
  {
    "objectID": "blog/removing-google-fonts/index.html#implementation",
    "href": "blog/removing-google-fonts/index.html#implementation",
    "title": "Removing Google Fonts from a Quarto website",
    "section": "Implementation",
    "text": "Implementation\nEnough of all this legal mumbo jumbo, let’s get to the hopefully more interesting technical implementation. I will show how to remove Google Fonts from a minimal website project, which you can generate with the following command:\nquarto create-project mysite --type website\nThis will create a minimal template inside the mysite folder with the following four files:\n\n_quarto.yml\nabout.qmd\nindex.qmd\nstyles.css\n\nWe will need to create an additional file called custom.scss, which for the moment is just an empty file.\nHere’s the content of _quarto.yml, the global configuration file:\nproject:\n  type: website\n\nwebsite:\n  title: \"mysite\"\n  navbar:\n    left:\n      - href: index.qmd\n        text: Home\n      - about.qmd\n\nformat:\n  html:\n    theme: cosmo\n    css: styles.css\n    toc: true\nThe default theme is cosmo. We’ll use this theme in our example, but you can also replace it with another theme if you want. However, we need to adapt the theme in order to remove Google Fonts, so we change the theme: cosmo line to:\n    theme: [custom.scss, cosmo]\nThis will pull in stuff that we define in custom.scss later.\nNow let’s find out which fonts are downloaded from Google Fonts by rendering the website, which you can do by entering the following command in a terminal:\nquarto preview\nThis will open the website in your web browser. It should look something like this:\n\nThere are several ways to determine which Google Fonts are used, but since I am using Chrome I will only describe this process for this browser (but it should work similarly in Firefox, Edge, or whatever browser you are using):\n\nRight-click anywhere in your website and select “Inspect”. This will open the Developer Tools pane.\nSelect the “Network” tab.\nCheck the “3rd-party requests” box.\nRefresh the page.\nFind the requests that fetch fonts from Google, the font name should be contained somewhere in the name.\n\n\nIn this example, we can see that the website requests the “Source Sans Pro” font from Google Fonts. Now all we have to do is download this font, add it to our website folder, and redirect some font definitions.\nFirst, let’s download the font files from Google Fonts. We will not use the official website, because it is rather complicated to get the font in the right format. Instead, we will use google-webfonts-helper, a simpler interface that makes it very easy to download the desired fonts.\nOpen this website, and select “Source Sans Pro” in the left sidebar (you can narrow down the displayed fonts by typing “Source Sans Pro” into the text box in the top left corner). In addition to the “regular” style (which is already pre-selected), check at least “300” and “700” (these are the styles that are used by our example website according to the name of the request in the Developer Tools pane). It doesn’t hurt to download more styles, so I recommend to at least also check the italic variants of those three weights.\nIn the third section (“3. Copy CSS”), select “Modern Browsers”. Change the value of the “Customize folder prefix (optional)” textbox below to /fonts/ (so delete the preceding ..).\nThat’s it, now click on the download icon at the bottom to download the selected fonts as a zip file. Extract the contents of the file, and put all files into a folder called fonts located in your website root.\nIn addition, copy the CSS (displayed in the third section) and paste everything into the empty styles.css file.\nFinally, add the following lines to custom.scss:\n/*-- scss:rules --*/\n$web-font-path: false;  // disable Google Fonts\nYour website now uses the Source Sans Pro font from its own folder, which you can readily verify with the Developer Tools (the third-party request to Google Fonts is now gone)."
  },
  {
    "objectID": "blog/warming-stripes-r/index.html",
    "href": "blog/warming-stripes-r/index.html",
    "title": "Warming stripes with R",
    "section": "",
    "text": "Warming stripes are a popular way to represent temperature changes in a particular location over time. They are typically used to illustrate climate change caused by human activity (most notably by CO₂ emissions). The #ShowYourStripes website has warming stripes (in four variations) for selected regions, but it is much more interesting and fun to generate these plots with R – this is what the remainder of this post is about."
  },
  {
    "objectID": "blog/warming-stripes-r/index.html#background",
    "href": "blog/warming-stripes-r/index.html#background",
    "title": "Warming stripes with R",
    "section": "",
    "text": "Warming stripes are a popular way to represent temperature changes in a particular location over time. They are typically used to illustrate climate change caused by human activity (most notably by CO₂ emissions). The #ShowYourStripes website has warming stripes (in four variations) for selected regions, but it is much more interesting and fun to generate these plots with R – this is what the remainder of this post is about."
  },
  {
    "objectID": "blog/warming-stripes-r/index.html#data",
    "href": "blog/warming-stripes-r/index.html#data",
    "title": "Warming stripes with R",
    "section": "Data",
    "text": "Data\nWe are going to use NASA GISS Surface Temperature Analysis data (short GISTEMP), which contains surface temperature readings of almost 28,000 measurement stations across the globe. Each station provides at least 20 years of temperature data.\nLet’s choose my home town Graz, Austria as an example location for this post. We need to find out the corresponding closest weather station ID, which can be searched on the GISTEMP station data website. We can either hover over a red spot on the globe (which is a bit difficult to navigate) or use the search box at the bottom of the page. There, we can enter a name or coordinates and click “Search”, which will generate a list of possible matches. In either case, the station ID for Graz is AU000016402 (the first row in the search result list). Clicking on the station name opens a new page with more details, and at the bottom of that page we can download the temperature data as a CSV file.\nI wrote a short R function read_station() to make this process a little more convenient. All we need to know is the station ID and it will return the temperature data as a tibble:\n\nlibrary(readr)\n\nread_station = function(id) {\n    base = \"https://data.giss.nasa.gov/\"\n    # first trigger the creation of the CSV file\n    close(url(\n        sprintf(\"%scgi-bin/gistemp/stdata_show_v4.cgi?id=%s&ds=14&dt=1\", base, id),\n        open=\"r\"\n    ))\n    # then import the CSV file\n    read_delim(\n        sprintf(\"%stmp/gistemp/STATIONS_v4/tmp_%s_14_0_1/station.csv\", base, id),\n        na=\"999.90\"\n    )\n}\n\n\n\n\n\n\n\nNote\n\n\n\nThe CSV files are not permanently available, so I trigger their creation with the url() function before downloading.\n\n\nLet’s call this function to get our data:\n\n# get station ID at https://data.giss.nasa.gov/gistemp/station_data_v4_globe/\nid = \"AU000016402\"\n(df = read_station(id))\n\n# A tibble: 145 × 18\n    YEAR   JAN   FEB   MAR   APR   MAY   JUN   JUL   AUG   SEP   OCT   NOV   DEC\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  1880 -5.71 -3.71  2.39 11.0   11.7  15.5  19.2  16.2  14.0  9.29  2.89  1.38\n 2  1881 -6.62 -2.02  4.18  6.28  12.3  15.9  18.8  17.6  12.6  5.68  1.18 -1.02\n 3  1882 -1.82 -0.12  8.38  7.98  13.7  15.0  18.2  16.2  13.6  9.78  2.88 -0.02\n 4  1883 -3.42  0.28 -0.92  6.38  13.1  17.1  17.5  16.9  13.4  8.78  1.88 -2.62\n 5  1884 -1.62  0.08  4.98  7.68  13.7  14.0  19.0  16.7  13.8  7.18  0.48 -0.92\n 6  1885 -4.72 -0.12  4.88  9.58  11.4  17.0  19.3  16.4  14.2  7.98  3.28 -3.42\n 7  1886 -3.02 -0.52  0.08  9.18  13.1  15.8  18.2  17.4  15.2  9.58  3.28  0.08\n 8  1887 -3.62 -3.62  1.28  7.98  12.2  16.3  19.6  17.1  14.7  6.08  2.98 -2.43\n 9  1888 -4.73 -2.83  2.17  7.07  13.4  17.1  16.4  17.6  13.9  6.27  0.07 -1.63\n10  1889 -4.13 -2.52  1.77  7.87  15.4  18.4  17.6  16.5  11.5  9.67  1.57 -4.43\n# ℹ 135 more rows\n# ℹ 5 more variables: `D-J-F` &lt;dbl&gt;, `M-A-M` &lt;dbl&gt;, `J-J-A` &lt;dbl&gt;,\n#   `S-O-N` &lt;dbl&gt;, metANN &lt;dbl&gt;\n\n\nThe structure is pretty straightforward. A particular row corresponds to a year defined in the YEAR column, and the annual average temperature is contained in the metANN column. These are the only two columns we are going to use."
  },
  {
    "objectID": "blog/warming-stripes-r/index.html#line-chart",
    "href": "blog/warming-stripes-r/index.html#line-chart",
    "title": "Warming stripes with R",
    "section": "Line chart",
    "text": "Line chart\nLet’s start with a simple line chart showing annual temperatures over time. We will need the mean temperature between 1971 and 2000 for various plots, so let’s put that value into its own variable tmean:\n\n(tmean = mean(subset(df, YEAR &gt;= 1971 & YEAR &lt;= 2000)$metANN))\n\n[1] 8.783\n\n\nWe will also need the first and last available years (mainly for nice tick labels):\n\nstart = df$YEAR[1]\nstop = df$YEAR[nrow(df)]\n\nWe are now ready to create the time series plot:\n\nlibrary(ggplot2)\n\ntheme_set(theme_minimal())\n\nggplot(data=df, mapping=aes(x=YEAR, y=metANN)) +\n    geom_hline(yintercept=tmean, color=\"red\") +\n    geom_line() +\n    geom_point() +\n    geom_smooth() +\n    scale_x_continuous(breaks=seq(start, stop, 10)) +\n    labs(x=NULL, y=\"Temperature (°C)\", title=sprintf(\"Annual mean temperatures (%s)\", id))\n\n\n\n\n\n\n\n\nThe horizontal red line shows the mean temperature between 1971 and 2000, and the blue line is a local polynomial regression smoother which clearly indicates how quickly temperatures are rising for this location."
  },
  {
    "objectID": "blog/warming-stripes-r/index.html#warming-stripes",
    "href": "blog/warming-stripes-r/index.html#warming-stripes",
    "title": "Warming stripes with R",
    "section": "Warming stripes",
    "text": "Warming stripes\nWarming stripes visualize the same data in a different way. Instead of mapping annual temperatures to height on the y-axis, they color-code temperature to create a patch of colored stripes.\nWe are going to use the diverging 11-class Red/Blue colormap available in {RColorBrewer}.\n\nlibrary(RColorBrewer)\n\nThe plot is comprised of columns of constant height using geom_col(width=1), mapping their color fill to temperatures metANN. To set the colormap, we use scale_fill_gradientn() (note the n at the end of the function name, which creates a custom n-color gradient from our 11-class Red/Blue colormap) using the colormap generated with RColorBrewer::brewer.pal(). We have to reverse it to associate colder temperatures with blue and warmer temperatures with red.\n\nggplot(data=df, mapping=aes(x=YEAR, y=1, fill=metANN)) +\n    geom_col(width=1) +\n    scale_x_continuous(breaks=seq(start, stop, 10), expand=c(0, 0)) +\n    scale_y_continuous(expand=c(0, 0)) +\n    scale_fill_gradientn(colors=rev(brewer.pal(11, name=\"RdBu\")), na.value=\"gray\") +\n    labs(x=NULL, y=NULL, title=sprintf(\"Annual mean temperatures (%s)\", id), fill=\"T (°C)\") +\n    theme(axis.text.y=element_blank(), panel.grid=element_blank())\n\n\n\n\n\n\n\n\nNote that there is one missing temperature value for 1945, which is displayed in gray."
  },
  {
    "objectID": "blog/warming-stripes-r/index.html#enhanced-warming-stripes",
    "href": "blog/warming-stripes-r/index.html#enhanced-warming-stripes",
    "title": "Warming stripes with R",
    "section": "Enhanced warming stripes",
    "text": "Enhanced warming stripes\nThe beauty of warming stripes is that they convey important information without containing too many (distracting) details. However, if we wanted to show additional quantitative data, we can\n\nmap the height of each bar to the corresponding temperature,\nand include axis labels, tick labels, as well as a legend.\n\nIn addition, it might be interesting to plot temperature differences relative to the mean between 1971 and 2000. Here’s how to create this alternative version of the image:\n\ntmin = round(min(df$metANN - tmean, na.rm=TRUE))\ntmax = round(max(df$metANN - tmean, na.rm=TRUE))\n\nggplot(data=df, mapping=aes(x=YEAR, y=metANN - tmean, fill=metANN - tmean)) +\n    geom_col(width=1) +\n    scale_x_continuous(breaks=seq(start, stop, 10)) +\n    scale_y_continuous(breaks=seq(tmin, tmax, 0.5), expand=c(0, 0)) +\n    scale_fill_gradientn(colors=rev(brewer.pal(11, name=\"RdBu\")), na.value=\"gray\") +\n    labs(\n        x=NULL,\n        y=\"Temperature change (°C)\",\n        title=sprintf(\"Annual mean temperature changes relative to 1971–2000 (%s)\", id)\n    ) +\n    theme(panel.grid=element_blank(), legend.position=\"none\")\n\n\n\n\n\n\n\n\nIn my opinion, this representation is even more dramatic and shows, for example, that current temperatures in Graz, Austria, are already at least 1.5°C higher than the average between 1971 and 2000."
  },
  {
    "objectID": "blog/removing-eog-ica/index.html",
    "href": "blog/removing-eog-ica/index.html",
    "title": "Removing eye activity from EEG via ICA",
    "section": "",
    "text": "In a previous post, we used linear regression to remove ocular artifacts from EEG signals. Independent component analysis (ICA) is a popular alternative to this approach. In a nutshell, ICA decomposes multi-channel EEG recordings into maximally independent components. Components that represent ocular activity can be identified and eliminated to reconstruct artifact-free EEG signals. This approach is described in more detail in Jung et al. (2000).\nA comprehensive comparison between the two methods is beyond the scope of this post. Instead, I will only list some important properties of both techniques.\nFirst, the regression-based approach requires EOG channels, whereas ICA works without any reference signal. This means you can use ICA to remove ocular activity even if your recording does not include EOG channels.\nBoth methods potentially remove brain activity in addition to ocular activity. ICA can separate ocular components from brain components well if many EEG channels are available (which in turn requires a relatively large number of data samples). A cleaner separation also means that less brain activity will be removed when ocular components are eliminated. The minimum number of EEG channels required for ICA decomposition varies, but as a rule of thumb, at least 20 channels seem to be necessary (the EEGLAB tutorial has more details on the amount of data required for ICA). In contrast, the regression approach works fine even if only a few EEG channels are available. However, the EOG reference channels always contain some amount of brain activity, which will also be removed from the data in addition to ocular activity.\nThe ICA method entails manual identification of ocular components, although several algorithms exist to automate this process (for example EyeCatch or ADJUST). ICA also takes longer to compute than the regression approach (but efficient implementations are available that keep computation time to a minimum). Finally, ICA is an optimization problem that is not guaranteed to find the global optimal solution. Depending on the initial conditions, the algorithm might find different independent components from run to run. However, this is not a huge issue in this application, because ocular components are relatively large and therefore stable across decompositions.\nLet’s now turn to an example to see how ICA can be used to remove ocular artifacts with MNE."
  },
  {
    "objectID": "blog/removing-eog-ica/index.html#background",
    "href": "blog/removing-eog-ica/index.html#background",
    "title": "Removing eye activity from EEG via ICA",
    "section": "",
    "text": "In a previous post, we used linear regression to remove ocular artifacts from EEG signals. Independent component analysis (ICA) is a popular alternative to this approach. In a nutshell, ICA decomposes multi-channel EEG recordings into maximally independent components. Components that represent ocular activity can be identified and eliminated to reconstruct artifact-free EEG signals. This approach is described in more detail in Jung et al. (2000).\nA comprehensive comparison between the two methods is beyond the scope of this post. Instead, I will only list some important properties of both techniques.\nFirst, the regression-based approach requires EOG channels, whereas ICA works without any reference signal. This means you can use ICA to remove ocular activity even if your recording does not include EOG channels.\nBoth methods potentially remove brain activity in addition to ocular activity. ICA can separate ocular components from brain components well if many EEG channels are available (which in turn requires a relatively large number of data samples). A cleaner separation also means that less brain activity will be removed when ocular components are eliminated. The minimum number of EEG channels required for ICA decomposition varies, but as a rule of thumb, at least 20 channels seem to be necessary (the EEGLAB tutorial has more details on the amount of data required for ICA). In contrast, the regression approach works fine even if only a few EEG channels are available. However, the EOG reference channels always contain some amount of brain activity, which will also be removed from the data in addition to ocular activity.\nThe ICA method entails manual identification of ocular components, although several algorithms exist to automate this process (for example EyeCatch or ADJUST). ICA also takes longer to compute than the regression approach (but efficient implementations are available that keep computation time to a minimum). Finally, ICA is an optimization problem that is not guaranteed to find the global optimal solution. Depending on the initial conditions, the algorithm might find different independent components from run to run. However, this is not a huge issue in this application, because ocular components are relatively large and therefore stable across decompositions.\nLet’s now turn to an example to see how ICA can be used to remove ocular artifacts with MNE."
  },
  {
    "objectID": "blog/removing-eog-ica/index.html#implementation",
    "href": "blog/removing-eog-ica/index.html#implementation",
    "title": "Removing eye activity from EEG via ICA",
    "section": "Implementation",
    "text": "Implementation\nBefore we start, it is worth mentioning that ICA will generally run faster using a multi-threaded numeric library such as OpenBLAS. If you install the official NumPy package (see this post for more details on how to install Python packages), you will get OpenBLAS support out of the box.\n\nData preprocessing\nWe will use the same data set that we already used with the regression approach. Specifically, we’ll use participant A01T from data set 001-2014 from the BNCI Horizon 2020 website (check out the post on the regression approach for more details on this data set). To recap, download this file and save it to your working directory. Note that this data set contains 22 EEG and 3 EOG channels. Although EOG channels can (and should) be used for ICA decomposition (provided that they use the same reference electrode as the EEG channels), we will only use EEG channels here to keep things simple.\nAs always, we start by importing the necessary packages:\n\nfrom scipy.io import loadmat\nimport mne\n\nThe data comes as a MAT file, so we use scipy.io.loadmat() to load it as a NumPy array. Note that this time we only load the fourth run containing the actual experimental data – we do not need the calibration run.\n\nmat = loadmat(\"A01T.mat\", simplify_cells=True)\neeg = mat[\"data\"][3][\"X\"] * 1e-6  # convert to volts\n\nWe will plot ICA components as projections on the scalp surface later on. To this end, MNE needs to know the channel labels, which unfortunately are not present in the data. However, the data description contains a picture of the montage, which we can use to populate a list of channel names:\n\nch_names = [\n    \"Fz\",\n    \"FC3\",\n    \"FC1\",\n    \"FCz\",\n    \"FC2\",\n    \"FC4\",\n    \"C5\",\n    \"C3\",\n    \"C1\",\n    \"Cz\",\n    \"C2\",\n    \"C4\",\n    \"C6\",\n    \"CP3\",\n    \"CP1\",\n    \"CPz\",\n    \"CP2\",\n    \"CP4\",\n    \"P1\",\n    \"Pz\",\n    \"P2\",\n    \"POz\",\n    \"EOG1\",\n    \"EOG2\",\n    \"EOG3\",\n]\n\nWe create an info object using this list, which we need to create a Raw object containing the EEG and associated meta data (which in our case is just the sampling frequency of 250 Hz and the channel types). Finally, we add a 10–20 montage, which maps the channel labels to their locations on the scalp (this is required for topographic plots).\n\ninfo = mne.create_info(ch_names, 250, ch_types=[\"eeg\"] * 22 + [\"eog\"] * 3)\nraw = mne.io.RawArray(eeg.T, info)\nraw.set_montage(\"easycap-M1\")\n\n\n\nPerforming ICA\nICA does not work well in the presence of low-frequency drifts, so we create a copy of our raw object and apply a high-pass filter to this copy.\n\nraw_tmp = raw.copy()\nraw_tmp.filter(l_freq=1, h_freq=None)\n\n\n\n\n\n\n\nImportant\n\n\n\nIn the next step, we will use the Picard ICA algorithm, which is not included in MNE. Therefore, make sure to install it (see here for a quick overview of how to set up Python and install packages).\nPicard is much faster than the default Extended Infomax implementation shipped with MNE, and it can compute the same solution by setting fit_params={\"extended\": True, \"ortho\": False}. Alternatively, if you prefer the FastICA solution, set fit_params={\"extended\": True, \"ortho\": True}. More details are available here.\n\n\nWe are now ready to perform ICA. First, we instantiate an ICA object and specify that we want to use the Picard algorithm by setting method=\"picard\". In addition, the random_state argument should be set for reproducible results.\n\nica = mne.preprocessing.ICA(\n    method=\"picard\",\n    fit_params={\"extended\": True, \"ortho\": False},\n    random_state=1\n)\n\nNext, we fit ica to our filtered raw data raw_tmp (note that this uses only the 22 EEG channels and ignores the 3 EOG channels by default, but this could be changed with the picks argument).\n\nica.fit(raw_tmp)\n\n\n\nIdentifying ocular components\nOur next task is to identify ocular components among the computed independent components. This is usually done by visual inspection, so we start by plotting scalp projections of all components:\n\nica.plot_components(inst=raw_tmp, picks=range(22))\n\n\n\n\n\n\n\n\n\n\nLet’s focus on the first few components, because ocular components are generally found among these. From these scalp projections, the component labeled as ICA001 looks like it could represent eye movements due to its frontal location. To be sure, we can click on this component to open a new window with more details (this is possible because we specified inst=raw_tmp in the previous call):\nBesides the scalp projection, we can now also see\n\nthe component power spectral density (which is typical for ocular activity, because the characteristic EEG alpha peak is missing),\nthe epochs image, which color-codes the component activity over (virtual) epochs (which shows typical intermittent activity as blue and red stripes),\nand the epochs variance (which in this case is not really helpful in identifying the component).\n\nIn summary, we can be pretty sure that component ICA001 represents ocular activity. To be extra safe, let’s plot the component time courses to corroborate our assumption:\n\nica.plot_sources(inst=raw_tmp)\n\n\n\n\n\n\n\n\n\n\nIndeed, if you scroll through the data, ICA001 does primarily capture eye movements and eye blinks.\nNote that very often, two ocular components can be found in the decomposition, but this is not the case in our example data (all remaining components do not seem to originate from eye activity).\n\n\nRemoving ocular components\nIn the final step, we create a list attribute ica.exclude containing the indices of all components that should be removed when reconstructing EEG signals. In our case, this list contains only a single component:\n\nica.exclude = [1]\n\nNote that you can click on the component title (ICA001) in the ICA components plot to include/exclude a component (the title of an excluded component will turn gray). This will also add/remove this component from/to the underlying ica.exclude attribute.\nNow we can apply our ICA decomposition (without the excluded component) to a copy of the original (unfiltered) EEG to obtain artifact-free signals:\n\nraw_corrected = raw.copy()\nica.apply(raw_corrected)\n\n\n\nVisualizing results\nSo how did ICA perform? Let’s take a look at a segment of the original EEG containing a clear eye movement artifact:\n\nraw.plot(n_channels=25, start=53, duration=5)\n\n\n\n\n\n\n\n\n\n\nAnd here is the corrected signal:\n\nraw_corrected.plot(n_channels=25, start=53, duration=5)\n\n\n\n\n\n\n\n\n\n\nLooks like ICA did a pretty decent job in removing eye artifacts."
  },
  {
    "objectID": "blog/removing-eog-ica/index.html#code",
    "href": "blog/removing-eog-ica/index.html#code",
    "title": "Removing eye activity from EEG via ICA",
    "section": "Code",
    "text": "Code\n\n\nCode\nfrom scipy.io import loadmat\nimport mne\n\nmat = loadmat(\"A01T.mat\", simplify_cells=True)\n\neeg = mat[\"data\"][3][\"X\"] * 1e-6  # convert to volts\n\nch_names = [\n    \"Fz\",\n    \"FC3\",\n    \"FC1\",\n    \"FCz\",\n    \"FC2\",\n    \"FC4\",\n    \"C5\",\n    \"C3\",\n    \"C1\",\n    \"Cz\",\n    \"C2\",\n    \"C4\",\n    \"C6\",\n    \"CP3\",\n    \"CP1\",\n    \"CPz\",\n    \"CP2\",\n    \"CP4\",\n    \"P1\",\n    \"Pz\",\n    \"P2\",\n    \"POz\",\n    \"EOG1\",\n    \"EOG2\",\n    \"EOG3\",\n]\n\ninfo = mne.create_info(ch_names, 250, ch_types=[\"eeg\"] * 22 + [\"eog\"] * 3)\nraw = mne.io.RawArray(eeg.T, info)\nraw.set_montage(\"easycap-M1\")\n\nraw_tmp = raw.copy()\nraw_tmp.filter(l_freq=1, h_freq=None)\nica = mne.preprocessing.ICA(\n    method=\"picard\",\n    fit_params={\"extended\": True, \"ortho\": False},\n    random_state=1\n)\nica.fit(raw_tmp)\n\nica.plot_components(inst=raw_tmp, picks=range(22))\nica.plot_sources(inst=raw_tmp)\nica.exclude = [1]\n\nraw_corrected = raw.copy()\nica.apply(raw_corrected)\n\nraw.plot(n_channels=25, start=53, duration=5, title=\"Before\")\nraw_corrected.plot(n_channels=25, start=53, duration=5, title=\"After\")"
  },
  {
    "objectID": "blog/visualizing-eeg-data/index.html",
    "href": "blog/visualizing-eeg-data/index.html",
    "title": "Visualizing EEG data",
    "section": "",
    "text": "In this tutorial, we will continue to use the EEG motor movement/imagery data set from the previous post (again, we use run 4 of subject 1). I’ll quickly reiterate all steps we have performed so far:\n\nimport mne\n\nraw = mne.io.read_raw(\"S001R04.edf\", preload=True)\nraw.rename_channels(lambda s: s.strip(\".\"))\nraw.drop_channels([\"T9\", \"T10\"])\nraw.set_montage(\"easycap-M1\", match_case=False)\nraw.set_eeg_reference(\"average\")"
  },
  {
    "objectID": "blog/visualizing-eeg-data/index.html#prerequisites",
    "href": "blog/visualizing-eeg-data/index.html#prerequisites",
    "title": "Visualizing EEG data",
    "section": "",
    "text": "In this tutorial, we will continue to use the EEG motor movement/imagery data set from the previous post (again, we use run 4 of subject 1). I’ll quickly reiterate all steps we have performed so far:\n\nimport mne\n\nraw = mne.io.read_raw(\"S001R04.edf\", preload=True)\nraw.rename_channels(lambda s: s.strip(\".\"))\nraw.drop_channels([\"T9\", \"T10\"])\nraw.set_montage(\"easycap-M1\", match_case=False)\nraw.set_eeg_reference(\"average\")"
  },
  {
    "objectID": "blog/visualizing-eeg-data/index.html#interactive-visualization",
    "href": "blog/visualizing-eeg-data/index.html#interactive-visualization",
    "title": "Visualizing EEG data",
    "section": "Interactive visualization",
    "text": "Interactive visualization\nWe can now open an interactive visualization window as follows:\n\nraw.plot()\n\n\nThe signals do not look quite right yet, so let’s walk through some interactive features of this plot. First, you can rescale the signals with the + and - keys. The default scaling is not really appropriate here, but pressing the - key a few times will result in a much nicer display. The topmost channel label Fc5 includes a purple scale bar, which indicates the current scale of the data (the length of the purple bar corresponds to 40 µV in this example). This scale applies to all channels of the same type (such as all EEG channels). If the scale bar gets in your way, you can toggle it with the s key.\nNotice that 20 channels are visible per page by default. To view the remaining channels, you can scroll down with the ↓ key. Conversely, you can scroll back up again by pressing the ↑ key. Alternatively, you can increase or decrease the number of channels visible on a page with the Page Up or Page Down keys, respectively (if you don’t have these keys on your Mac, you can use the combinations fn↑ and fn↓ instead).\nBy default, 10 seconds of data are visible per page. If you want to navigate in time, you can use the → and ← keys to move forward and backward by a quarter of the visible duration (in this example by 2.5 seconds). Pressing Shift→ and Shift← will scroll forward/backward by an entire page (10 s here).\nYou can increase or decrease the amount of time shown per page with the End or Home keys (if you don’t have these keys on your Mac, these shortcuts can usually be accessed with fn→ and fn←).\nThe following table summarizes important keyboard shortcuts for the interactive visualization window (we will discuss annotation, butterfly, and zen modes in a minute). Note that the “Help” button in the lower left corner (or pressing the ? key) pops up a dialog window with these shortcuts.\n\n\n\nAction\nKeyboard shortcut\n\n\n\n\nScale up\n+\n\n\nScale down\n-\n\n\nScroll up\n↑\n\n\nScroll down\n↓\n\n\nScroll left (quarter page)\n←\n\n\nScroll right (quarter page)\n→\n\n\nScroll left (whole page)\nShift←\n\n\nScroll right (whole page)\nShift→\n\n\nMore channels\nPage Up\n\n\nFewer channels\nPage Down\n\n\nMore time\nEnd\n\n\nLess time\nHome\n\n\nScale bars\ns\n\n\nZen mode (toggle scrollbars)\nz\n\n\nDC removal\nd\n\n\nAnnotation mode\na\n\n\nButterfly mode\nb\n\n\nDraggable annotations\np\n\n\nHelp\n?\n\n\n\nYou can change initial settings with arguments to raw.plot(). For example, we might use the following arguments to start with suitable values for our example data:\n\nraw.plot(n_channels=64, duration=5, scalings={\"eeg\": 75e-6}, start=10)\n\nThe example data contains annotations (stored in raw.annotations), which are visualized as colored patches (the annotation descriptions are located at the top of the plot). In this case, the blue, red, and green patches correspond to T0, T1, and T2, respectively.\nIn addition to keyboard shortcuts, you can also use the mouse to navigate through the data. You might have already noticed the horizontal bar below the plot. This overview bar summarizes the entire time range of the signal, and the currently visible time segment is highlighted by a gray box. You can click inside the overview bar to quickly skip around to different time segments. Furthermore, the overview bar also shows annotations.\nTo quickly identify time segments with high variance, you can switch to butterfly mode by pressing the b key. Signals of all channels of the same type will be collapsed onto each other, which makes it easy to spot abnormal activity. Press b again to exit butterfly mode.\n\nIf you want to focus on the actual signals with fewer distractions, you can get rid of the scroll and overview bars. Press z (zen mode) to toggle these user interface elements.\nFinally, you can quickly filter out offsets (slow drifts) in the signals by pressing the d key (toggle DC removal)."
  },
  {
    "objectID": "blog/visualizing-eeg-data/index.html#annotating-segments",
    "href": "blog/visualizing-eeg-data/index.html#annotating-segments",
    "title": "Visualizing EEG data",
    "section": "Annotating segments",
    "text": "Annotating segments\nWe are now ready to interactively create annotations, for example to mark segments containing artifacts. To this end, we switch to annotation mode by pressing the a key, which will open a dialog window.\n\n\n\n\n\n\nImportant\n\n\n\nMake sure to keep this dialog window open as long as you want to stay in annotation mode – if you close it, you will return to normal visualization mode.\n\n\nThis annotations dialog shows a list of currently available annotations (T0, T1, and T2 in our example data). The annotation label marked with a filled circle is currently active (T0), which means that if we create a new annotation in the data window (more on this soon), it will be of this type. However, we can also create new annotation types. Notice that the “BAD_” label is actually an input text field – you can start typing/editing to change the annotation label before creating it. We would like to create annotations called “BAD”, so let’s remove the underscore by pressing the Backspace key once. The new annotation type gets added to the list of available labels once you click on the large “Add new label” button or hit the Enter key.\n\n\n\n\n\n\n\nTip\n\n\n\nAnnotation labels starting with “BAD” (or “bad”) are special in MNE, because many functions ignore data marked by such annotations. This gives us a nice way to mark artifact segments without completely discarding data points.\n\n\nLet’s create some “BAD” annotations in our example data. First, we make sure that the “BAD” label is active by clicking inside the red ring (active labels are represented by a filled circle). Optionally, we can hide all T0, T1, and T2 annotations by clicking on their “show/hide” checkboxes. Now we switch back to the main visualization window (again, make sure not to close the annotation window). Using the mouse, we can now click and drag to select a portion of the data and mark it with the currently active “BAD” label. During this process, you can interactively scroll around in the data with the keyboard shortcuts listed before, or you can click within the overview bar below the plot.\nIf you want to edit the start and/or end point of an existing annotation, press the p key or click the “Draggable edges” checkbox to toggle snap mode. If snap mode is enabled, you can drag the start and/or end points of an annotation to the desired locations. It is impossible to create an annotation inside another annotation with snap mode enabled, so that’s when you want to turn it off. Overlapping annotations of the same type will be merged to a single annotation. If you want to completely delete an annotation, you can simply right-click on it.\nWhen you are done, you can close the annotation window. You can also close the visualization window, because all annotations are automatically stored in the raw.annotations attribute. Let’s take a look what it looks like now:\n\nraw.annotations\n\n&lt;Annotations | 39 segments: BAD (9), T0 (15), T1 (8), T2 (7)&gt;\nYay, we’ve just created 9 new annotations, all of which are interpreted as bad (because they start with the word “BAD” or “bad”)."
  },
  {
    "objectID": "blog/visualizing-eeg-data/index.html#marking-channels",
    "href": "blog/visualizing-eeg-data/index.html#marking-channels",
    "title": "Visualizing EEG data",
    "section": "Marking channels",
    "text": "Marking channels\nSometimes, signals are noisy in particular channels during the entire recording. In such cases, it is possible to mark the whole channel as bad. In the visualization window, click on a channel label on the left side of the plot or on a channel trace to mark a channel as bad (the label and associated time course will turn gray to reflect this). If you want to unmark a channel, click on its label (or its trace) again. The selection of bad channels is immediately stored in raw.info[\"bads\"] (a list containing the channel labels of all bad channels). Channels marked as bad are generally ignored by MNE functions."
  },
  {
    "objectID": "blog/visualizing-eeg-data/index.html#saving-and-loading-annotations",
    "href": "blog/visualizing-eeg-data/index.html#saving-and-loading-annotations",
    "title": "Visualizing EEG data",
    "section": "Saving and loading annotations",
    "text": "Saving and loading annotations\nAlthough changes to annotations or bad channels are immediately reflected in raw.annotations and raw.info[\"bads\"], we still need a way to persistently store this information for later use. Consequently, we will also need a way to load this information and associate it with an existing raw object.\nIt is always a good idea to use simple storage formats whenever possible. Therefore, MNE allows us to store annotations as simple text files. Here’s how we could save annotations in a file called S001R04_annotations.txt:\nraw.annotations.save(\"S001R04_annotations.txt\", overwrite=True)\nThis creates a comma-separated text file with three columns “onset”, “duration”, and “description”. Onsets are relative to the start of the recording (so they start at zero), and both onsets and durations are measured in seconds.\n\n\n\n\n\n\nTip\n\n\n\nIf you specify a .csv instead of a .txt extension, you will get absolute time stamps for the onsets (based on the measurement onset time stamp).\n\n\nWe can read this file with:\nannotations = mne.read_annotations(\"S001R04_annotations.txt\")\nFinally, we can associate these annotations with an existing raw object:\nraw.set_annotations(annotations)"
  },
  {
    "objectID": "blog/visualizing-eeg-data/index.html#saving-and-loading-bad-channels",
    "href": "blog/visualizing-eeg-data/index.html#saving-and-loading-bad-channels",
    "title": "Visualizing EEG data",
    "section": "Saving and loading bad channels",
    "text": "Saving and loading bad channels\nThere is no built-in function to save bad channels to a file, but we can use the following code snippet to store the list of bad channels in a text file called S001R04_bads.txt:\nwith open(\"S001R04_bads.txt\", \"w\") as f:\n    f.write(\",\".join(raw.info[\"bads\"]))\n    f.write(\"\\n\")\nTo load this file and update an existing raw object, we can use:\nwith open(\"S001R04_bads.txt\") as f:\n    raw.info[\"bads\"] = f.read().strip().split(\",\")"
  },
  {
    "objectID": "blog/r-dataframe-subsetting/index.html",
    "href": "blog/r-dataframe-subsetting/index.html",
    "title": "Subsetting data frames in R",
    "section": "",
    "text": "Data frames are one of the most important data structures for representing tabular data. Base R includes the tried and tested data.frame type, which is technically a list of equal-length vectors (where each vector corresponds to a column in the data frame). The tibble package (part of the Tidyverse) offers a slightly tweaked data frame type called tibble. In practical data analysis pipelines, we frequently create subsets of the data frame, for example by selecting one or more columns and/or rows. In most situations, the data.frame and tibble types are interchangeable. However, there are subtle differences in the context of subsetting, which I will highlight in this post.\nSubsetting data frames is slightly more challenging than subsetting vectors (see this post), mainly because there is a multitude of available (and partly redundant) options. We’ll start with a small data frame df consisting of four rows and three columns:\n\n(df = data.frame(first=1:4, second=seq(pi, pi + 3), third=LETTERS[1:4]))\n\n  first   second third\n1     1 3.141593     A\n2     2 4.141593     B\n3     3 5.141593     C\n4     4 6.141593     D\n\n\nThis is a classic data.frame, so let’s also create a tibble with identical contents:\n\n(tf = tibble::as_tibble(df))\n\n# A tibble: 4 × 3\n  first second third\n  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;\n1     1   3.14 A    \n2     2   4.14 B    \n3     3   5.14 C    \n4     4   6.14 D"
  },
  {
    "objectID": "blog/r-dataframe-subsetting/index.html#introduction",
    "href": "blog/r-dataframe-subsetting/index.html#introduction",
    "title": "Subsetting data frames in R",
    "section": "",
    "text": "Data frames are one of the most important data structures for representing tabular data. Base R includes the tried and tested data.frame type, which is technically a list of equal-length vectors (where each vector corresponds to a column in the data frame). The tibble package (part of the Tidyverse) offers a slightly tweaked data frame type called tibble. In practical data analysis pipelines, we frequently create subsets of the data frame, for example by selecting one or more columns and/or rows. In most situations, the data.frame and tibble types are interchangeable. However, there are subtle differences in the context of subsetting, which I will highlight in this post.\nSubsetting data frames is slightly more challenging than subsetting vectors (see this post), mainly because there is a multitude of available (and partly redundant) options. We’ll start with a small data frame df consisting of four rows and three columns:\n\n(df = data.frame(first=1:4, second=seq(pi, pi + 3), third=LETTERS[1:4]))\n\n  first   second third\n1     1 3.141593     A\n2     2 4.141593     B\n3     3 5.141593     C\n4     4 6.141593     D\n\n\nThis is a classic data.frame, so let’s also create a tibble with identical contents:\n\n(tf = tibble::as_tibble(df))\n\n# A tibble: 4 × 3\n  first second third\n  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;\n1     1   3.14 A    \n2     2   4.14 B    \n3     3   5.14 C    \n4     4   6.14 D"
  },
  {
    "objectID": "blog/r-dataframe-subsetting/index.html#selecting-a-single-column",
    "href": "blog/r-dataframe-subsetting/index.html#selecting-a-single-column",
    "title": "Subsetting data frames in R",
    "section": "Selecting a single column",
    "text": "Selecting a single column\nIn the following examples, we will explore different options to select the second column (named \"second\").\n\nThe $ operator\nWe’ll start with the $ operator, which extracts a single column by name as follows:\n\ndf$second  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ndf$\"second\"  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\n\nWe can enclose the desired column name in quotes, but the first variant without quotes is more common. In either case, R returns the single column as a basic vector. This is also true when working with a tibble:\n\ntf$second  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ntf$\"second\"  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\n\nThe $ notation is convenient for interactive exploration, because we don’t have to type a lot of extra characters (except for the $ sign). In addition, RStudio offers auto-completion of matching column names in its console.\n\n\n\n\n\n\nImportant\n\n\n\nSubsetting a data.frame with $ performs partial matching. This means that R will return the first column that partially matches the given name, for example:\n\ndf$s  # extracts column \"second\"\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\n\nR will happily return df$second in this example. You can learn more about $ by typing ?`$` in the interactive console.\n\n\nThe $ operator applied to a tibble does not perform partial matching. Instead, the following example will result in NULL and raise a warning:\n\ntf$s  # returns NULL (no partial matching)\n\nWarning: Unknown or uninitialised column: `s`.\n\n\nNULL\n\n\nIt is easy to shoot yourself in the foot with partial matching. Therefore, I advise against using the $ notation when working with data.frame objects.\n\n\nThe [[]] operator\nAnother way to select a single column uses double square brackets notation [[]]. We can specify either the position or the name of the desired column:\n\ndf[[2]]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ndf[[\"second\"]]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ntf[[2]]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ntf[[\"second\"]]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\n\nBoth data.frame and tibble objects return the desired column as a vector.\n\n\n\n\n\n\nNote\n\n\n\nIf you really want, you can enable partial matching for data.frame types as follows (but you probably don’t want to do this):\n\ndf[[\"s\", exact=FALSE]]  # vector (partial matching)\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\n\n\n\n\n\nThe [] operator\nInterestingly, we can also select a single column with single square bracket notation [], which we’ve already seen with atomic vectors:\n\ndf[2]  # data.frame\n\n    second\n1 3.141593\n2 4.141593\n3 5.141593\n4 6.141593\n\ndf[\"second\"]  # data.frame\n\n    second\n1 3.141593\n2 4.141593\n3 5.141593\n4 6.141593\n\ntf[2]  # tibble\n\n# A tibble: 4 × 1\n  second\n   &lt;dbl&gt;\n1   3.14\n2   4.14\n3   5.14\n4   6.14\n\ntf[\"second\"]  # tibble\n\n# A tibble: 4 × 1\n  second\n   &lt;dbl&gt;\n1   3.14\n2   4.14\n3   5.14\n4   6.14\n\n\nThe important difference here is that the resulting subset is a data frame (depending on the original type either a data.frame or a tibble) and not a vector, even though we select only a single column."
  },
  {
    "objectID": "blog/r-dataframe-subsetting/index.html#selecting-multiple-columns",
    "href": "blog/r-dataframe-subsetting/index.html#selecting-multiple-columns",
    "title": "Subsetting data frames in R",
    "section": "Selecting multiple columns",
    "text": "Selecting multiple columns\nIf we want to select multiple columns, we have to use single square bracket notation []. We can specify both a row selection and a column selection, separated by a comma, within the square brackets. However, we can omit either or both indices to select entire columns or rows.\nLet’s start with selecting a single column. For example, we can grab the second column by omitting the row selection:\n\ndf[, 2]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ndf[, \"second\"]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ntf[, 2]  # tibble\n\n# A tibble: 4 × 1\n  second\n   &lt;dbl&gt;\n1   3.14\n2   4.14\n3   5.14\n4   6.14\n\ntf[, \"second\"]  # tibble\n\n# A tibble: 4 × 1\n  second\n   &lt;dbl&gt;\n1   3.14\n2   4.14\n3   5.14\n4   6.14\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA data.frame will return the column as a vector, whereas a tibble will return a tibble (with a single column):\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen selecting a single column, we can set the returned value to be a vector or a single-column data frame with the drop argument (drop=TRUE means vector, whereas drop=FALSE means data frame):\n\ndf[, 2, drop=FALSE]  # data.frame\n\n    second\n1 3.141593\n2 4.141593\n3 5.141593\n4 6.141593\n\ndf[, \"second\", drop=FALSE]  # data.frame\n\n    second\n1 3.141593\n2 4.141593\n3 5.141593\n4 6.141593\n\ntf[, 2, drop=TRUE]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\ntf[, \"second\", drop=TRUE]  # vector\n\n[1] 3.141593 4.141593 5.141593 6.141593\n\n\nI’ve rarely seen this in practice, so I don’t recommend using it unless there is no other option.\n\n\nIn contrast to $ and [[]], single square bracket notation [] allows us to select multiple columns:\n\ndf[, c(\"second\", \"third\")]\n\n    second third\n1 3.141593     A\n2 4.141593     B\n3 5.141593     C\n4 6.141593     D\n\ndf[, c(2, 3)]\n\n    second third\n1 3.141593     A\n2 4.141593     B\n3 5.141593     C\n4 6.141593     D\n\ndf[c(\"second\", \"third\")]\n\n    second third\n1 3.141593     A\n2 4.141593     B\n3 5.141593     C\n4 6.141593     D\n\ndf[c(2, 3)]\n\n    second third\n1 3.141593     A\n2 4.141593     B\n3 5.141593     C\n4 6.141593     D\n\ntf[, c(\"second\", \"third\")]\n\n# A tibble: 4 × 2\n  second third\n   &lt;dbl&gt; &lt;chr&gt;\n1   3.14 A    \n2   4.14 B    \n3   5.14 C    \n4   6.14 D    \n\ntf[, c(2, 3)]\n\n# A tibble: 4 × 2\n  second third\n   &lt;dbl&gt; &lt;chr&gt;\n1   3.14 A    \n2   4.14 B    \n3   5.14 C    \n4   6.14 D    \n\ntf[c(\"second\", \"third\")]\n\n# A tibble: 4 × 2\n  second third\n   &lt;dbl&gt; &lt;chr&gt;\n1   3.14 A    \n2   4.14 B    \n3   5.14 C    \n4   6.14 D    \n\ntf[c(2, 3)]\n\n# A tibble: 4 × 2\n  second third\n   &lt;dbl&gt; &lt;chr&gt;\n1   3.14 A    \n2   4.14 B    \n3   5.14 C    \n4   6.14 D    \n\n\nThe returned subset will always be a data frame.\n\n\n\n\n\n\nTip\n\n\n\nA tibble is more consistent than a data.frame when using []-style subsetting, because the result will always be a tibble. In contrast, we get a vector when selecting a single column and a data.frame when selecting multiple columns with data.frame objects."
  },
  {
    "objectID": "blog/r-dataframe-subsetting/index.html#selecting-rows",
    "href": "blog/r-dataframe-subsetting/index.html#selecting-rows",
    "title": "Subsetting data frames in R",
    "section": "Selecting rows",
    "text": "Selecting rows\nSelecting one or more rows is also known as filtering. We use the row index (the value before the comma) within single square brackets [] to create the desired subset. The result will always be a data frame.\nFor example, we can select the second row as follows (don’t forget the trailing comma):\n\ndf[2, ]  # data.frame\n\n  first   second third\n2     2 4.141593     B\n\ntf[2, ]  # tibble\n\n# A tibble: 1 × 3\n  first second third\n  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;\n1     2   4.14 B    \n\n\nSimilarly, we can also select multiple rows:\n\ndf[c(2, 3), ]  # data.frame\n\n  first   second third\n2     2 4.141593     B\n3     3 5.141593     C\n\ntf[c(2, 3), ]  # tibble\n\n# A tibble: 2 × 3\n  first second third\n  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;\n1     2   4.14 B    \n2     3   5.14 C    \n\n\nLogical subsetting is especially useful for filtering rows. The following example creates a subset by selecting rows where the values in the second column are greater than 5:\n\ndf[df[, 2] &gt; 5, ]\n\n  first   second third\n3     3 5.141593     C\n4     4 6.141593     D\n\ntf[tf[, 2] &gt; 5, ]\n\n# A tibble: 2 × 3\n  first second third\n  &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt;\n1     3   5.14 C    \n2     4   6.14 D"
  }
]