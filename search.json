[{"categories":null,"content":"Introduction Whitening (also known as sphering) is a linear transformation used for decorrelating signals. Applied to EEG, this means that the original channel time series (which tend to be highly correlated) are transformed into uncorrelated signals with identical variances. The term whitening is derived from white noise (which in turn draws its name from white light), which consists of serially uncorrelated samples. Whitening thus transforms a random vector into a white noise vector with uncorrelated components.\nTheoretically, there are infinitely many possibilities to perform a whitening transformation. We will explore two popular whitening methods in more detail, namely principal component analysis (PCA) and zero-phase component analysis (ZCA), which was introduced by Bell and Sejnowski (1997). These methods are commonly used in EEG/MEG analysis as a preprocessing step prior to independent component analysis (ICA) (see this previous post on how to remove ocular artifacts with ICA). If you want to delve into the matter more deeply, Kessy et al. (2018) discuss even more possible whitening methods.\nMathematically, whitening transforms a random vector $\\mathbf{x}$ (the original EEG channel time series) into a random vector $\\mathbf{z}$ using a whitening matrix $\\mathbf{W}$:\n$$\\mathbf{z} = \\mathbf{W} \\mathbf{x}$$\nImportantly, the original covariance matrix $\\text{cov}(\\mathbf{x}) = \\mathbf{\\Sigma}$ becomes $\\text{cov}(\\mathbf{z}) = \\mathbf{I}$ after the transformation – the identity matrix. This means that all components of $\\mathbf{z}$ have unit variance and all correlations have been removed.\nToy data To illustrate the differences between PCA and ZCA whitening, let’s create some toy data. Specifically, we generate 1000 samples of two correlated time series $x_1$ and $x_2$.\nimport numpy as np import matplotlib.pyplot as plt np.random.seed(1) mu = [0, 0] sigma = [[5, 4], [4, 5]] # must be positive semi-definite n = 1000 x = np.random.multivariate_normal(mu, sigma, size=n).T The two time series are stored in the NumPy array x of shape (2, 1000).\nFor visualization purposes that will become clear in a moment, we determine the 20 most extreme values and denote their indices as set1 (the indices of the remaining data points are stored in set2):\nset1 = np.argsort(np.linalg.norm(x, axis=0))[-20:] set2 = list(set(range(n)) - set(set1)) Let’s now plot all values of $x_1$ versus their corresponding values of $x_2$.\nfig, ax = plt.subplots() ax.scatter(x[0, set1], x[1, set1], s=20, c=\"red\", alpha=0.2) ax.scatter(x[0, set2], x[1, set2], s=20, alpha=0.2) ax.set_aspect(\"equal\") ax.set_xlim(-8, 8) ax.set_ylim(-8, 8) ax.set_xlabel(\"$x_1$\") ax.set_ylabel(\"$x_2$\") ax.spines[\"top\"].set_visible(False) ax.spines[\"right\"].set_visible(False) ax.set_title(\"Original\") Clearly, these two time series appear to be highly correlated. The ellipsoidal shape of the scatter plot indicates that as the values of $x_1$ increase, the values of $x_2$ also tend to increase. Indeed, the Pearson correlation between $x_1$ and $x_2$ is 0.80 and can be computed with:\nnp.corrcoef(x)[0, 1] The red dots indicate the most extreme values, and we will observe how these points are transformed by the subsequent whitening procedures.\nEigendecomposition Both PCA and ZCA are based on eigenvectors and eigenvalues of the (empirical) covariance matrix. In particular, the covariance matrix can be decomposed into its eigenvectors $\\mathbf{U}$ and eigenvalues $\\mathbf{\\Lambda}$ as:\n$$\\mathbf{\\Sigma} = \\mathbf{U} \\mathbf{\\Lambda} \\mathbf{U}^T$$\nLet’s compute these quantities for our toy data:\nsigma = np.cov(x) evals, evecs = np.linalg.eigh(sigma) Note that we now use the empirical covariance matrix derived from the data instead of the true covariance matrix, which is generally unknown. Furthermore, note that since a covariance matrix is always symmetric, we can use the optimized np.linalg.eigh function instead of the more generic np.linalg.eig version (this also makes sure that we will always get real eigenvalues instead of complex ones). Alternatively, we could also use np.linalg.svd directly on the data x (instead of the covariance matrix) to compute the eigenvectors and eigenvalues, which can be numerically more stable in some situations.\nWhitening with PCA The whitening matrix $\\mathbf{W}^{\\mathrm{PCA}}$ for PCA can be written as:\n$$\\mathbf{W}^{\\mathrm{PCA}} = \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T$$\nThis means that the data can be transformed as follows:\n$$\\mathbf{z} = \\mathbf{W}^{\\mathrm{PCA}} \\mathbf{x} = \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T \\mathbf{x}$$\nTherefore, we can whiten our toy data accordingly:\nz = np.diag(evals**(-1/2)) @ evecs.T @ x Let’s see how our transformed toy data looks like in a scatter plot:\nfig, ax = plt.subplots() ax.scatter(z[0, set1], z[1, set1], s=20, c=\"red\", alpha=0.2) ax.scatter(z[0, set2], z[1, set2], s=20, alpha=0.2) ax.set_aspect(\"equal\") ax.set_xlim(-8, 8) ax.set_ylim(-8, 8) ax.set_xlabel(\"$z_1$\") ax.set_ylabel(\"$z_2$\") ax.spines[\"top\"].set_visible(False) ax.spines[\"right\"].set_visible(False) ax.set_title(\"PCA\") Clearly, the transformation removed the correlation between the two time series, because the scatter plot now looks like a sphere (a circle in two dimensions) – hence the name sphering. Indeed, the correlation coefficient (np.corrcoef(z)[0, 1]) yields a value practically equal to zero. Importantly, PCA has rotated all data points as illustrated by the new positions of the red dots; these do not lie on a diagonal with roughly 45 degrees anymore, but are now aligned with the vertical axis.\nWhitening with ZCA The whitening matrix $\\mathbf{W}^{\\mathrm{ZCA}}$ for ZCA can be written as:\n$$\\mathbf{W}^{\\mathrm{ZCA}} = \\mathbf{U} \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T$$\nIn fact, this transformation looks almost like PCA whitening, but with an additional rotation by $\\mathbf{U}$. Again, the original data can be transformed as follows:\n$$\\mathbf{z} = \\mathbf{W}^{\\mathrm{ZCA}} \\mathbf{x} = \\mathbf{U} \\mathbf{\\Lambda}^{-\\frac{1}{2}} \\mathbf{U}^T \\mathbf{x}$$\nWe whiten our data accordingly and take a look at the resulting scatter plot:\nz = evecs @ np.diag(evals**(-1/2)) @ evecs.T @ x fig, ax = plt.subplots() ax.scatter(z[0, set1], z[1, set1], s=20, c=\"red\", alpha=0.2) ax.scatter(z[0, set2], z[1, set2], s=20, alpha=0.2) ax.set_aspect(\"equal\") ax.set_xlim(-8, 8) ax.set_ylim(-8, 8) ax.set_xlabel(\"$z_1$\") ax.set_ylabel(\"$z_2$\") ax.spines[\"top\"].set_visible(False) ax.spines[\"right\"].set_visible(False) ax.set_title(\"ZCA\") Again, ZCA has decorrelated the data because the scatter plot looks spherical (and the value of np.corrcoef(z)[0, 1] is practically zero). In contrast to PCA, ZCA has preserved the orientation of the original data points. This can be observed from the positions of the red dots, which are aligned along the same direction as the original data. This property has given this whitening transformation its name “zero-phase”, because it minimally distorts the original phase (i.e. orientation) of the data.\nConclusions Both PCA and ZCA whiten the original data, but they perform different rotations. It can be shown that PCA is optimal if the goal is compression of the original data (because principal components are sorted according to their explained variance), whereas ZCA is optimal if the goal is to keep the transformed random vector as similar as possible to the original one (thus ZCA cannot be used to compress the data). Kessy et al. (2018) provide mathematical proofs of these propositions.\nIt is worth mentioning that standardizing the data prior to whitening might sometimes be useful, especially if the individual signals are on different scales. Usually, standardization is not necessary if all signals are EEG signals, but if a combination of EEG and MEG signals simultaneously enter the analysis, all data should be rescaled to avoid biasing the whitening transformation to signals with higher variance.\nICA algorithms are typically kick-started from whitened data. A recent article by Montoya-Martínez et al. (2017) suggests that some ICA variants can be sensitive to the choice of the initial whitening procedure. Specifically, it can make a difference whether PCA or ZCA is used prior to performing Extended Infomax as implemented in EEGLAB. The reason seems to be the slow convergence of this particular ICA algorithm. PICARD (Ablin et al., 2018) improves upon this implementation and provides much faster convergence for both Extended Infomax and FastICA variants. Therefore, it should be rather insensitive to the choice of the particular whitening procedure.\nFinally, regarding the common practice of reducing dimensionality with PCA prior to ICA, a recent article by Artoni et al. (2018) argues that pruning principal components might adversely affect the quality of the resulting independent components. This means that if PCA is used to whiten the data, all components should be retained (i.e. the data should not be compressed).\nAcknowledgments I’d like to thank Pierre Ablin for his very helpful comments on an earlier version of this post.\nDownload the code The script pca-zca.py contains all relevant code snippets from this post.\n","description":"","tags":["Python","EEG","ICA"],"title":"Whitening with PCA and ZCA","uri":"/posts/whitening-pca-zca/"},{"categories":null,"content":"Background In this previous post, we used linear regression to remove ocular artifacts from EEG signals. A popular alternative to this approach is independent component analysis (ICA). In a nutshell, ICA decomposes multi-channel EEG recordings into maximally independent components. Components that represent ocular activity can be identified and eliminated to reconstruct artifact-free EEG signals. This approach is described in more detail in Jung et al. (2000).\nA comprehensive comparison between the two methods is beyond the scope of this post. Instead, I will only list some important properties of both techniques.\nFirst, the regression-based approach requires EOG channels, whereas ICA works without any reference signal.\nBoth methods potentially remove brain activity in addition to ocular activity. ICA can separate ocular components from brain components well if many EEG channels are available (which in turn requires a relatively large number of data samples). A cleaner separation also means that less brain activity will be removed when ocular components are eliminated. The minimum number of EEG channels required for ICA decomposition varies, but as a rule of thumb at least 20 channels seem to be necessary (the EEGLAB tutorial has more details on the amount of data required for ICA). In constrast, the regression approach works even if only a few EEG channels are available. However, the EOG reference channels always contain some amount of brain activity, which will also be removed from the data in addition to ocular activity.\nThe ICA method entails manual identification of ocular components, although several algorithms exist to automate this process (for example EyeCatch or ADJUST). ICA also takes longer to compute than the regression approach (but efficient implementations are available that keep computation time to a minimum). Finally, ICA is an optimization problem that is not guaranteed to find the globally optimal solution. Depending on the initial conditions, the algorithm might find different independent components from run to run. However, this is not a huge issue in this application, because ocular components are relatively stable across decompositions.\nLet’s now turn to an example to see how ICA can be used to remove ocular artifacts with MNE.\nImplementation Before we start, it is worth mentioning that ICA will generally run faster using a multi-threaded numeric library such as Intel MKL or OpenBLAS. If you have an Anaconda environment (which I recommend in this previous post), you should already have NumPy with MKL support.\nData preprocessing We will use the same data set that we already used with the regression approach. Specifically, we’ll use participant A01T from data set 001-2014 from the BNCI Horizon 2020 website (check out the post on the regression approach for more details on this data set). Download this file and save it to your working directory. Note that this data set contains 22 EEG and 3 EOG channels. Although EOG channels can (and should) be used for ICA decomposition (provided that they use the same reference electrode as the EEG channels), we will only use EEG channels here to keep things simple.\nAs always, we start by firing up IPython and performing the usual initial steps:\nfrom scipy.io import loadmat import mne The data comes in a MAT file, so we use SciPy’s loadmat function to load it as a NumPy array. Note that this time we only load the fourth run containing the actual experimental data – we do not need the calibration run.\nmat = loadmat(\"A01T.mat\", simplify_cells=True) eeg = mat[\"data\"][3][\"X\"] * 1e-6 # convert to volts We will plot ICA components as projections on the scalp surface later on. To this end, MNE needs to know the channel labels, which unfortunately are not present in the data. However, the data description contains a picture of the montage, which we can use to populate a list of channel names.\nch_names = [\"Fz\", \"FC3\", \"FC1\", \"FCz\", \"FC2\", \"FC4\", \"C5\", \"C3\", \"C1\", \"Cz\", \"C2\", \"C4\", \"C6\", \"CP3\", \"CP1\", \"CPz\", \"CP2\", \"CP4\", \"P1\", \"Pz\", \"P2\", \"POz\", \"EOG1\", \"EOG2\", \"EOG3\"] We create an info object using this list, which we need to create a Raw object containing the EEG data and associated meta information (which in our case is just the sampling frequency of 250 Hz and the channel types). Finally, we add a standard 10–20 montage, which maps the channel labels to their locations on the scalp. This is required for topographic plots.\ninfo = mne.create_info(ch_names, 250, ch_types=[\"eeg\"] * 22 + [\"eog\"] * 3) raw = mne.io.RawArray(eeg.T, info) raw.set_montage(\"standard_1020\") Performing ICA ICA does not work in the presence of low-frequency drifts, so we create a copy of our raw object and apply a high-pass filter to this copy.\nraw_tmp = raw.copy() raw_tmp.filter(l_freq=1, h_freq=None) We are now ready to perform ICA. First, we instantiate an ICA object and specify that we want to use the extended Infomax algorithm. Note that you can (and should) use the picard method from the python-picard package – this algorithm computes the extended Infomax solution much faster (see here for more details). In any case, the random_state argument should be set for reproducible results.\nica = mne.preprocessing.ICA(method=\"infomax\", fit_params={\"extended\": True}, random_state=1) Next, we fit ica to our filtered raw data raw_tmp (note that this uses only the 22 EEG channels and ignores the 3 EOG channels by default, but this can be changed with the picks argument).\nica.fit(raw_tmp) Identifying ocular components Our next task is to identify ocular components. This is usually done by visual inspection, so we start by plotting all 22 independent components.\nica.plot_components(inst=raw_tmp, picks=range(22)) A new figure with the 22 independent components will pop up. Let’s focus on the first few components, because ocular components are generally found among these.\nFrom these scalp projections, the component labeled as ICA001 looks like it could represent eye movements because of its frontal location. To be sure, we can click on this component to open a new window with more details on this component (this is possible because we specified inst=raw_tmp in the previous call):\nBesides the scalp projection, we can now also inspect\n the component power spectral density (which is typical for ocular activity because the characteristic EEG alpha peak is missing), the epochs image, which color-codes the component activity over (virtual) epochs (which shows typical intermittent activity as blue and red stripes), and the epochs variance (which in this case is not really helpful in identifying the component).  In summary, we can be pretty sure that component ICA001 represents ocular activity. To be extra safe, let’s plot the component time courses to corroborate our assumption:\nica.plot_sources(inst=raw_tmp) Indeed, if you scroll through the data, ICA001 captures primarily eye movements and eye blinks.\nNote that usually two ocular components can be found in the decomposition, but this is not the case in our example data (all remaining components do not seem to originate from eye activity).\nRemoving ocular components In the final step, we create a list attribute ica.exclude containing the indices of all components that should be removed when reconstructing EEG signals. In our case, this list contains only a single component.\nica.exclude = [1] Note that you can click on the component title (ICA001) in the ICA components plot created with ica.plot_components(inst=raw_tmp, picks=range(22)) to include/exclude a component (the title of an excluded component will turn gray). This will also add/remove this component from/to the underlying ica.exclude list.\nNow we can apply our ICA results (without the excluded component) to a copy of the original (unfiltered) EEG to obtain artifact-free signals:\nraw_corrected = raw.copy() ica.apply(raw_corrected) Visualizing results So how did ICA perform? Let’s take a look at a segment of the original EEG containing a clear eye movment artifact:\nraw.plot(n_channels=25, start=53, duration=5, title=\"Before\") And here is the corrected signal:\nraw_corrected.plot(n_channels=25, start=53, duration=5, title=\"After\") Looks like ICA did a pretty decent job in removing eye artifacts.\nDownload the code The script removing-eog-ica.py contains all relevant code snippets from this post.\n","description":"","tags":["Python","MNE","EEG","EOG","Artifacts","ICA"],"title":"Removing eye activity from EEG signals via ICA","uri":"/posts/removing-eog-ica/"},{"categories":null,"content":"Prerequisites In this tutorial, we will continue to use the EEG motor movement/imagery data set from the previous post (again, we use run 4 of subject 1). I’ll quickly reiterate all important steps we have performed so far:\nimport mne raw = mne.io.read_raw_edf(\"S001R04.edf\", preload=True) raw.rename_channels(lambda s: s.strip(\".\")) raw.set_montage(\"standard_1020\", match_case=False) raw.set_eeg_reference(\"average\") Interactive visualization We can now open an interactive visualization window as follows:\nraw.plot() The signals don’t look quite right yet, so let’s walk through some interactive features of this plot. First, you can rescale the signals with the + and – keys. The default scaling is not really appropriate here, but pressing the – key a few times will result in a much nicer display. The topmost channel label Fc5 includes a purple scale bar which indicates the current scale of the data (the length of the purple bar corresponds to 40 µV). This scale applies to all channels of the same type (so in our example to all EEG channels). If this scale gets in your way, you can toggle it with the s key.\nNotice that 20 channels are visible per page by default. To view the remaining channels, you can scroll down with the ↓ key. Conversely, you can scroll back up again by pressing the ↑ key. Alternatively, you can increase or decrease the number of channels visible on a page with the Page Up or Page Down keys, respectively (on a Mac, these keys can usually be accessed with the combinations fn + ↑ or fn + ↓).\nBy default, 10 seconds of data are visible per page. If you want to navigate in time, you can use the → and ← keys to move forward and backward by a quarter of the visible duration (in this example by 2.5 seconds). Pressing Shift + → and Shift + ← will scroll forward/backward by a whole page. You can increase or decrease the amount of time shown per page with the End or Home keys (on a Mac, these keys can usually be accessed with fn + → and fn + ←).\nThe following table summarizes all keyboard shortcuts for the interactive visualization window (we will discuss annotation, butterfly, and zen modes in a minute). Note that the Help button in the lower left corner (or pressing the ? key) pops up a dialog window with these shortcuts.\n   Action Keyboard shortcut Mac equivalent     Scale up +    Scale down –    Scroll up ↑    Scroll down ↓    Scroll left (quarter page) ←    Scroll right (quarter page) →    Scroll left (whole page) Shift + ←    Scroll right (whole page) Shift + →    More channels Page Up fn + ↑   Fewer channels Page Down fn + ↓   More time End fn + →   Less time Home fn + ←   Scale bars s    Zen mode z    Toggle DC removal d    Annotation mode a    Butterfly mode b    Toggle draggable annotations p     You can change initial settings with arguments to raw.plot. For example, we might use the following arguments to start with suitable values for our example data:\nraw.plot(n_channels=64, duration=5, scalings={\"eeg\": 75e-6}, start=10) Our example data contains annotations (stored in raw.annotations), which are visualized as colored patches (the annotation descriptions are located at the top of the plot). In this case, the blue, red, and green patches correspond to T0, T1, and T2 annotations, respectively.\nIn addition to keyboard shortcuts, you can also use the mouse to navigate through the data. You might have already noticed the horizontal bar below the plot. This overview bar summarizes the whole time range of the signal, and the currently visible time segment is highlighted in (light) gray. You can click inside the overview bar to quickly skip around to different time segments. Furthermore, the overview bar also shows annotations.\nTo quickly identify time segments with high variance, you can switch to butterfly mode by pressing the b key. Signals of all channels of the same type will be collapsed onto each other, which makes it easy to spot abnormal activity. Press b again to exit butterfly mode.\nIf you want to focus on the actual signals with fewer distractions, you can get rid of the scroll and overview bars. Press z (zen mode) to toggle these user interface elements.\nFinally, you can quickly filter out offsets (slow drifts) in the signals by pressing the d key (toggle DC removal).\nAnnotations We are now ready to interactively create annotations, for example to mark segments containing artifacts. To this end, we switch to annotation mode by pressing the a key. A small dialog window will pop up. Make sure to keep this dialog window open as long as you want to stay in annotation mode – if you close it, you will return to normal visualization mode.\nThe annotations dialog shows a list of currently available annotations (T0, T1, and T2 in our example data). The annotation label marked with a filled circle is currently active (T0), which means that if we create a new annotation it will be of this type. However, we can also add new annotation types. Notice that the “BAD_” label is actually an input text field – you can start typing/editing to change the annotation label before adding. We would like to create annotations called “BAD”, so let’s remove the underscore by pressing the backspace key. The annotation gets added to the list of available labels once you click on the large “Add new label” button or hit the Enter key.\nNote that annotation labels starting with “BAD” (or “bad”) are special in MNE, because many functions ignore data contained within such annotations. This gives us a nice way to mark artifact segments without completely discarding data points.\nLet’s create some “BAD” annotations in our example data. First, we make sure that the “BAD” label is active by clicking inside the red ring (inactive labels are not filled). Optionally, we can hide all T0, T1, and T2 annotations by clicking on their “show/hide” checkboxes. Now we switch back to the main visualization window (again, make sure not to close the annotation window). Using the mouse, we can now click and drag to select a portion of the data and mark it with the currently active “BAD” label. During this process, you can interactively scroll around in the data with the keyboard shortcuts listed before, or you can click within the overview bar below the plot.\nIf you want to edit the start and/or end point of an existing annotation, press the p key to toggle snap mode. If snap mode is enabled, you can drag the start and/or end points of an annotation to the desired locations. It is impossible to create an annotation inside another annotation with snap mode enabled, so that’s when you want to turn off snap mode. If you want to completely delete an annotation, you can right-click on it and it disappears.\nWhen you are done, you can close the annotation window. You can also close the visualization window, because all annotations are automatically stored in the raw.annotations attribute. Let’s take a look what it looks like:\nraw.annotations \u003cAnnotations | 39 segments: BAD (9), T0 (15), T1 (8), T2 (7)\u003e\rYay, we’ve just created 9 new annotations, all of which are interpreted as bad (because they start with the word “BAD” or “bad”).\nMarking channels Sometimes, signals are noisy in particular channels during the whole recording. In such cases, it is possible to mark the whole channel as bad. In the visualization window, we can click on a channel label on the left side of the plot or on a channel trace to mark a channel as bad (the label and associated time course will turn gray to reflect this). If you want to unmark a channel, click on its label (or its trace) again. The selection of bad channels is immediately stored in raw.info[\"bads\"] (a list containing the channel labels of all bad channels). Channels marked as bad are generally ignored by MNE functions.\nSaving and loading annotations Although changes to annotations or bad channels are immediately reflected in raw.annotations and raw.info[\"bads\"], we still need a way to permanently store this information on disk for later use. Consequently, we will also need a way to load this information and add it to an existing raw object.\nIt is always a good idea to use simple storage formats. Therefore, MNE allows us to store annotations as simple text files. Here’s how we could save annotations in a file called S001R04_annotations.txt:\nraw.annotations.save(\"S001R04_annotations.txt\") This creates a comma-separated text file with three columns “onset”, “duration”, and “description”. Onsets are relative to the start of the recording (so they start at zero), and both onsets and durations are measured in seconds. If you specify a .csv instead of a .txt extension, you will get absolute time stamps for the onsets (based on the measurement onset time stamp).\nWe can read this file with:\nannotations = mne.read_annotations(\"S001R04_annotations.txt\") Finally, we can associate these annotations with an existing raw object:\nraw.set_annotations(annotations) Saving and loading bad channels The following lines of code store the list of bad channels in a text file called S001R04_bads.txt:\nwith open(\"S001R04_bads.txt\", \"w\") as f: f.write(\",\".join(raw.info[\"bads\"])) f.write(\"\\n\") To load this file and update the raw object, we can use:\nwith open(\"S001R04_bads.txt\") as f: raw.info[\"bads\"] = f.read().strip().split(\",\") ","description":"","tags":["Python","MNE","EEG","Artifacts"],"title":"Visualizing EEG data with MNE","uri":"/posts/visualizing-eeg-data/"},{"categories":null,"content":"MNE MNE is a popular Python package for EEG/MEG processing. It offers a wide variety of useful tools including reading and writing of various file formats, filtering, independent component analysis (ICA), forward modeling, inverse solutions, time-frequency decompositions, visualization, and more. In fact, if you are familiar with EEGLAB you might find that you can perform many similar analyses with Python and MNE. In this post I will show how to load EEG data as well as view and edit associated meta information.\nPrerequisites The first step in almost any EEG processing pipeline is loading the raw data files into memory. There are many different file formats for storing EEG data, mostly because each EEG amplifier manufacturer uses its own data format. For example, BrainProducts amplifiers store data as an EEG/VHDR/VMRK file triplet, BioSemi amplifiers create BDF files (an extension of the European Data Format), Neuroscan amplifiers use proprietary CNT files, and so on. Luckily, MNE comes with support for many popular EEG file formats.\nThe EEG motor movement/imagery data set we will use in this tutorial was contributed to the public domain by the developers of the BCI2000 system. In brief, they recorded 64-channel EEG from over 100 participants during motor execution and motor imagery tasks. In this tutorial, we will analyze only one participant and only one motor imagery run. If you want to follow along, go ahead and download run 4 of subject 1 (note that MNE has a dedicated loader in mne.datasets.eegbci for this data set, which makes it even easier to load particular subjects and runs). All subsequent commands assume that the file is located in the current working directory.\nNow that we have selected our data, it is time to fire up Python. I recommend IPython as an enhanced interactive Python shell (go ahead and read my article on how to set up Python for EEG analysis if you do not have a working Python environment yet). You can start IPython from a terminal by typing ipython.\nWe want to use the MNE package, so we have to import it.\nimport mne You can check the version of MNE as follows (make sure you always use the latest version):\nmne.__version__ '0.22.0'\rLoading EEG data Loading EEG data works as follows. First, we need to know the file type of the file we want to load. In our case, the data is stored in an EDF file called S001R04.edf. Second, Python needs to know exactly where this file is located. We can either specify the full path to the file, or we can make sure that the file is in the current working directory, in which case the file name alone is sufficient. In this case, we can use the following command to load the raw data:\nraw = mne.io.read_raw_edf(\"S001R04.edf\", preload=True) The argument preload=True means that MNE performs the actual loading process immediately instead of the default lazy behavior. Also note that there are many more reader functions available in the mne.io package, including mne.io.read_raw_bdf, mne.io.read_raw_brainvision, mne.io.read_raw_cnt, and mne.io.read_raw_eeglab.\nI won’t reproduce the output that is generated during the loading process here. Usually, it contains some more or less useful logging messages, so if anything goes wrong make sure to carefully study these messages. If you don’t want to see these messages, you can suppress them as follows:\nmne.set_log_level(\"WARNING\") This will instruct MNE to only print out warnings and errors.\nViewing and editing meta information The previous assignment generated a Raw object in our workspace. This object holds the actual EEG data and associated meta information. We can get some basic information by inspecting this object in the interactive Python session:\nraw \u003cRawEDF | S001R04.edf, 64 x 20000 (125.0 s), ~9.8 MB, data loaded\u003e\rWe can see the file name, the number of channels and sample points, the length in seconds, and the approximate size of the data in memory.\nThe meta information attribute We can dig deeper into the meta information by inspecting the info attribute associated with raw.\nraw.info \u003cInfo | 7 non-empty values\rbads: []\rch_names: Fc5., Fc3., Fc1., Fcz., Fc2., Fc4., Fc6., C5.., C3.., C1.., ...\rchs: 64 EEG\rcustom_ref_applied: False\rhighpass: 0.0 Hz\rlowpass: 80.0 Hz\rmeas_date: 2009-08-12 16:15:00 UTC\rnchan: 64\rprojs: []\rsfreq: 160.0 Hz\r\u003e\rThere are seven non-empty values in this attribute. For example, the line chs: 64 EEG near the top of the output tells us that there are 64 EEG channels (the first few channel names are listed in the line starting with ch_names). Individual elements of raw.info can be accessed with dictionary-like indexing. For example, the sampling frequency is stored in:\nraw.info[\"sfreq\"] 160.0\rOther useful info keys are:\n \"bads\": A list of noisy (bad) channels which should be ignored in further analyses. Initially, this list is empty (as in our example), but we will manually populate it in a later post. \"ch_names\": A list of channel names. \"chs\": A detailed list of channel properties, including their types (for example, EEG, EOG or MISC). \"highpass\" and \"lowpass\": Highpass and lowpass edge frequencies that were used during recording. \"meas_date\": The recording date (a datetime.datetime object).  Renaming channels The output of raw.info revealed that some channel names are suffixed with one or more dots. Since these are non-standard names, let’s rename the channels by removing all trailing dots:\nraw.rename_channels(lambda s: s.strip(\".\")) Assigning a montage For good measure (and for later use), we can assign a montage to the data (a montage relates channels names to standardized or actual locations on the scalp surface). First, let’s list all montages that ship with MNE.\nmne.channels.get_builtin_montages() ['EGI_256',\r'GSN-HydroCel-128',\r'GSN-HydroCel-129',\r'GSN-HydroCel-256',\r'GSN-HydroCel-257',\r'GSN-HydroCel-32',\r'GSN-HydroCel-64_1.0',\r'GSN-HydroCel-65_1.0',\r'biosemi128',\r'biosemi16',\r'biosemi160',\r'biosemi256',\r'biosemi32',\r'biosemi64',\r'easycap-M1',\r'easycap-M10',\r'mgh60',\r'mgh70',\r'standard_1005',\r'standard_1020',\r'standard_alphabetic',\r'standard_postfixed',\r'standard_prefixed',\r'standard_primed']\rAccording to the data set documentation, the channel locations conform to the international 10–10 system. MNE does not seem to ship a 10–10 montage, but standard_1020 contains template locations from the extended 10–20 system:\nmontage = mne.channels.make_standard_montage(\"standard_1020\") montage.plot() It looks like this montage contains all channels used in our example data set. Therefore, we can assign this montage to our raw object.\nraw.set_montage(montage, match_case=False) Re-referencing The data documentation does not mention any reference electrode, but it is safe to assume that all channels are referenced to some standard location such as a mastoid or the nose. Often, we want to re-reference EEG data to the so-called average reference (the average over all recording channels). In MNE, we can compute average referenced signals as follows:\nraw.set_eeg_reference(\"average\") Note that in general, methods modify MNE objects in-place.\nAnnotations Finally, many EEG data sets come with discrete events, either in the form of an analog stimulation channel or (text) annotations. Our example data set contains annotations that can be accessed with the annotations attribute:\nraw.annotations \u003cAnnotations | 30 segments: T0 (15), T1 (8), T2 (7)\u003e\rWe see that there are 30 annotations in total. There are three kinds of annotations named T0, T1, and T2. According to the data set description, T0 corresponds to rest, T1 corresponds to motion onset of the left fist, and T2 corresponds to motion onset of the right fist.\nIf you encounter a file with an analog stimulation channel (this is typically the case for data recorded with BioSemi amplifiers), you need to extract discrete events from this channel as a first step. The mne.find_events function converts information contained in an analog stimulation channel to a NumPy array of shape (N, 3), where N (the number of rows) is the number of detected events. The first column contains event onsets (in samples), whereas the third column contains (integer) event codes. The second column contains the values of the stimulation channel one sample before the detected events (this column can usually be ignored).\nThis NumPy array can be converted to an Annotations object using mne.annotations_from_events, which is often necessary for further analyses (note that you can associate an existing Annotations object with a Raw object by calling the raw.set_annotations method).\nIn the next post, I will demonstrate how to visualize this data set and how to interactively mark bad channels and bad segments.\nDownload the code The script loading-eeg-data.py contains all relevant code snippets from this post.\n","description":"","tags":["Python","MNE","EEG"],"title":"Loading EEG data in Python","uri":"/posts/loading-eeg-data/"},{"categories":null,"content":"Background Eye movement and eye blinks are clearly visible in the ongoing EEG. These ocular artifacts are produced by changes in the electrical dipole between the front and back of the eyes. Usually, it is important to minimize the influence of eye activity on the recorded EEG.\nThere are two popular methods available to remove ocular artifacts, namely approaches based on (multivariate linear) regression and approaches based on independent component analysis (ICA). In this post, I will focus on the regression method and leave the ICA-based variant for another post.\nThe regression-based approach in its simplest form dates back to Hillyard and Galambos (1970), who used data from a pre-experimental calibration run containing voluntarily produced ocular artifacts to estimate regression coefficients on the EEG. More than a decade later, Gratton, Coles, and Donchin (1983) improved upon this procedure in two ways. First, they used EOG recordings from the experimental data (the same data that needs to be corrected) instead of a calibration run. Second, they estimated regression coefficients separately for eye movements and eye blinks.\nDespite its age, removing ocular artifacts via linear regression remains a popular approach because it works well in many situations. However, it is important to keep in mind that dedicated EOG electrodes placed close to the eyes must be used. In particular, if an EOG electrode fails during the recording session, the whole method breaks down. Furthermore, it is reasonable to assume that EOG electrodes will record some amount of brain activity (after all, the brain sits right behind these sensors). Therefore, this method will likely remove brain activity in addition to ocular artifacts. On the plus side, there is no restriction on the number of EEG electrodes required – the regression-based approach can clean even single-channel EEG recordings.\nImplementation Data preprocessing To demonstrate this method, we will download a publicly available EEG data set from the BNCI Horizon 2020 website. Specifically, we’ll use participant A01T from data set 001-2014 (go ahead and download this file and put it in your working directory if you want to follow along). The description mentions that the third run contains calibration eye movements. We will use these to estimate regression coefficients and correct EEG data in the subsequent fourth run.\nAs always, we start with some setup commands. Since the example data is stored in a MAT file, we need to import scipy.io.loadmat. We will also use numpy in our analysis.\nimport numpy as np from scipy.io import loadmat import mne Loading the example data is then pretty straightforward.\nmat = loadmat(\"A01T.mat\", simplify_cells=True) This gives us a mat dictionary containing the data in a nicely organized way. Let’s first check the available dictionary keys.\nmat.keys() dict_keys(['__header__', '__version__', '__globals__', 'data']) The EEG data is stored in the data key as a list, where each list element corresponds to a run. For example, mat[\"data\"][0] contains data from the first run, mat[\"data\"][1] corresponds to the second run, and so on. Each run is represented by a dictionary, and the EEG data is stored under the \"X\" key.\nWe are interested in EEG data from the calibration run (third run, list index 2) and the experimental run (fourth run, list index 3), which we store as two separate names eeg1 and eeg2 (note that we multiply by 10-6 to convert the units from microvolts to volts). This results in two NumPy arrays.\neeg1 = mat[\"data\"][2][\"X\"] * 1e-6 # run 3 (calibration) eeg2 = mat[\"data\"][3][\"X\"] * 1e-6 # run 4 (experiment) The next step is not necessary, but we’ll convert eeg1 and eeg2 into MNE Raw objects. This will give us the ability to quickly generate plots of raw EEG traces before and after ocular artifact removal.\ninfo = mne.create_info(25, 250, ch_types=[\"eeg\"] * 22 + [\"eog\"] * 3) raw1 = mne.io.RawArray(eeg1.T, info) raw2 = mne.io.RawArray(eeg2.T, info) The arguments to mne.create_info set the number of channels in the data (25), the sampling frequency (250 Hz), and the channel types (the first 22 channels are EEG whereas the last three channels are EOG). We have to transpose the arrays because MNE expects channels in rows and samples in columns.\nEstimating regression coefficients We are now ready to estimate regression coefficients to remove the influence of ocular artifacts on the EEG. It turns out that converting the three monopolar EOG channels EOG1, EOG2, and EOG3 to two bipolar channels is a good idea. One way to go about this is to multiply the monopolar EOG signals with a suitable matrix which generates bipolar derivations EOG1–EOG2 and EOG3–EOG2. If you happen to have four monopolar EOG channels (which is also a common EOG recording setup), you can convert them into a horizontal and a vertical bipolar channel, respectively.\nbip = np.array([[1, -1, 0], [0, -1, 1]]) raw1_eog = bip @ raw1[22:, :][0] raw2_eog = bip @ raw2[22:, :][0] Note that we have performed this conversion for both runs raw1 and raw2 separately. Furthermore, indexing a raw object returns a tuple with the requested data and associated time values. We only need the data, and [0] selects just the first entry in the returned tuple.\nFor shorter notation, we also create separate names for the EEG signals (the first 22 channels) of both runs.\nraw1_eeg = raw1[:22, :][0] raw2_eeg = raw2[:22, :][0] Now comes the important line where we perform ordinary least squares to get the linear regression coefficients. Note that we actually solve for all EEG channels simultaneously (or in other words, there are several dependent or response variables). We also have several independent or predictor variables in the form of EOG channels. Such a model is sometimes referred to as a multivariate (more than one response variable) multiple (more than one predictor variable) regression model.\nIf we denote our response variables (the EEG signals) with $Y$, our predictor variables (the EOG signals) as $X$, and the regression coefficients as $B$, we can write the linear model as follows:\n$$Y = X B$$\nWe can then compute the least squares solution for $B$ by left-multiplying with the Moore-Penrose inverse $X^+ = (X^T X)^{-1} X^T$:\n$$B = X^+ Y = (X^T X)^{-1} X^T Y$$\nIn Python code, this looks as follows (note that we need to transpose our signals because we have our channels in rows, whereas the equation assumes that they are in columns):\nb = np.linalg.inv(raw1_eog @ raw1_eog.T) @ raw1_eog @ raw1_eeg.T Note that the @ operator computes the dot product of matrices.\nAlternatively, we can use the numerically more stable method np.linalg.solve to compute the regression coefficients:\nb = np.linalg.solve(raw1_eog @ raw1_eog.T, raw1_eog @ raw1_eeg.T) As a quick sanity check, let’s inspect the shape of our regression parameter matrix b.\nb.shape (2, 22) This makes sense because there are two EOG channels (predictors) and 22 EEG channels (responses).\nNow all we need to do to remove ocular artifacts from new data is to subtract the estimated EOG influence from the measured EEG. We’ll create a new raw3 object to store this corrected data.\neeg_corrected = (raw2_eeg.T - raw2_eog.T @ b).T raw3 = raw2.copy() raw3._data[:22, :] = eeg_corrected Visualizing results Let’s see if the method worked. First, we plot a segment of the original raw2 data with some prominent eye activity. For this visualization, we set the number of simultaneously visible channels to 25, the start of the plot to second 53, and the duration to 5 seconds.\nraw2.plot(n_channels=25, start=53, duration=5, block=True, title=\"Before\") We produce the same plot for the corrected raw3 data.\nraw3.plot(n_channels=25, start=53, duration=5, block=True, title=\"After\") If you look closely, the method successfully removed ocular artifacts clearly visible in this time segment.\nDownload the code The script removing-eog-regression.py contains all relevant code snippets from this post.\n","description":"","tags":["Python","MNE","EEG","EOG","Artifacts"],"title":"Removing eye activity from EEG signals via regression","uri":"/posts/removing-eog-regression/"},{"categories":null,"content":"I am a senior postdoc with a background in electrical/biomedical engineering and computer engineering. I work at the Educational Neuroscience group at the Institute of Psychology, University of Graz, Austria.\nI am interested in neuroscientific substrates of number processing and arithmetic, EEG oscillations and connectivity analysis, biomedical signal processing, applied machine learning and statistics, brain-computer interfaces, and software development. I am a big fan of open source software and I believe that science should be open as well, including data and analysis scripts. Python is my favorite language, but I also enjoy performing data analysis and statistical tests with R (thanks in particular to its awesome Tidyverse). I have also started to use Julia recently.\nI maintain and develop the following larger projects:\n MNELAB, a graphical user interface for processing EEG/MEG data using MNE-Python SleepECG, a Python package for sleep staging using ECG (which includes a fast R-peak detector) HeartBeats.jl, a Pan-Tompkins R-peak ECG detector in Julia XDF.jl, a Julia package for reading XDF files SigViewer, a Qt/C++ based biosignal visualization tool SCoT, a Python package for EEG-based source connectivity estimation  In addition, I am part of the MNE-Python and pyXDF development teams, and I have contributed to numerous open source projects such as scikit-learn, pandas, PsychoPy, SciPy, Matplotlib, pybv, BioSig, and several smaller contributions for other projects (check out my GitHub profile for a detailed activity summary).\nAlthough I mainly work on macOS, I also like Arch Linux, for which I maintain several packages in the AUR.\n","description":"","tags":null,"title":"About","uri":"/about/"},{"categories":null,"content":"Installing Python There are several ways to install a Python environment on your computer. For example, the most obvious way involves downloading an installer from the official Python website. Official installers are available for Windows and macOS. Most Linux distributions include Python out of the box (if not you can use the package manager to quickly set up a working Python environment). While this option works in many situations, obtaining additional Python packages for scientific computing might not be particularly convenient for beginners.\nAn alternative available on macOS is the package manager Homebrew. Although this is my preferred way of installing Python on a Mac, I will not describe it in detail here (other than mention that running brew install python in a terminal is all it takes).\nWhile Python is relatively easy to set up, installing additional packages sometimes involves compiling source code. This means that you need to have a working build toolchain on your machine, which is standard on Linux systems, relatively straightforward on macOS, but a bit more challenging on Windows.\nThis is where Python distributions come in handy – they bundle Python with additional pre-built packages, which means that compiling is not necessary. One of the most popular Python distributions (at least in the scientific community) seems to be Anaconda, which includes hundreds of useful Python packages. Among the most notable features are versions of NumPy, SciPy, and Scikit-learn with Intel Math Kernel Library (MKL) support. This basically means that if you have an Intel CPU many computations will be faster than with standard versions of these packages (although in my experience OpenBLAS is just as fast and also runs on AMD CPUs).\nInstalling Anaconda is as easy as downloading and running the installer for your operating system. After that, you are all set.\nIf you do not want to install dozens of packages that come with Anaconda (for example because you want to save precious disk space), you can install its barebones sibling called Miniconda. Just know that you will need to install (almost) all additional packages manually via the conda package manager.\nInstalling additional packages Although Python comes with an extensive standard library, most scientific packages are not part of Python itself. Luckily, installing additional Python packages is not difficult. If you use Anaconda or Miniconda, you can leverage the conda command line tool to manage Python packages. Open a terminal (sometimes also called a command line) to use the tool.\nThe first step is to find out if a particular package is available in the Anaconda repository (replace \u003cpackage\u003e with the actual name of the package):\nconda search \u003cpackage\u003e\rIf the package is available (meaning that the previous command returned a match), here’s how you can install it:\nconda install \u003cpackage\u003e\rFor example, if you are planning to do EEG analysis, chances are that you will be using MNE. Let’s see if a package called mne is available in Anaconda:\nconda search mne\rUnfortunately, the search returns no matches (meaning that MNE is not available in Anaconda), so we need to use another method to install it. The standard way to go about this is to use the official Python package manager pip for packages that are not available with conda. The pip command line tool searches the Python Packaging Index (PyPI), which is the central repository for third-party Python packages.\nHere’s how you can search for the mne package:\npip search mne\rThis command spits out several matches, including a package called mne. We can install the package as follows:\npip install mne\rThat’s all there’s to it. Just remember to use conda to search and install a package first, because Anaconda packages are often optimized and tested to work well in combination with other Anaconda packages. If a package is not available in Anaconda, use pip to install it. If you do not use Anaconda/Miniconda, you can only use pip anyway.\nRecommended scientific packages Here are some useful packages that I recommend if you want to use Python for scientific computing in general and EEG processing in particular (use conda and/or pip to install them):\n NumPy provides a multi-dimensional array data type; it is the basis for almost all scientific packages. SciPy contains a large number of various algorithms used in scientific computing. Pandas extends the NumPy array and provides a more flexible data frame type similar to the one found in R. Matplotlib is the most popular package to create all kinds of plots in Python. IPython is an enhanced interactive Python shell. Scikit-learn is a powerful machine learning package for Python. MNE is a package for EEG/MEG processing. PyQt5 provides Python bindings for the Qt GUI toolkit.  Keeping everything up to date It is generally a good idea to use the most recent version of Python. In addition, you probably also want to keep all installed packages up to date. If you use Anaconda, you can update all installed packages with:\nconda update --all\rPackages installed via pip cannot be updated with conda. However, you can use pip to get a list of outdated packages:\npip list --outdated\rThis list might also include packages installed via conda (in general, pip offers the latest versions before they get into Anaconda/Miniconda). You can keep track of packages installed via pip by inspecting the rightmost “Channel” column of the output of conda list. If it says “pypi” then you can (and should) only update this package using pip:\npip install -U \u003cpackage\u003e\rIf you do not use Anaconda (or Miniconda), you have to use pip to find and update all outdated packages. Unfortunately, there is no built-in flag to update all outdated packages at once. You can either update each package individually or use any of the numerous methods mentioned in this StackOverflow post.\nFinally, feel free to install as many packages as you like. After all, Python makes it easy and fun to try out stuff that others have already implemented so that you don’t have to! If you decide later on that you do not need a specific package, you can completely remove it with either conda or pip (depending on how you installed it):\nconda uninstall \u003cpackage\u003e\rpip uninstall \u003cpackage\u003e\r","description":"","tags":["Python","EEG"],"title":"Setting up Python for EEG analysis","uri":"/posts/setting-up-python/"}]
