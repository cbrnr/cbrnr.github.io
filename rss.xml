<?xml version="1.0" encoding="utf-8"?>






<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Clemens Brunner</title>
        <link>https://cbrnr.github.io/</link>
        <description>MemE is a powerful and highly customizable GoHugo theme for personal blogs.</description>
        <generator>Hugo 0.92.2 https://gohugo.io/</generator>
        
            <language>en</language>
        
        
            <managingEditor>clemens.brunner@gmail.com (Clemens Brunner)</managingEditor>
        
        
            <webMaster>clemens.brunner@gmail.com (Clemens Brunner)</webMaster>
        
        
            <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
        
        <lastBuildDate>Thu, 24 Feb 2022 11:17:11 &#43;0100</lastBuildDate>
        
            <atom:link rel="self" type="application/rss&#43;xml" href="https://cbrnr.github.io/rss.xml" />
        
        
            <item>
                <title>Whitening with PCA and ZCA</title>
                <link>https://cbrnr.github.io/posts/whitening-pca-zca/</link>
                <guid isPermaLink="true">https://cbrnr.github.io/posts/whitening-pca-zca/</guid>
                <pubDate>Mon, 17 Dec 2018 00:00:00 &#43;0000</pubDate>
                
                    <author>clemens.brunner@gmail.com (Clemens Brunner)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Whitening_transformation&#34;&gt;Whitening&lt;/a&gt; (also known as sphering) is a linear transformation used for decorrelating signals. Applied to EEG, this means that the original channel time series (which tend to be highly correlated) are transformed into uncorrelated signals with identical variances. The term whitening is derived from &lt;a href=&#34;https://en.wikipedia.org/wiki/White_noise&#34;&gt;white noise&lt;/a&gt; (which in turn draws its name from white light), which consists of serially uncorrelated samples. Whitening thus transforms a random vector into a &lt;a href=&#34;https://en.wikipedia.org/wiki/White_noise#White_noise_vector&#34;&gt;white noise vector&lt;/a&gt; with uncorrelated components.&lt;/p&gt;
&lt;p&gt;Theoretically, there are infinitely many possibilities to perform a whitening transformation. We will explore two popular whitening methods in more detail, namely &lt;a href=&#34;https://en.wikipedia.org/wiki/Principal_component_analysis&#34;&gt;principal component analysis (PCA)&lt;/a&gt; and zero-phase component analysis (ZCA), which was introduced by &lt;a href=&#34;https://doi.org/10.1016/S0042-6989(97)00121-1&#34;&gt;Bell and Sejnowski (1997)&lt;/a&gt;. These methods are commonly used in EEG/MEG analysis as a preprocessing step prior to independent component analysis (ICA) (see &lt;a href=&#34;https://cbrnr.github.io/posts/removing-eog-ica/&#34;&gt;this previous post&lt;/a&gt; on how to remove ocular artifacts with ICA). If you want to delve into the matter more deeply, &lt;a href=&#34;https://doi.org/10.1080/00031305.2016.1277159&#34;&gt;Kessy et al. (2018)&lt;/a&gt; discuss even more possible whitening methods.&lt;/p&gt;
&lt;p&gt;Mathematically, whitening transforms a random vector $\mathbf{x}$ (the original EEG channel time series) into a random vector $\mathbf{z}$ using a whitening matrix $\mathbf{W}$:&lt;/p&gt;
&lt;p&gt;$$\mathbf{z} = \mathbf{W} \mathbf{x}$$&lt;/p&gt;
&lt;p&gt;Importantly, the original covariance matrix $\text{cov}(\mathbf{x}) = \mathbf{\Sigma}$ becomes $\text{cov}(\mathbf{z}) = \mathbf{I}$ after the transformation – the identity matrix. This means that all components of $\mathbf{z}$ have unit variance and all correlations have been removed.&lt;/p&gt;
&lt;h2 id=&#34;toy-data&#34;&gt;Toy data&lt;/h2&gt;
&lt;p&gt;To illustrate the differences between PCA and ZCA whitening, let&amp;rsquo;s create some toy data. Specifically, we generate 1000 samples of two correlated time series $x_1$ and $x_2$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;plt&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;seed&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]]&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# must be positive semi-definite&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1000&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;random&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;multivariate_normal&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;mu&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;size&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The two time series are stored in the NumPy array &lt;code&gt;x&lt;/code&gt; of shape &lt;code&gt;(2, 1000)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;For visualization purposes that will become clear in a moment, we determine the 20 most extreme values and denote their indices as &lt;code&gt;set1&lt;/code&gt; (the indices of the remaining data points are stored in &lt;code&gt;set2&lt;/code&gt;):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;argsort&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;norm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;axis&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))[&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;set2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;list&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;set&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s now plot all values of $x_1$ versus their corresponding values of $x_2$.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;fig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subplots&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_aspect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;equal&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_xlim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_ylim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;$x_1$&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;$x_2$&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;top&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;right&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Original&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/scatter_x.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Clearly, these two time series appear to be highly correlated. The ellipsoidal shape of the scatter plot indicates that as the values of $x_1$ increase, the values of $x_2$ also tend to increase. Indeed, the Pearson correlation between $x_1$ and $x_2$ is 0.80 and can be computed with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;corrcoef&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The red dots indicate the most extreme values, and we will observe how these points are transformed by the subsequent whitening procedures.&lt;/p&gt;
&lt;h2 id=&#34;eigendecomposition&#34;&gt;Eigendecomposition&lt;/h2&gt;
&lt;p&gt;Both PCA and ZCA are based on eigenvectors and eigenvalues of the (empirical) covariance matrix. In particular, the covariance matrix can be decomposed into its eigenvectors $\mathbf{U}$ and eigenvalues $\mathbf{\Lambda}$ as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{\Sigma} = \mathbf{U} \mathbf{\Lambda} \mathbf{U}^T$$&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s compute these quantities for our toy data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;cov&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;evals&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;evecs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eigh&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;sigma&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that we now use the empirical covariance matrix derived from the data instead of the true covariance matrix, which is generally unknown. Furthermore, note that since a covariance matrix is always symmetric, we can use the optimized &lt;code&gt;np.linalg.eigh&lt;/code&gt; function instead of the more generic &lt;code&gt;np.linalg.eig&lt;/code&gt; version (this also makes sure that we will always get real eigenvalues instead of complex ones). Alternatively, we could also use &lt;code&gt;np.linalg.svd&lt;/code&gt; directly on the data &lt;code&gt;x&lt;/code&gt; (instead of the covariance matrix) to compute the eigenvectors and eigenvalues, which can be numerically more stable in some situations.&lt;/p&gt;
&lt;h2 id=&#34;whitening-with-pca&#34;&gt;Whitening with PCA&lt;/h2&gt;
&lt;p&gt;The whitening matrix $\mathbf{W}^{\mathrm{PCA}}$ for PCA can be written as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{W}^{\mathrm{PCA}} = \mathbf{\Lambda}^{-\frac{1}{2}} \mathbf{U}^T$$&lt;/p&gt;
&lt;p&gt;This means that the data can be transformed as follows:&lt;/p&gt;
&lt;p&gt;$$\mathbf{z} = \mathbf{W}^{\mathrm{PCA}} \mathbf{x} = \mathbf{\Lambda}^{-\frac{1}{2}} \mathbf{U}^T \mathbf{x}$$&lt;/p&gt;
&lt;p&gt;Therefore, we can whiten our toy data accordingly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evals&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;evecs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s see how our transformed toy data looks like in a scatter plot:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;fig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subplots&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_aspect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;equal&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_xlim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_ylim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;$z_1$&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;$z_2$&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;top&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;right&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;PCA&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/scatter_pca.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Clearly, the transformation removed the correlation between the two time series, because the scatter plot now looks like a sphere (a circle in two dimensions) – hence the name sphering. Indeed, the correlation coefficient (&lt;code&gt;np.corrcoef(z)[0, 1]&lt;/code&gt;) yields a value practically equal to zero. Importantly, PCA has rotated all data points as illustrated by the new positions of the red dots; these do not lie on a diagonal with roughly 45 degrees anymore, but are now aligned with the vertical axis.&lt;/p&gt;
&lt;h2 id=&#34;whitening-with-zca&#34;&gt;Whitening with ZCA&lt;/h2&gt;
&lt;p&gt;The whitening matrix $\mathbf{W}^{\mathrm{ZCA}}$ for ZCA can be written as:&lt;/p&gt;
&lt;p&gt;$$\mathbf{W}^{\mathrm{ZCA}} = \mathbf{U} \mathbf{\Lambda}^{-\frac{1}{2}} \mathbf{U}^T$$&lt;/p&gt;
&lt;p&gt;In fact, this transformation looks almost like PCA whitening, but with an additional rotation by $\mathbf{U}$. Again, the original data can be transformed as follows:&lt;/p&gt;
&lt;p&gt;$$\mathbf{z} = \mathbf{W}^{\mathrm{ZCA}} \mathbf{x} = \mathbf{U} \mathbf{\Lambda}^{-\frac{1}{2}} \mathbf{U}^T \mathbf{x}$$&lt;/p&gt;
&lt;p&gt;We whiten our data accordingly and take a look at the resulting scatter plot:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;evecs&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;diag&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;evals&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;**&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;evecs&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;fig&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;plt&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;subplots&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;c&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;red&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;scatter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;z&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;set2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;alpha&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_aspect&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;equal&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_xlim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_ylim&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_xlabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;$z_1$&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_ylabel&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;$z_2$&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;top&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;spines&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;right&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_visible&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ax&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_title&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;ZCA&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/scatter_zca.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again, ZCA has decorrelated the data because the scatter plot looks spherical (and the value of &lt;code&gt;np.corrcoef(z)[0, 1]&lt;/code&gt; is practically zero). In contrast to PCA, ZCA has preserved the orientation of the original data points. This can be observed from the positions of the red dots, which are aligned along the same direction as the original data. This property has given this whitening transformation its name &amp;ldquo;zero-phase&amp;rdquo;, because it minimally distorts the original phase (i.e. orientation) of the data.&lt;/p&gt;
&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Both PCA and ZCA whiten the original data, but they perform different rotations. It can be shown that PCA is optimal if the goal is compression of the original data (because principal components are sorted according to their explained variance), whereas ZCA is optimal if the goal is to keep the transformed random vector as similar as possible to the original one (thus ZCA cannot be used to compress the data). &lt;a href=&#34;https://doi.org/10.1080/00031305.2016.1277159&#34;&gt;Kessy et al. (2018)&lt;/a&gt; provide mathematical proofs of these propositions.&lt;/p&gt;
&lt;p&gt;It is worth mentioning that &lt;a href=&#34;https://en.wikipedia.org/wiki/Standard_score&#34;&gt;standardizing&lt;/a&gt; the data prior to whitening might sometimes be useful, especially if the individual signals are on different scales. Usually, standardization is not necessary if all signals are EEG signals, but if a combination of EEG and MEG signals simultaneously enter the analysis, all data should be rescaled to avoid biasing the whitening transformation to signals with higher variance.&lt;/p&gt;
&lt;p&gt;ICA algorithms are typically kick-started from whitened data. A recent article by &lt;a href=&#34;https://hal.archives-ouvertes.fr/hal-01451432&#34;&gt;Montoya-Martínez et al. (2017)&lt;/a&gt; suggests that some ICA variants can be sensitive to the choice of the initial whitening procedure. Specifically, it can make a difference whether PCA or ZCA is used prior to performing &lt;a href=&#34;https://doi.org/10.1162/089976699300016719&#34;&gt;Extended Infomax&lt;/a&gt; as implemented in &lt;a href=&#34;https://sccn.ucsd.edu/eeglab/index.php&#34;&gt;EEGLAB&lt;/a&gt;. The reason seems to be the slow convergence of this particular ICA algorithm. &lt;a href=&#34;https://github.com/pierreablin/picard&#34;&gt;PICARD&lt;/a&gt; (&lt;a href=&#34;https://doi.org/10.1109/TSP.2018.2844203&#34;&gt;Ablin et al., 2018&lt;/a&gt;) improves upon this implementation and provides much faster convergence for both Extended Infomax and &lt;a href=&#34;https://doi.org/10.1109%2F72.761722&#34;&gt;FastICA&lt;/a&gt; variants. Therefore, it should be rather insensitive to the choice of the particular whitening procedure.&lt;/p&gt;
&lt;p&gt;Finally, regarding the common practice of reducing dimensionality with PCA prior to ICA, a recent article by &lt;a href=&#34;https://doi.org/10.1016/j.neuroimage.2018.03.016&#34;&gt;Artoni et al. (2018)&lt;/a&gt; argues that pruning principal components might adversely affect the quality of the resulting independent components. This means that if PCA is used to whiten the data, all components should be retained (i.e. the data should not be compressed).&lt;/p&gt;
&lt;h2 id=&#34;acknowledgments&#34;&gt;Acknowledgments&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;d like to thank &lt;a href=&#34;https://pierreablin.com/&#34;&gt;Pierre Ablin&lt;/a&gt; for his very helpful comments on an earlier version of this post.&lt;/p&gt;
&lt;h2 id=&#34;download-the-code&#34;&gt;Download the code&lt;/h2&gt;
&lt;p&gt;The script &lt;a href=&#34;https://cbrnr.github.io/code/pca-zca.py&#34;&gt;pca-zca.py&lt;/a&gt; contains all relevant code snippets from this post.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/python/">Python</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eeg/">EEG</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/ica/">ICA</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Removing eye activity from EEG signals via ICA</title>
                <link>https://cbrnr.github.io/posts/removing-eog-ica/</link>
                <guid isPermaLink="true">https://cbrnr.github.io/posts/removing-eog-ica/</guid>
                <pubDate>Mon, 29 Jan 2018 00:00:00 &#43;0000</pubDate>
                
                    <author>clemens.brunner@gmail.com (Clemens Brunner)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;In &lt;a href=&#34;https://cbrnr.github.io/posts/removing-eog-regression/&#34;&gt;this previous post&lt;/a&gt;, we used linear regression to remove ocular artifacts from EEG signals. A popular alternative to this approach is independent component analysis (ICA). In a nutshell, ICA decomposes multi-channel EEG recordings into maximally independent components. Components that represent ocular activity can be identified and eliminated to reconstruct artifact-free EEG signals. This approach is described in more detail in &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/1469-8986.3720163/abstract&#34;&gt;Jung et al. (2000)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A comprehensive comparison between the two methods is beyond the scope of this post. Instead, I will only list some important properties of both techniques.&lt;/p&gt;
&lt;p&gt;First, the regression-based approach requires EOG channels, whereas ICA works without any reference signal.&lt;/p&gt;
&lt;p&gt;Both methods potentially remove brain activity in addition to ocular activity. ICA can separate ocular components from brain components well if many EEG channels are available (which in turn requires a relatively large number of data samples). A cleaner separation also means that less brain activity will be removed when ocular components are eliminated. The minimum number of EEG channels required for ICA decomposition varies, but as a rule of thumb at least 20 channels seem to be necessary (the &lt;a href=&#34;https://sccn.ucsd.edu/wiki/Chapter_09:_Decomposing_Data_Using_ICA#Running_ICA_decompositions&#34;&gt;EEGLAB tutorial&lt;/a&gt; has more details on the amount of data required for ICA). In constrast, the regression approach works even if only a few EEG channels are available. However, the EOG reference channels always contain some amount of brain activity, which will also be removed from the data in addition to ocular activity.&lt;/p&gt;
&lt;p&gt;The ICA method entails manual identification of ocular components, although several algorithms exist to automate this process (for example &lt;a href=&#34;https://github.com/bigdelys/eye-catch&#34;&gt;EyeCatch&lt;/a&gt; or &lt;a href=&#34;http://onlinelibrary.wiley.com/doi/10.1111/j.1469-8986.2010.01061.x/abstract&#34;&gt;ADJUST&lt;/a&gt;). ICA also takes longer to compute than the regression approach (but efficient implementations are available that keep computation time to a minimum). Finally, ICA is an optimization problem that is not guaranteed to find the globally optimal solution. Depending on the initial conditions, the algorithm might find different independent components from run to run. However, this is not a huge issue in this application, because ocular components are relatively stable across decompositions.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now turn to an example to see how ICA can be used to remove ocular artifacts with &lt;a href=&#34;https://mne.tools&#34;&gt;MNE&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;p&gt;Before we start, it is worth mentioning that ICA will generally run faster using a multi-threaded numeric library such as Intel MKL or OpenBLAS. If you have an Anaconda environment (which I recommend in &lt;a href=&#34;https://cbrnr.github.io/posts/setting-up-python/&#34;&gt;this previous post&lt;/a&gt;), you should already have NumPy with MKL support.&lt;/p&gt;
&lt;h3 id=&#34;data-preprocessing&#34;&gt;Data preprocessing&lt;/h3&gt;
&lt;p&gt;We will use the same data set that we already used with the regression approach. Specifically, we&amp;rsquo;ll use participant A01T from data set 001-2014 from the &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets&#34;&gt;BNCI Horizon 2020&lt;/a&gt; website (check out the &lt;a href=&#34;https://cbrnr.github.io/posts/removing-eog-regression/&#34;&gt;post on the regression approach&lt;/a&gt; for more details on this data set). Download &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat&#34;&gt;this file&lt;/a&gt; and save it to your working directory. Note that this data set contains 22 EEG and 3 EOG channels. Although EOG channels can (and should) be used for ICA decomposition (provided that they use the same reference electrode as the EEG channels), we will only use EEG channels here to keep things simple.&lt;/p&gt;
&lt;p&gt;As always, we start by firing up IPython and performing the usual initial steps:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scipy.io&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loadmat&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mne&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The data comes in a MAT file, so we use SciPy&amp;rsquo;s &lt;code&gt;loadmat&lt;/code&gt; function to load it as a NumPy array. Note that this time we only load the fourth run containing the actual experimental data – we do not need the calibration run.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loadmat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;A01T.mat&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;simplify_cells&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;eeg&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1e-6&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# convert to volts&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We will plot ICA components as projections on the scalp surface later on. To this end, MNE needs to know the channel labels, which unfortunately are not present in the data. However, the &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets/001-2014/description.pdf&#34;&gt;data description&lt;/a&gt; contains a picture of the montage, which we can use to populate a list of channel names.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ch_names&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Fz&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;FC3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;FC1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;FCz&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;FC2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;FC4&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;C5&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;C3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;C1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Cz&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;C2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;C4&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;C6&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;CP3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;CP1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;CPz&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;CP2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;CP4&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;P1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;Pz&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
            &lt;span class=&#34;s2&#34;&gt;&amp;#34;P2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;POz&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;EOG1&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;EOG2&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;EOG3&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We create an &lt;code&gt;info&lt;/code&gt; object using this list, which we need to create a &lt;code&gt;Raw&lt;/code&gt; object containing the EEG data and associated meta information (which in our case is just the sampling frequency of 250 Hz and the channel types). Finally, we add a standard 10–20 montage, which maps the channel labels to their locations on the scalp. This is required for topographic plots.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;info&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ch_names&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;250&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ch_types&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;eeg&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;eog&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;io&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RawArray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eeg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_montage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;standard_1020&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;performing-ica&#34;&gt;Performing ICA&lt;/h3&gt;
&lt;p&gt;ICA does not work in the presence of low-frequency drifts, so we create a copy of our &lt;code&gt;raw&lt;/code&gt; object and apply a high-pass filter to this copy.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw_tmp&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;copy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw_tmp&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;filter&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;l_freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;h_freq&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;None&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We are now ready to perform ICA. First, we instantiate an &lt;code&gt;ICA&lt;/code&gt; object and specify that we want to use the extended Infomax algorithm. Note that you can (and should) use the &lt;code&gt;picard&lt;/code&gt; method from the &lt;code&gt;python-picard&lt;/code&gt; package – this algorithm computes the extended Infomax solution much faster (see &lt;a href=&#34;https://pierreablin.github.io/picard/&#34;&gt;here&lt;/a&gt; for more details). In any case, the &lt;code&gt;random_state&lt;/code&gt; argument should be set for reproducible results.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ica&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;preprocessing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;ICA&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;method&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;infomax&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;fit_params&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;extended&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt;
                            &lt;span class=&#34;n&#34;&gt;random_state&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we fit &lt;code&gt;ica&lt;/code&gt; to our filtered raw data &lt;code&gt;raw_tmp&lt;/code&gt; (note that this uses only the 22 EEG channels and ignores the 3 EOG channels by default, but this can be changed with the &lt;code&gt;picks&lt;/code&gt; argument).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ica&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;fit&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_tmp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;identifying-ocular-components&#34;&gt;Identifying ocular components&lt;/h3&gt;
&lt;p&gt;Our next task is to identify ocular components. This is usually done by visual inspection, so we start by plotting all 22 independent components.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ica&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot_components&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inst&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_tmp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;picks&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nb&#34;&gt;range&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;A new figure with the 22 independent components will pop up. Let&amp;rsquo;s focus on the first few components, because ocular components are generally found among these.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/ica_components.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;From these scalp projections, the component labeled as &lt;em&gt;ICA001&lt;/em&gt; looks like it could represent eye movements because of its frontal location. To be sure, we can click on this component to open a new window with more details on this component (this is possible because we specified &lt;code&gt;inst=raw_tmp&lt;/code&gt; in the previous call):&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/ica001_properties.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Besides the scalp projection, we can now also inspect&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the component power spectral density (which is typical for ocular activity because the characteristic EEG alpha peak is missing),&lt;/li&gt;
&lt;li&gt;the epochs image, which color-codes the component activity over (virtual) epochs (which shows typical intermittent activity as blue and red stripes),&lt;/li&gt;
&lt;li&gt;and the epochs variance (which in this case is not really helpful in identifying the component).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, we can be pretty sure that component &lt;em&gt;ICA001&lt;/em&gt; represents ocular activity. To be extra safe, let&amp;rsquo;s plot the component time courses to corroborate our assumption:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ica&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot_sources&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inst&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_tmp&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/ica_sources.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Indeed, if you scroll through the data, &lt;em&gt;ICA001&lt;/em&gt; captures primarily eye movements and eye blinks.&lt;/p&gt;
&lt;p&gt;Note that usually two ocular components can be found in the decomposition, but this is not the case in our example data (all remaining components do not seem to originate from eye activity).&lt;/p&gt;
&lt;h3 id=&#34;removing-ocular-components&#34;&gt;Removing ocular components&lt;/h3&gt;
&lt;p&gt;In the final step, we create a list attribute &lt;code&gt;ica.exclude&lt;/code&gt; containing the indices of all components that should be removed when reconstructing EEG signals. In our case, this list contains only a single component.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;ica&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;exclude&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that you can click on the component title (&lt;em&gt;ICA001&lt;/em&gt;) in the ICA components plot created with &lt;code&gt;ica.plot_components(inst=raw_tmp, picks=range(22))&lt;/code&gt; to include/exclude a component (the title of an excluded component will turn gray). This will also add/remove this component from/to the underlying &lt;code&gt;ica.exclude&lt;/code&gt; list.&lt;/p&gt;
&lt;p&gt;Now we can apply our ICA results (without the excluded component) to a copy of the original (unfiltered) EEG to obtain artifact-free signals:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw_corrected&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;copy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;ica&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;apply&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw_corrected&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;visualizing-results&#34;&gt;Visualizing results&lt;/h3&gt;
&lt;p&gt;So how did ICA perform? Let&amp;rsquo;s take a look at a segment of the original EEG containing a clear eye movment artifact:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;53&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;duration&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Before&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/original_with_eog.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;And here is the corrected signal:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw_corrected&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;53&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;duration&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;After&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/ica_corrected_without_eog.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Looks like ICA did a pretty decent job in removing eye artifacts.&lt;/p&gt;
&lt;h2 id=&#34;download-the-code&#34;&gt;Download the code&lt;/h2&gt;
&lt;p&gt;The script &lt;a href=&#34;https://cbrnr.github.io/code/removing-eog-ica.py&#34;&gt;removing-eog-ica.py&lt;/a&gt; contains all relevant code snippets from this post.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/python/">Python</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/mne/">MNE</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eeg/">EEG</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eog/">EOG</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/artifacts/">Artifacts</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/ica/">ICA</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Visualizing EEG data with MNE</title>
                <link>https://cbrnr.github.io/posts/visualizing-eeg-data/</link>
                <guid isPermaLink="true">https://cbrnr.github.io/posts/visualizing-eeg-data/</guid>
                <pubDate>Tue, 28 Nov 2017 00:00:00 &#43;0000</pubDate>
                
                    <author>clemens.brunner@gmail.com (Clemens Brunner)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;In this tutorial, we will continue to use the &lt;a href=&#34;https://physionet.org/content/eegmmidb/1.0.0/&#34;&gt;EEG motor movement/imagery data set&lt;/a&gt; from the &lt;a href=&#34;https://cbrnr.github.io/posts/loading-eeg-data/&#34;&gt;previous post&lt;/a&gt; (again, we use &lt;a href=&#34;https://physionet.org/files/eegmmidb/1.0.0/S001/S001R04.edf?download&#34;&gt;run 4 of subject 1&lt;/a&gt;). I&amp;rsquo;ll quickly reiterate all important steps we have performed so far:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mne&lt;/span&gt;

&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;io&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_raw_edf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;S001R04.edf&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;preload&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rename_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_montage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;standard_1020&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;match_case&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_eeg_reference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;average&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;interactive-visualization&#34;&gt;Interactive visualization&lt;/h2&gt;
&lt;p&gt;We can now open an interactive visualization window as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/plot_raw_initial.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The signals don&amp;rsquo;t look quite right yet, so let&amp;rsquo;s walk through some interactive features of this plot. First, you can rescale the signals with the + and – keys. The default scaling is not really appropriate here, but pressing the – key a few times will result in a much nicer display. The topmost channel label Fc5 includes a purple scale bar which indicates the current scale of the data (the length of the purple bar corresponds to 40 µV). This scale applies to all channels of the same type (so in our example to all EEG channels). If this scale gets in your way, you can toggle it with the &lt;em&gt;s&lt;/em&gt; key.&lt;/p&gt;
&lt;p&gt;Notice that 20 channels are visible per page by default. To view the remaining channels, you can scroll down with the ↓ key. Conversely, you can scroll back up again by pressing the ↑ key. Alternatively, you can increase or decrease the number of channels visible on a page with the Page Up or Page Down keys, respectively (on a Mac, these keys can usually be accessed with the combinations fn + ↑ or fn + ↓).&lt;/p&gt;
&lt;p&gt;By default, 10 seconds of data are visible per page. If you want to navigate in time, you can use the → and ← keys to move forward and backward by a quarter of the visible duration (in this example by 2.5 seconds). Pressing Shift + → and Shift + ← will scroll forward/backward by a whole page. You can increase or decrease the amount of time shown per page with the End or Home keys (on a Mac, these keys can usually be accessed with fn + → and fn + ←).&lt;/p&gt;
&lt;p&gt;The following table summarizes all keyboard shortcuts for the interactive visualization window (we will discuss annotation, butterfly, and zen modes in a minute). Note that the Help button in the lower left corner (or pressing the &lt;em&gt;?&lt;/em&gt; key) pops up a dialog window with these shortcuts.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;th&gt;Keyboard shortcut&lt;/th&gt;
&lt;th&gt;Mac equivalent&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Scale up&lt;/td&gt;
&lt;td&gt;+&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale down&lt;/td&gt;
&lt;td&gt;–&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scroll up&lt;/td&gt;
&lt;td&gt;↑&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scroll down&lt;/td&gt;
&lt;td&gt;↓&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scroll left (quarter page)&lt;/td&gt;
&lt;td&gt;←&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scroll right (quarter page)&lt;/td&gt;
&lt;td&gt;→&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scroll left (whole page)&lt;/td&gt;
&lt;td&gt;Shift + ←&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scroll right (whole page)&lt;/td&gt;
&lt;td&gt;Shift + →&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;More channels&lt;/td&gt;
&lt;td&gt;Page Up&lt;/td&gt;
&lt;td&gt;fn + ↑&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fewer channels&lt;/td&gt;
&lt;td&gt;Page Down&lt;/td&gt;
&lt;td&gt;fn + ↓&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;More time&lt;/td&gt;
&lt;td&gt;End&lt;/td&gt;
&lt;td&gt;fn + →&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Less time&lt;/td&gt;
&lt;td&gt;Home&lt;/td&gt;
&lt;td&gt;fn + ←&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Scale bars&lt;/td&gt;
&lt;td&gt;s&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Zen mode&lt;/td&gt;
&lt;td&gt;z&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle DC removal&lt;/td&gt;
&lt;td&gt;d&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Annotation mode&lt;/td&gt;
&lt;td&gt;a&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Butterfly mode&lt;/td&gt;
&lt;td&gt;b&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Toggle draggable annotations&lt;/td&gt;
&lt;td&gt;p&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;You can change initial settings with arguments to &lt;code&gt;raw.plot&lt;/code&gt;. For example, we might use the following arguments to start with suitable values for our example data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;64&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;duration&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;scalings&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;{&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;eeg&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;75e-6&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;},&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Our example data contains annotations (stored in &lt;code&gt;raw.annotations&lt;/code&gt;), which are visualized as colored patches (the annotation descriptions are located at the top of the plot). In this case, the blue, red, and green patches correspond to T0, T1, and T2 annotations, respectively.&lt;/p&gt;
&lt;p&gt;In addition to keyboard shortcuts, you can also use the mouse to navigate through the data. You might have already noticed the horizontal bar below the plot. This overview bar summarizes the whole time range of the signal, and the currently visible time segment is highlighted in (light) gray. You can click inside the overview bar to quickly skip around to different time segments. Furthermore, the overview bar also shows annotations.&lt;/p&gt;
&lt;p&gt;To quickly identify time segments with high variance, you can switch to butterfly mode by pressing the &lt;em&gt;b&lt;/em&gt; key. Signals of all channels of the same type will be collapsed onto each other, which makes it easy to spot abnormal activity. Press &lt;em&gt;b&lt;/em&gt; again to exit butterfly mode.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/plot_raw_butterfly.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you want to focus on the actual signals with fewer distractions, you can get rid of the scroll and overview bars. Press &lt;em&gt;z&lt;/em&gt; (zen mode) to toggle these user interface elements.&lt;/p&gt;
&lt;p&gt;Finally, you can quickly filter out offsets (slow drifts) in the signals by pressing the &lt;em&gt;d&lt;/em&gt; key (toggle DC removal).&lt;/p&gt;
&lt;h2 id=&#34;annotations&#34;&gt;Annotations&lt;/h2&gt;
&lt;p&gt;We are now ready to interactively create annotations, for example to mark segments containing artifacts. To this end, we switch to annotation mode by pressing the &lt;em&gt;a&lt;/em&gt; key. A small dialog window will pop up. Make sure to keep this dialog window open as long as you want to stay in annotation mode – if you close it, you will return to normal visualization mode.&lt;/p&gt;
&lt;p&gt;The annotations dialog shows a list of currently available annotations (T0, T1, and T2 in our example data). The annotation label marked with a filled circle is currently active (T0), which means that if we create a new annotation it will be of this type. However, we can also add new annotation types. Notice that the &amp;ldquo;BAD_&amp;rdquo; label is actually an input text field – you can start typing/editing to change the annotation label before adding. We would like to create annotations called &amp;ldquo;BAD&amp;rdquo;, so let&amp;rsquo;s remove the underscore by pressing the backspace key. The annotation gets added to the list of available labels once you click on the large &amp;ldquo;Add new label&amp;rdquo; button or hit the Enter key.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/annotations.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Note that annotation labels starting with &amp;ldquo;BAD&amp;rdquo; (or &amp;ldquo;bad&amp;rdquo;) are special in MNE, because many functions ignore data contained within such annotations. This gives us a nice way to mark artifact segments without completely discarding data points.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s create some &amp;ldquo;BAD&amp;rdquo; annotations in our example data. First, we make sure that the &amp;ldquo;BAD&amp;rdquo; label is active by clicking inside the red ring (inactive labels are not filled). Optionally, we can hide all T0, T1, and T2 annotations by clicking on their &amp;ldquo;show/hide&amp;rdquo; checkboxes. Now we switch back to the main visualization window (again, make sure not to close the annotation window). Using the mouse, we can now click and drag to select a portion of the data and mark it with the currently active &amp;ldquo;BAD&amp;rdquo; label. During this process, you can interactively scroll around in the data with the keyboard shortcuts listed before, or you can click within the overview bar below the plot.&lt;/p&gt;
&lt;p&gt;If you want to edit the start and/or end point of an existing annotation, press the &lt;em&gt;p&lt;/em&gt; key to toggle snap mode. If snap mode is enabled, you can drag the start and/or end points of an annotation to the desired locations. It is impossible to create an annotation inside another annotation with snap mode enabled, so that&amp;rsquo;s when you want to turn off snap mode. If you want to completely delete an annotation, you can right-click on it and it disappears.&lt;/p&gt;
&lt;p&gt;When you are done, you can close the annotation window. You can also close the visualization window, because all annotations are automatically stored in the &lt;code&gt;raw.annotations&lt;/code&gt; attribute. Let&amp;rsquo;s take a look what it looks like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;annotations&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;Annotations | 39 segments: BAD (9), T0 (15), T1 (8), T2 (7)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Yay, we&amp;rsquo;ve just created 9 new annotations, all of which are interpreted as bad (because they start with the word &amp;ldquo;BAD&amp;rdquo; or &amp;ldquo;bad&amp;rdquo;).&lt;/p&gt;
&lt;h2 id=&#34;marking-channels&#34;&gt;Marking channels&lt;/h2&gt;
&lt;p&gt;Sometimes, signals are noisy in particular channels during the whole recording. In such cases, it is possible to mark the whole channel as bad. In the visualization window, we can click on a channel label on the left side of the plot or on a channel trace to mark a channel as bad (the label and associated time course will turn gray to reflect this). If you want to unmark a channel, click on its label (or its trace) again. The selection of bad channels is immediately stored in &lt;code&gt;raw.info[&amp;quot;bads&amp;quot;]&lt;/code&gt; (a list containing the channel labels of all bad channels). Channels marked as bad are generally ignored by MNE functions.&lt;/p&gt;
&lt;h2 id=&#34;saving-and-loading-annotations&#34;&gt;Saving and loading annotations&lt;/h2&gt;
&lt;p&gt;Although changes to annotations or bad channels are immediately reflected in &lt;code&gt;raw.annotations&lt;/code&gt; and &lt;code&gt;raw.info[&amp;quot;bads&amp;quot;]&lt;/code&gt;, we still need a way to permanently store this information on disk for later use. Consequently, we will also need a way to load this information and add it to an existing &lt;code&gt;raw&lt;/code&gt; object.&lt;/p&gt;
&lt;p&gt;It is always a good idea to use simple storage formats. Therefore, MNE allows us to store annotations as simple text files. Here&amp;rsquo;s how we could save annotations in a file called &lt;code&gt;S001R04_annotations.txt&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;save&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;S001R04_annotations.txt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This creates a comma-separated text file with three columns &amp;ldquo;onset&amp;rdquo;, &amp;ldquo;duration&amp;rdquo;, and &amp;ldquo;description&amp;rdquo;. Onsets are relative to the start of the recording (so they start at zero), and both onsets and durations are measured in seconds. If you specify a &lt;code&gt;.csv&lt;/code&gt; instead of a &lt;code&gt;.txt&lt;/code&gt; extension, you will get absolute time stamps for the onsets (based on the measurement onset time stamp).&lt;/p&gt;
&lt;p&gt;We can read this file with:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;annotations&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;S001R04_annotations.txt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, we can associate these &lt;code&gt;annotations&lt;/code&gt; with an existing &lt;code&gt;raw&lt;/code&gt; object:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;annotations&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;saving-and-loading-bad-channels&#34;&gt;Saving and loading bad channels&lt;/h2&gt;
&lt;p&gt;The following lines of code store the list of bad channels in a text file called &lt;code&gt;S001R04_bads.txt&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;S001R04_bads.txt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;w&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;join&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;bads&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]))&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;write&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;se&#34;&gt;\n&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To load this file and update the &lt;code&gt;raw&lt;/code&gt; object, we can use:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;k&#34;&gt;with&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;open&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;S001R04_bads.txt&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;
    &lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;bads&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;f&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;split&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;,&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/python/">Python</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/mne/">MNE</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eeg/">EEG</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/artifacts/">Artifacts</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Loading EEG data in Python</title>
                <link>https://cbrnr.github.io/posts/loading-eeg-data/</link>
                <guid isPermaLink="true">https://cbrnr.github.io/posts/loading-eeg-data/</guid>
                <pubDate>Mon, 23 Oct 2017 00:00:00 &#43;0000</pubDate>
                
                    <author>clemens.brunner@gmail.com (Clemens Brunner)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;mne&#34;&gt;MNE&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mne.tools&#34;&gt;MNE&lt;/a&gt; is a popular Python package for EEG/MEG processing. It offers a wide variety of useful tools including reading and writing of various file formats, filtering, independent component analysis (ICA), forward modeling, inverse solutions, time-frequency decompositions, visualization, and more. In fact, if you are familiar with &lt;a href=&#34;https://sccn.ucsd.edu/eeglab/index.php&#34;&gt;EEGLAB&lt;/a&gt; you might find that you can perform many similar analyses with Python and MNE. In this post I will show how to load EEG data as well as view and edit associated meta information.&lt;/p&gt;
&lt;h2 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;The first step in almost any EEG processing pipeline is loading the raw data files into memory. There are many different file formats for storing EEG data, mostly because each EEG amplifier manufacturer uses its own data format. For example, &lt;a href=&#34;http://www.brainproducts.com/&#34;&gt;BrainProducts&lt;/a&gt; amplifiers store data as an EEG/VHDR/VMRK file triplet, &lt;a href=&#34;https://www.biosemi.com/&#34;&gt;BioSemi&lt;/a&gt; amplifiers create BDF files (an extension of the &lt;a href=&#34;https://www.edfplus.info/&#34;&gt;European Data Format&lt;/a&gt;), &lt;a href=&#34;https://compumedicsneuroscan.com/&#34;&gt;Neuroscan&lt;/a&gt; amplifiers use proprietary CNT files, and so on. Luckily, MNE comes with support for many popular EEG file formats.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://physionet.org/content/eegmmidb/1.0.0/&#34;&gt;EEG motor movement/imagery data set&lt;/a&gt; we will use in this tutorial was contributed to the public domain by the developers of the &lt;a href=&#34;https://www.bci2000.org/mediawiki/index.php/Main_Page&#34;&gt;BCI2000&lt;/a&gt; system. In brief, they recorded 64-channel EEG from over 100 participants during motor execution and motor imagery tasks. In this tutorial, we will analyze only one participant and only one motor imagery run. If you want to follow along, go ahead and download &lt;a href=&#34;https://physionet.org/files/eegmmidb/1.0.0/S001/S001R04.edf?download&#34;&gt;run 4 of subject 1&lt;/a&gt; (note that MNE has a dedicated loader in &lt;code&gt;mne.datasets.eegbci&lt;/code&gt; for this data set, which makes it even easier to load particular subjects and runs). All subsequent commands assume that the file is located in the current working directory.&lt;/p&gt;
&lt;p&gt;Now that we have selected our data, it is time to fire up Python. I recommend &lt;a href=&#34;https://ipython.org/&#34;&gt;IPython&lt;/a&gt; as an enhanced interactive Python shell (go ahead and read my article on &lt;a href=&#34;https://cbrnr.github.io/posts/setting-up-python/&#34;&gt;how to set up Python for EEG analysis&lt;/a&gt; if you do not have a working Python environment yet). You can start IPython from a terminal by typing &lt;code&gt;ipython&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We want to use the MNE package, so we have to import it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mne&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You can check the version of MNE as follows (make sure you always use the latest version):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;__version__&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&#39;0.22.0&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;loading-eeg-data&#34;&gt;Loading EEG data&lt;/h2&gt;
&lt;p&gt;Loading EEG data works as follows. First, we need to know the file type of the file we want to load. In our case, the data is stored in an &lt;a href=&#34;https://www.edfplus.info/&#34;&gt;EDF&lt;/a&gt; file called &lt;code&gt;S001R04.edf&lt;/code&gt;. Second, Python needs to know exactly where this file is located. We can either specify the full path to the file, or we can make sure that the file is in the current working directory, in which case the file name alone is sufficient. In this case, we can use the following command to load the raw data:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;io&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;read_raw_edf&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;S001R04.edf&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;preload&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The argument &lt;code&gt;preload=True&lt;/code&gt; means that MNE performs the actual loading process immediately instead of the default lazy behavior. Also note that there are many more reader functions available in the &lt;code&gt;mne.io&lt;/code&gt; package, including &lt;code&gt;mne.io.read_raw_bdf&lt;/code&gt;, &lt;code&gt;mne.io.read_raw_brainvision&lt;/code&gt;, &lt;code&gt;mne.io.read_raw_cnt&lt;/code&gt;, and &lt;code&gt;mne.io.read_raw_eeglab&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I won&amp;rsquo;t reproduce the output that is generated during the loading process here. Usually, it contains some more or less useful logging messages, so if anything goes wrong make sure to carefully study these messages. If you don&amp;rsquo;t want to see these messages, you can suppress them as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_log_level&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;WARNING&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will instruct MNE to only print out warnings and errors.&lt;/p&gt;
&lt;h2 id=&#34;viewing-and-editing-meta-information&#34;&gt;Viewing and editing meta information&lt;/h2&gt;
&lt;p&gt;The previous assignment generated a &lt;a href=&#34;https://mne.tools/stable/generated/mne.io.Raw.html&#34;&gt;&lt;code&gt;Raw&lt;/code&gt;&lt;/a&gt; object in our workspace. This object holds the actual EEG data and associated meta information. We can get some basic information by inspecting this object in the interactive Python session:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;RawEDF | S001R04.edf, 64 x 20000 (125.0 s), ~9.8 MB, data loaded&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can see the file name, the number of channels and sample points, the length in seconds, and the approximate size of the data in memory.&lt;/p&gt;
&lt;h3 id=&#34;the-meta-information-attribute&#34;&gt;The meta information attribute&lt;/h3&gt;
&lt;p&gt;We can dig deeper into the meta information by inspecting the &lt;code&gt;info&lt;/code&gt; attribute associated with &lt;code&gt;raw&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;Info | 7 non-empty values
 bads: []
 ch_names: Fc5., Fc3., Fc1., Fcz., Fc2., Fc4., Fc6., C5.., C3.., C1.., ...
 chs: 64 EEG
 custom_ref_applied: False
 highpass: 0.0 Hz
 lowpass: 80.0 Hz
 meas_date: 2009-08-12 16:15:00 UTC
 nchan: 64
 projs: []
 sfreq: 160.0 Hz
&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There are seven non-empty values in this attribute. For example, the line &lt;code&gt;chs: 64 EEG&lt;/code&gt; near the top of the output tells us that there are 64 EEG channels (the first few channel names are listed in the line starting with &lt;code&gt;ch_names&lt;/code&gt;). Individual elements of &lt;code&gt;raw.info&lt;/code&gt; can be accessed with dictionary-like indexing. For example, the sampling frequency is stored in:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;sfreq&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;160.0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Other useful &lt;code&gt;info&lt;/code&gt; keys are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;bads&amp;quot;&lt;/code&gt;: A list of noisy (bad) channels which should be ignored in further analyses. Initially, this list is empty (as in our example), but we will manually populate it &lt;a href=&#34;https://cbrnr.github.io/posts/visualizing-eeg-data/&#34;&gt;in a later post&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;ch_names&amp;quot;&lt;/code&gt;: A list of channel names.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;chs&amp;quot;&lt;/code&gt;: A detailed list of channel properties, including their types (for example, &lt;code&gt;EEG&lt;/code&gt;, &lt;code&gt;EOG&lt;/code&gt; or &lt;code&gt;MISC&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;highpass&amp;quot;&lt;/code&gt; and &lt;code&gt;&amp;quot;lowpass&amp;quot;&lt;/code&gt;: Highpass and lowpass edge frequencies that were used during recording.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;&amp;quot;meas_date&amp;quot;&lt;/code&gt;: The recording date (a &lt;code&gt;datetime.datetime&lt;/code&gt; object).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;renaming-channels&#34;&gt;Renaming channels&lt;/h3&gt;
&lt;p&gt;The output of &lt;code&gt;raw.info&lt;/code&gt; revealed that some channel names are suffixed with one or more dots. Since these are non-standard names, let&amp;rsquo;s rename the channels by removing all trailing dots:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;rename_channels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;lambda&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;s&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;strip&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;.&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;assigning-a-montage&#34;&gt;Assigning a montage&lt;/h3&gt;
&lt;p&gt;For good measure (and for later use), we can assign a montage to the data (a montage relates channels names to standardized or actual locations on the scalp surface). First, let&amp;rsquo;s list all montages that ship with MNE.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;get_builtin_montages&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;[&#39;EGI_256&#39;,
 &#39;GSN-HydroCel-128&#39;,
 &#39;GSN-HydroCel-129&#39;,
 &#39;GSN-HydroCel-256&#39;,
 &#39;GSN-HydroCel-257&#39;,
 &#39;GSN-HydroCel-32&#39;,
 &#39;GSN-HydroCel-64_1.0&#39;,
 &#39;GSN-HydroCel-65_1.0&#39;,
 &#39;biosemi128&#39;,
 &#39;biosemi16&#39;,
 &#39;biosemi160&#39;,
 &#39;biosemi256&#39;,
 &#39;biosemi32&#39;,
 &#39;biosemi64&#39;,
 &#39;easycap-M1&#39;,
 &#39;easycap-M10&#39;,
 &#39;mgh60&#39;,
 &#39;mgh70&#39;,
 &#39;standard_1005&#39;,
 &#39;standard_1020&#39;,
 &#39;standard_alphabetic&#39;,
 &#39;standard_postfixed&#39;,
 &#39;standard_prefixed&#39;,
 &#39;standard_primed&#39;]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;According to the &lt;a href=&#34;https://physionet.org/content/eegmmidb/1.0.0/&#34;&gt;data set documentation&lt;/a&gt;, the channel locations conform to the international 10–10 system. MNE does not seem to ship a 10–10 montage, but &lt;code&gt;standard_1020&lt;/code&gt; contains template locations from the extended 10–20 system:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;montage&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;make_standard_montage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;standard_1020&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;montage&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/montage.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;It looks like this montage contains all channels used in our example data set. Therefore, we can assign this montage to our &lt;code&gt;raw&lt;/code&gt; object.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_montage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;montage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;match_case&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;False&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;re-referencing&#34;&gt;Re-referencing&lt;/h3&gt;
&lt;p&gt;The data documentation does not mention any reference electrode, but it is safe to assume that all channels are referenced to some standard location such as a mastoid or the nose. Often, we want to re-reference EEG data to the so-called average reference (the average over all recording channels). In MNE, we can compute average referenced signals as follows:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;set_eeg_reference&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;average&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that in general, methods modify MNE objects in-place.&lt;/p&gt;
&lt;h3 id=&#34;annotations&#34;&gt;Annotations&lt;/h3&gt;
&lt;p&gt;Finally, many EEG data sets come with discrete events, either in the form of an analog stimulation channel or (text) annotations. Our example data set contains annotations that can be accessed with the &lt;code&gt;annotations&lt;/code&gt; attribute:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;annotations&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;&amp;lt;Annotations | 30 segments: T0 (15), T1 (8), T2 (7)&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We see that there are 30 annotations in total. There are three kinds of annotations named T0, T1, and T2. According to the &lt;a href=&#34;https://physionet.org/content/eegmmidb/1.0.0/&#34;&gt;data set description&lt;/a&gt;, T0 corresponds to rest, T1 corresponds to motion onset of the left fist, and T2 corresponds to motion onset of the right fist.&lt;/p&gt;
&lt;p&gt;If you encounter a file with an analog stimulation channel (this is typically the case for data recorded with BioSemi amplifiers), you need to extract discrete events from this channel as a first step. The &lt;code&gt;mne.find_events&lt;/code&gt; function converts information contained in an analog stimulation channel to a NumPy array of shape &lt;code&gt;(N, 3)&lt;/code&gt;, where &lt;code&gt;N&lt;/code&gt; (the number of rows) is the number of detected events. The first column contains event onsets (in samples), whereas the third column contains (integer) event codes. The second column contains the values of the stimulation channel one sample before the detected events (this column can usually be ignored).&lt;/p&gt;
&lt;p&gt;This NumPy array can be converted to an &lt;code&gt;Annotations&lt;/code&gt; object using &lt;code&gt;mne.annotations_from_events&lt;/code&gt;, which is often necessary for further analyses (note that you can associate an existing &lt;code&gt;Annotations&lt;/code&gt; object with a &lt;code&gt;Raw&lt;/code&gt; object by calling the &lt;code&gt;raw.set_annotations&lt;/code&gt; method).&lt;/p&gt;
&lt;p&gt;In the next post, I will demonstrate how to visualize this data set and how to interactively mark bad channels and bad segments.&lt;/p&gt;
&lt;h2 id=&#34;download-the-code&#34;&gt;Download the code&lt;/h2&gt;
&lt;p&gt;The script &lt;a href=&#34;https://cbrnr.github.io/code/loading-eeg-data.py&#34;&gt;loading-eeg-data.py&lt;/a&gt; contains all relevant code snippets from this post.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/python/">Python</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/mne/">MNE</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eeg/">EEG</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Removing eye activity from EEG signals via regression</title>
                <link>https://cbrnr.github.io/posts/removing-eog-regression/</link>
                <guid isPermaLink="true">https://cbrnr.github.io/posts/removing-eog-regression/</guid>
                <pubDate>Fri, 20 Oct 2017 00:00:00 &#43;0000</pubDate>
                
                    <author>clemens.brunner@gmail.com (Clemens Brunner)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Eye movement and eye blinks are clearly visible in the ongoing EEG. These ocular artifacts are produced by changes in the electrical dipole between the front and back of the eyes. Usually, it is important to minimize the influence of eye activity on the recorded EEG.&lt;/p&gt;
&lt;p&gt;There are two popular methods available to remove ocular artifacts, namely approaches based on (multivariate linear) regression and approaches based on independent component analysis (ICA). In this post, I will focus on the regression method and leave the ICA-based variant for &lt;a href=&#34;https://cbrnr.github.io/posts/removing-eog-ica/&#34;&gt;another post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The regression-based approach in its simplest form dates back to &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0013469470901859&#34;&gt;Hillyard and Galambos (1970)&lt;/a&gt;, who used data from a pre-experimental calibration run containing voluntarily produced ocular artifacts to estimate regression coefficients on the EEG. More than a decade later, &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/0013469483901359&#34;&gt;Gratton, Coles, and Donchin (1983)&lt;/a&gt; improved upon this procedure in two ways. First, they used EOG recordings from the experimental data (the same data that needs to be corrected) instead of a calibration run. Second, they estimated regression coefficients separately for eye movements and eye blinks.&lt;/p&gt;
&lt;p&gt;Despite its age, removing ocular artifacts via linear regression remains a popular approach because it works well in many situations. However, it is important to keep in mind that dedicated EOG electrodes placed close to the eyes must be used. In particular, if an EOG electrode fails during the recording session, the whole method breaks down. Furthermore, it is reasonable to assume that EOG electrodes will record some amount of brain activity (after all, the brain sits right behind these sensors). Therefore, this method will likely remove brain activity in addition to ocular artifacts. On the plus side, there is no restriction on the number of EEG electrodes required – the regression-based approach can clean even single-channel EEG recordings.&lt;/p&gt;
&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;
&lt;h3 id=&#34;data-preprocessing&#34;&gt;Data preprocessing&lt;/h3&gt;
&lt;p&gt;To demonstrate this method, we will download a publicly available EEG data set from the &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets&#34;&gt;BNCI Horizon 2020&lt;/a&gt; website. Specifically, we&amp;rsquo;ll use participant A01T from data set 001-2014 (go ahead and download &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets/001-2014/A01T.mat&#34;&gt;this file&lt;/a&gt; and put it in your working directory if you want to follow along). The &lt;a href=&#34;http://bnci-horizon-2020.eu/database/data-sets/001-2014/description.pdf&#34;&gt;description&lt;/a&gt; mentions that the third run contains calibration eye movements. We will use these to estimate regression coefficients and correct EEG data in the subsequent fourth run.&lt;/p&gt;
&lt;p&gt;As always, we start with some setup commands. Since the example data is stored in a MAT file, we need to import &lt;code&gt;scipy.io.loadmat&lt;/code&gt;. We will also use &lt;code&gt;numpy&lt;/code&gt; in our analysis.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;numpy&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;np&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;from&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;scipy.io&lt;/span&gt; &lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loadmat&lt;/span&gt;
&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;mne&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Loading the example data is then pretty straightforward.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mat&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;loadmat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;A01T.mat&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;simplify_cells&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This gives us a &lt;code&gt;mat&lt;/code&gt; dictionary containing the data in a nicely organized way. Let&amp;rsquo;s first check the available dictionary keys.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;mat&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keys&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;dict_keys([&#39;__header__&#39;, &#39;__version__&#39;, &#39;__globals__&#39;, &#39;data&#39;])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The EEG data is stored in the &lt;code&gt;data&lt;/code&gt; key as a list, where each list element corresponds to a run. For example, &lt;code&gt;mat[&amp;quot;data&amp;quot;][0]&lt;/code&gt; contains data from the first run, &lt;code&gt;mat[&amp;quot;data&amp;quot;][1]&lt;/code&gt; corresponds to the second run, and so on. Each run is represented by a dictionary, and the EEG data is stored under the &lt;code&gt;&amp;quot;X&amp;quot;&lt;/code&gt; key.&lt;/p&gt;
&lt;p&gt;We are interested in EEG data from the calibration run (third run, list index &lt;code&gt;2&lt;/code&gt;) and the experimental run (fourth run, list index &lt;code&gt;3&lt;/code&gt;), which we store as two separate names &lt;code&gt;eeg1&lt;/code&gt; and &lt;code&gt;eeg2&lt;/code&gt; (note that we multiply by 10&lt;sup&gt;-6&lt;/sup&gt; to convert the units from microvolts to volts). This results in two NumPy arrays.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;eeg1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1e-6&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# run 3 (calibration)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;eeg2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;][&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mf&#34;&gt;1e-6&lt;/span&gt;  &lt;span class=&#34;c1&#34;&gt;# run 4 (experiment)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The next step is not necessary, but we&amp;rsquo;ll convert &lt;code&gt;eeg1&lt;/code&gt; and &lt;code&gt;eeg2&lt;/code&gt; into &lt;a href=&#34;https://mne.tools/stable/generated/mne.io.Raw.html&#34;&gt;MNE &lt;code&gt;Raw&lt;/code&gt;&lt;/a&gt; objects. This will give us the ability to quickly generate plots of raw EEG traces before and after ocular artifact removal.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;info&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;create_info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;250&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;ch_types&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;eeg&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;+&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;eog&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;*&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw1&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;io&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RawArray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eeg1&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw2&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;mne&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;io&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RawArray&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;eeg2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;info&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The arguments to &lt;code&gt;mne.create_info&lt;/code&gt; set the number of channels in the data (25), the sampling frequency (250 Hz), and the channel types (the first 22 channels are EEG whereas the last three channels are EOG). We have to transpose the arrays because MNE expects channels in rows and samples in columns.&lt;/p&gt;
&lt;h3 id=&#34;estimating-regression-coefficients&#34;&gt;Estimating regression coefficients&lt;/h3&gt;
&lt;p&gt;We are now ready to estimate regression coefficients to remove the influence of ocular artifacts on the EEG. It turns out that converting the three monopolar EOG channels EOG1, EOG2, and EOG3 to two bipolar channels is a good idea. One way to go about this is to multiply the monopolar EOG signals with a suitable matrix which generates bipolar derivations EOG1–EOG2 and EOG3–EOG2. If you happen to have four monopolar EOG channels (which is also a common EOG recording setup), you can convert them into a horizontal and a vertical bipolar channel, respectively.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;bip&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;array&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;([[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;],&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;mi&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]])&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw1_eog&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bip&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw2_eog&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;bip&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that we have performed this conversion for both runs &lt;code&gt;raw1&lt;/code&gt; and &lt;code&gt;raw2&lt;/code&gt; separately. Furthermore, indexing a &lt;code&gt;raw&lt;/code&gt; object returns a tuple with the requested data and associated time values. We only need the data, and &lt;code&gt;[0]&lt;/code&gt; selects just the first entry in the returned tuple.&lt;/p&gt;
&lt;p&gt;For shorter notation, we also create separate names for the EEG signals (the first 22 channels) of both runs.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw1_eeg&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw2_eeg&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:][&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now comes the important line where we perform &lt;a href=&#34;https://en.wikipedia.org/wiki/Ordinary_least_squares#Estimation&#34;&gt;ordinary least squares&lt;/a&gt; to get the linear regression coefficients. Note that we actually solve for all EEG channels simultaneously (or in other words, there are several dependent or response variables). We also have several independent or predictor variables in the form of EOG channels. Such a model is sometimes referred to as a multivariate (more than one response variable) multiple (more than one predictor variable) regression model.&lt;/p&gt;
&lt;p&gt;If we denote our response variables (the EEG signals) with $Y$, our predictor variables (the EOG signals) as $X$, and the regression coefficients as $B$, we can write the linear model as follows:&lt;/p&gt;
&lt;p&gt;$$Y = X B$$&lt;/p&gt;
&lt;p&gt;We can then compute the least squares solution for $B$ by left-multiplying with the &lt;a href=&#34;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&#34;&gt;Moore-Penrose inverse&lt;/a&gt; $X^+ = (X^T X)^{-1} X^T$:&lt;/p&gt;
&lt;p&gt;$$B = X^+ Y = (X^T X)^{-1} X^T Y$$&lt;/p&gt;
&lt;p&gt;In Python code, this looks as follows (note that we need to transpose our signals because we have our channels in rows, whereas the equation assumes that they are in columns):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;inv&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw1_eog&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1_eog&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1_eog&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1_eeg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Note that the &lt;code&gt;@&lt;/code&gt; operator computes the dot product of matrices.&lt;/p&gt;
&lt;p&gt;Alternatively, we can use the numerically more stable method &lt;code&gt;np.linalg.solve&lt;/code&gt; to compute the regression coefficients:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;np&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;linalg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;solve&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw1_eog&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1_eog&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1_eog&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw1_eeg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As a quick sanity check, let&amp;rsquo;s inspect the shape of our regression parameter matrix &lt;code&gt;b&lt;/code&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;shape&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;(2, 22)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This makes sense because there are two EOG channels (predictors) and 22 EEG channels (responses).&lt;/p&gt;
&lt;p&gt;Now all we need to do to remove ocular artifacts from new data is to subtract the estimated EOG influence from the measured EEG. We&amp;rsquo;ll create a new &lt;code&gt;raw3&lt;/code&gt; object to store this corrected data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;eeg_corrected&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;raw2_eeg&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;-&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw2_eog&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;@&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;b&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;T&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw3&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;raw2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;copy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;()&lt;/span&gt;
&lt;span class=&#34;n&#34;&gt;raw3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;_data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;[:&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;:]&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;eeg_corrected&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;visualizing-results&#34;&gt;Visualizing results&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s see if the method worked. First, we plot a segment of the original &lt;code&gt;raw2&lt;/code&gt; data with some prominent eye activity. For this visualization, we set the number of simultaneously visible channels to 25, the start of the plot to second 53, and the duration to 5 seconds.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw2&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;53&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;duration&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;Before&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/original_with_eog.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;We produce the same plot for the corrected &lt;code&gt;raw3&lt;/code&gt; data.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span class=&#34;n&#34;&gt;raw3&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;plot&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n_channels&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;start&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;53&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;duration&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;block&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;True&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;title&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;#34;After&amp;#34;&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://cbrnr.github.io/images/corrected_without_eog.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you look closely, the method successfully removed ocular artifacts clearly visible in this time segment.&lt;/p&gt;
&lt;h2 id=&#34;download-the-code&#34;&gt;Download the code&lt;/h2&gt;
&lt;p&gt;The script &lt;a href=&#34;https://cbrnr.github.io/code/removing-eog-regression.py&#34;&gt;removing-eog-regression.py&lt;/a&gt; contains all relevant code snippets from this post.&lt;/p&gt;
</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/python/">Python</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/mne/">MNE</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eeg/">EEG</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eog/">EOG</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/artifacts/">Artifacts</category>
                                
                            
                        
                    
                
            </item>
        
            <item>
                <title>Setting up Python for EEG analysis</title>
                <link>https://cbrnr.github.io/posts/setting-up-python/</link>
                <guid isPermaLink="true">https://cbrnr.github.io/posts/setting-up-python/</guid>
                <pubDate>Mon, 09 Oct 2017 00:00:00 &#43;0000</pubDate>
                
                    <author>clemens.brunner@gmail.com (Clemens Brunner)</author>
                
                <copyright>[CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/deed.en)</copyright>
                
                    <description>&lt;h2 id=&#34;installing-python&#34;&gt;Installing Python&lt;/h2&gt;
&lt;p&gt;There are several ways to install a Python environment on your computer. For example, the most obvious way involves downloading an installer from the &lt;a href=&#34;https://www.python.org/&#34;&gt;official Python website&lt;/a&gt;. Official installers are available for Windows and macOS. Most Linux distributions include Python out of the box (if not you can use the package manager to quickly set up a working Python environment). While this option works in many situations, obtaining additional Python packages for scientific computing might not be particularly convenient for beginners.&lt;/p&gt;
&lt;p&gt;An alternative available on macOS is the package manager &lt;a href=&#34;https://brew.sh/&#34;&gt;Homebrew&lt;/a&gt;. Although this is my preferred way of installing Python on a Mac, I will not describe it in detail here (other than mention that running &lt;code&gt;brew install python&lt;/code&gt; in a terminal is all it takes).&lt;/p&gt;
&lt;p&gt;While Python is relatively easy to set up, installing additional packages sometimes involves compiling source code. This means that you need to have a working build toolchain on your machine, which is standard on Linux systems, relatively straightforward on macOS, but a bit more challenging on Windows.&lt;/p&gt;
&lt;p&gt;This is where Python distributions come in handy – they bundle Python with additional pre-built packages, which means that compiling is not necessary. One of the most popular Python distributions (at least in the scientific community) seems to be &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;Anaconda&lt;/a&gt;, which includes &lt;a href=&#34;https://anaconda.org/anaconda/repo&#34;&gt;hundreds of useful Python packages&lt;/a&gt;. Among the most notable features are versions of &lt;a href=&#34;http://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt;, &lt;a href=&#34;https://www.scipy.org/scipylib/index.html&#34;&gt;SciPy&lt;/a&gt;, and &lt;a href=&#34;http://scikit-learn.org/stable/&#34;&gt;Scikit-learn&lt;/a&gt; with &lt;a href=&#34;https://software.intel.com/mkl&#34;&gt;Intel Math Kernel Library (MKL)&lt;/a&gt; support. This basically means that if you have an Intel CPU many computations will be faster than with standard versions of these packages (although in my experience &lt;a href=&#34;https://www.openblas.net&#34;&gt;OpenBLAS&lt;/a&gt; is just as fast and also runs on AMD CPUs).&lt;/p&gt;
&lt;p&gt;Installing Anaconda is as easy as downloading and running the &lt;a href=&#34;https://www.anaconda.com/distribution/&#34;&gt;installer&lt;/a&gt; for your operating system. After that, you are all set.&lt;/p&gt;
&lt;p&gt;If you do not want to install dozens of packages that come with Anaconda (for example because you want to save precious disk space), you can install its barebones sibling called &lt;a href=&#34;https://conda.io/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;. Just know that you will need to install (almost) all additional packages manually via the &lt;code&gt;conda&lt;/code&gt; package manager.&lt;/p&gt;
&lt;h2 id=&#34;installing-additional-packages&#34;&gt;Installing additional packages&lt;/h2&gt;
&lt;p&gt;Although Python comes with an extensive &lt;a href=&#34;https://docs.python.org/3/library/index.html&#34;&gt;standard library&lt;/a&gt;, most scientific packages are not part of Python itself. Luckily, installing additional Python packages is not difficult. If you use Anaconda or Miniconda, you can leverage the &lt;code&gt;conda&lt;/code&gt; command line tool to manage Python packages. Open a terminal (sometimes also called a command line) to use the tool.&lt;/p&gt;
&lt;p&gt;The first step is to find out if a particular package is available in the Anaconda repository (replace &lt;code&gt;&amp;lt;package&amp;gt;&lt;/code&gt; with the actual name of the package):&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;conda search &amp;lt;package&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If the package is available (meaning that the previous command returned a match), here&amp;rsquo;s how you can install it:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;conda install &amp;lt;package&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;For example, if you are planning to do EEG analysis, chances are that you will be using &lt;a href=&#34;https://mne.tools&#34;&gt;MNE&lt;/a&gt;. Let&amp;rsquo;s see if a package called &lt;code&gt;mne&lt;/code&gt; is available in Anaconda:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;conda search mne
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Unfortunately, the search returns no matches (meaning that MNE is not available in Anaconda), so we need to use another method to install it. The standard way to go about this is to use the official Python package manager &lt;code&gt;pip&lt;/code&gt; for packages that are not available with &lt;code&gt;conda&lt;/code&gt;. The &lt;code&gt;pip&lt;/code&gt; command line tool searches the &lt;a href=&#34;https://pypi.python.org/pypi&#34;&gt;Python Packaging Index (PyPI)&lt;/a&gt;, which is the central repository for third-party Python packages.&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s how you can search for the &lt;code&gt;mne&lt;/code&gt; package:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip search mne
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This command spits out several matches, including a package called &lt;code&gt;mne&lt;/code&gt;. We can install the package as follows:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip install mne
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That&amp;rsquo;s all there&amp;rsquo;s to it. Just remember to use &lt;code&gt;conda&lt;/code&gt; to search and install a package first, because Anaconda packages are often optimized and tested to work well in combination with other Anaconda packages. If a package is not available in Anaconda, use &lt;code&gt;pip&lt;/code&gt; to install it. If you do not use Anaconda/Miniconda, you can only use &lt;code&gt;pip&lt;/code&gt; anyway.&lt;/p&gt;
&lt;h2 id=&#34;recommended-scientific-packages&#34;&gt;Recommended scientific packages&lt;/h2&gt;
&lt;p&gt;Here are some useful packages that I recommend if you want to use Python for scientific computing in general and EEG processing in particular (use &lt;code&gt;conda&lt;/code&gt; and/or &lt;code&gt;pip&lt;/code&gt; to install them):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.numpy.org/&#34;&gt;NumPy&lt;/a&gt; provides a multi-dimensional array data type; it is the basis for almost all scientific packages.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.scipy.org/scipylib/index.html&#34;&gt;SciPy&lt;/a&gt; contains a large number of various algorithms used in scientific computing.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://pandas.pydata.org/&#34;&gt;Pandas&lt;/a&gt; extends the NumPy array and provides a more flexible data frame type similar to the one found in &lt;a href=&#34;https://www.r-project.org/&#34;&gt;R&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://matplotlib.org/&#34;&gt;Matplotlib&lt;/a&gt; is the most popular package to create all kinds of plots in Python.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ipython.org/&#34;&gt;IPython&lt;/a&gt; is an enhanced interactive Python shell.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://scikit-learn.org/stable/&#34;&gt;Scikit-learn&lt;/a&gt; is a powerful machine learning package for Python.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://martinos.org/mne/stable/index.html&#34;&gt;MNE&lt;/a&gt; is a package for EEG/MEG processing.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.riverbankcomputing.com/software/pyqt/intro&#34;&gt;PyQt5&lt;/a&gt; provides Python bindings for the &lt;a href=&#34;https://www.qt.io/&#34;&gt;Qt GUI toolkit&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;keeping-everything-up-to-date&#34;&gt;Keeping everything up to date&lt;/h2&gt;
&lt;p&gt;It is generally a good idea to use the most recent version of Python. In addition, you probably also want to keep all installed packages up to date. If you use Anaconda, you can update all installed packages with:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;conda update --all
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Packages installed via &lt;code&gt;pip&lt;/code&gt; cannot be updated with &lt;code&gt;conda&lt;/code&gt;. However, you can use &lt;code&gt;pip&lt;/code&gt; to get a list of outdated packages:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip list --outdated
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This list might also include packages installed via &lt;code&gt;conda&lt;/code&gt; (in general, &lt;code&gt;pip&lt;/code&gt; offers the latest versions before they get into Anaconda/Miniconda). You can keep track of packages installed via &lt;code&gt;pip&lt;/code&gt; by inspecting the rightmost &amp;ldquo;Channel&amp;rdquo; column of the output of &lt;code&gt;conda list&lt;/code&gt;. If it says &amp;ldquo;pypi&amp;rdquo; then you can (and should) only update this package using &lt;code&gt;pip&lt;/code&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip install -U &amp;lt;package&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you do not use Anaconda (or Miniconda), you have to use &lt;code&gt;pip&lt;/code&gt; to find and update all outdated packages. Unfortunately, there is no built-in flag to update all outdated packages at once. You can either update each package individually or use any of the numerous methods mentioned in this &lt;a href=&#34;https://stackoverflow.com/questions/2720014/upgrading-all-packages-with-pip&#34;&gt;StackOverflow post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Finally, feel free to install as many packages as you like. After all, Python makes it easy and fun to try out stuff that others have already implemented so that you don&amp;rsquo;t have to! If you decide later on that you do not need a specific package, you can completely remove it with either &lt;code&gt;conda&lt;/code&gt; or &lt;code&gt;pip&lt;/code&gt; (depending on how you installed it):&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;conda uninstall &amp;lt;package&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip uninstall &amp;lt;package&amp;gt;
&lt;/code&gt;&lt;/pre&gt;</description>
                
                
                
                
                
                    
                        
                    
                        
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/python/">Python</category>
                                
                            
                                
                                
                                
                                    <category domain="https://cbrnr.github.io/tags/eeg/">EEG</category>
                                
                            
                        
                    
                
            </item>
        
    </channel>
</rss>
